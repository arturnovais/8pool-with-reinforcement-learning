{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from game import GAME\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciando game e exemplificando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GAME(draw=True)  \n",
    "observations = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_function(information):\n",
    "    colisoes = information['colisoes']\n",
    "    bolas_caidas = information['bolas_caidas']\n",
    "    perdeu = information['perdeu']\n",
    "    ganhou = information['ganhou']\n",
    "    joga_novamente = information['joga_novamente']\n",
    "    bolas_jogador = information.get('bolas_jogador', [])\n",
    "    bolas_adversario = information.get('bolas_adversario', [])\n",
    "    winner = information.get('winner', None)\n",
    "    penalizado = information.get('penalizado', False)\n",
    "\n",
    "    rewards = 0\n",
    "\n",
    "    if joga_novamente:\n",
    "        rewards += 1\n",
    "\n",
    "    if perdeu:\n",
    "        rewards -= 1.5\n",
    "\n",
    "    if ganhou:\n",
    "        rewards += 1.5\n",
    "\n",
    "    if penalizado:\n",
    "        rewards -= 1\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.table.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state (tensor([[0.0137, 0.5000, 1.0000, 0.0000],\n",
      "        [0.9210, 0.0862, 1.0000, 0.0000],\n",
      "        [0.7305, 0.0800, 1.0000, 1.0000],\n",
      "        [0.9072, 0.5875, 1.0000, 0.0000],\n",
      "        [0.9466, 0.8325, 1.0000, 1.0000],\n",
      "        [0.9379, 0.7357, 1.0000, 0.0000],\n",
      "        [0.9423, 0.6031, 1.0000, 1.0000],\n",
      "        [0.7940, 0.3715, 1.0000, 0.0000],\n",
      "        [0.4413, 0.7356, 1.0000, 1.0000],\n",
      "        [0.9355, 0.3417, 1.0000, 0.0000],\n",
      "        [0.9119, 0.6792, 1.0000, 1.0000],\n",
      "        [0.8300, 0.7633, 1.0000, 1.0000],\n",
      "        [0.9534, 0.2136, 1.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000]]), tensor([0.4703, 0.5511]))\n",
      "termination False\n",
      "rewards 0\n"
     ]
    }
   ],
   "source": [
    "## jogador faz a jogada\n",
    "\n",
    "\n",
    "env.iniciou_jogada = False\n",
    "while not env.iniciou_jogada:\n",
    "    env.table.draw()\n",
    "\n",
    "obs, information, terminations, rewards = env.step((env.iniciou_jogada_angulo, \n",
    "                                                    env.inicou_jogada_intensidade), rewards_function=rewards_function)\n",
    "\n",
    "print('state', obs)\n",
    "print('termination', terminations)\n",
    "print('rewards', rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando jogada True\n",
      "Recompensa da jogada: -1\n"
     ]
    }
   ],
   "source": [
    "# computador faz a jogada\n",
    "print(\"Iniciando jogada\", env.jogador_atual)\n",
    "\n",
    "\n",
    "\n",
    "angulo = 10\n",
    "intensidade = 0.9\n",
    "\n",
    "env.iniciou_jogada = False\n",
    "obs, information, terminations, rewards = env.step( (angulo, intensidade),  rewards_function=rewards_function)\n",
    "\n",
    "print(\"Recompensa da jogada:\", rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Começando treinamento\n",
    "                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    embed_dim : int = 128\n",
    "    num_heads : int = 8         \n",
    "    ff_dim : int = embed_dim * 2\n",
    "    num_layers : int =  2       \n",
    "    dropout : float = 0.1         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo política"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ideia é que tenhamos uma rede neural como política, especificamente um transformers. \n",
    "\n",
    "Queremos que ele passe por cada bola e gere uma representação, no fim a representação do nosso estado será o embedding dabola branca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transformers(nn.Module):\n",
    "    def __init__(self, args : Args):\n",
    "        super(Transformers, self).__init__()\n",
    "        \n",
    "        # Layer de embedding (entra uma bola com (x,y,z, w) e sai um vetor de 128 dimensões que representa a bola)\n",
    "        self.embedding = nn.Linear(4, args.embed_dim)\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer( d_model=args.embed_dim, \n",
    "                                       nhead=args.num_heads, \n",
    "                                       dim_feedforward=args.ff_dim, \n",
    "                                       dropout=args.dropout) for _ in range(args.num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(args.embed_dim)\n",
    "        \n",
    "        #MlP para gerar regressão de angulo e intensidade\n",
    "        self.mlp = nn.Sequential( \n",
    "                nn.Linear(args.embed_dim, args.ff_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(args.ff_dim, 3) # Vamos sair com (sen, cos, intensidade)\n",
    "        )\n",
    "        \n",
    "        self.head_position = nn.Tanh() # Tanh para garantir que o angulo fique entre -1 e 1\n",
    "        self.head_intensity = nn.Sigmoid() # Sigmoid para garantir que a intensidade fique entre 0 e 1\n",
    "\n",
    "    def forward(self, x, bola_branca): \n",
    "        # adiciona a dimensão da bolinha branca -> batch, (x,y,1,-1)\n",
    "        b = bola_branca.shape[0]\n",
    "        t_concat = torch.zeros(b,2, device=bola_branca.device)\n",
    "        t_concat[:,0] = 1\n",
    "        t_concat[:,1] = -1\n",
    "        bola_branca = torch.concat( (bola_branca , t_concat), dim=-1).unsqueeze(1)\n",
    "        x = torch.concat((bola_branca,x),dim=1)\n",
    "        \n",
    "        x = self.embedding(x)        \n",
    "        \n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "    \n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        # bola branca é nosso Value, que ira representar o estado do jogo\n",
    "        x = x[:,0,:]        \n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        # transforma o x em angulo e intensidade\n",
    "        position = self.head_position(x[:,:2])\n",
    "        intensity = self.head_intensity(x[:,2:])\n",
    "        \n",
    "        x_sin = position[:,0:1] # seno\n",
    "        x_cos = position[:,1:2] # cosseno\n",
    "        \n",
    "        angles_deg = torch.rad2deg(torch.atan2(x_sin, x_cos)) # converte para graus\n",
    "        angles_deg = torch.where(angles_deg < 0, angles_deg + 360, angles_deg)\n",
    "        \n",
    "        # concatena o resultado\n",
    "        return torch.concat((angles_deg,intensity),dim=-1) # Retorna o angulo e a intensidade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instanciando e testando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(\n",
    ")\n",
    "\n",
    "model = Transformers(\n",
    "    args\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo de foward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[350.2479,   0.5267],\n",
       "        [334.3201,   0.5506]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, bola_branca = (torch.tensor([\n",
    "        [[0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.8250, 0.3750, 1.0000, 1.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
    "        [[0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.8250, 0.3750, 1.0000, 1.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000]]\n",
    "        ]),\n",
    "                      \n",
    " torch.tensor([[0.2000, 0.5000],[0.2000, 0.5000]]))\n",
    "\n",
    "model(state,bola_branca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerValueModel(nn.Module):\n",
    "    def __init__(self, args : Args):\n",
    "        super(TransformerValueModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Linear(4, args.embed_dim)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=args.embed_dim,\n",
    "                nhead=args.num_heads,\n",
    "                dim_feedforward=args.ff_dim,\n",
    "                dropout=args.dropout\n",
    "            ) for _ in range(args.num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(args.embed_dim)\n",
    "        \n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(args.embed_dim, args.ff_dim),  # Projeta para dimensão intermediária\n",
    "            nn.ReLU(),                    # Ativação não-linear\n",
    "            nn.Linear(args.ff_dim, 1)          # Saída escalar (valor do estado)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, bola_branca):\n",
    "        # Adiciona a dimensão da bolinha branca -> batch, (x,y,1,-1)\n",
    "        b = bola_branca.shape[0]\n",
    "        t_concat = torch.zeros(b, 2, device=bola_branca.device)\n",
    "        t_concat[:, 0] = 1\n",
    "        t_concat[:, 1] = -1\n",
    "        bola_branca = torch.concat((bola_branca, t_concat), dim=-1).unsqueeze(1)\n",
    "        x = torch.concat((bola_branca, x), dim=1)\n",
    "        \n",
    "        # Passa pela camada de embedding\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Passa pelas camadas de encoder\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Normalização final\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        # Seleciona a representação do estado do jogo (primeiro token)\n",
    "        x = x[:, 0, :]\n",
    "        \n",
    "        # Passa pela cabeça de valor para prever V(s)\n",
    "        value = self.value_head(x)\n",
    "        \n",
    "        # Garante que a saída está no formato (batch_size,)\n",
    "        return value  # Remove apenas a última dimensão\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(\n",
    "    embed_dim=128   \n",
    ")\n",
    "\n",
    "model = TransformerValueModel(\n",
    "    args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1074],\n",
       "        [0.1023]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(state,bola_branca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, action_dim, args):\n",
    "        super().__init__()\n",
    "        self.critic = TransformerValueModel(args)  # Modelo para estimar o valor\n",
    "        self.actor_mean = Transformers(args)  # Modelo para estimar a média das ações\n",
    "        self.actor_log_std = nn.Parameter(torch.zeros(action_dim))  # Desvio padrão fixo\n",
    "\n",
    "    def get_value(self, state, bola_branca):\n",
    "        \"\"\"Obtém o valor estimado pelo modelo crítico.\"\"\"\n",
    "        return self.critic(state, bola_branca)\n",
    "\n",
    "    def get_action_and_value(self, state, bola_branca, action=None):\n",
    "        \"\"\"\n",
    "        Obtém a ação, log_prob, entropia e valor.\n",
    "        \n",
    "        Args:\n",
    "            x: Entrada para o modelo.\n",
    "            bola_branca: Dados da bola branca.\n",
    "            action: Ação opcional para avaliar log_prob e entropia.\n",
    "        \n",
    "        Returns:\n",
    "            action: Ação amostrada ou fornecida.\n",
    "            log_prob: Logaritmo da probabilidade da ação.\n",
    "            entropy: Entropia da distribuição.\n",
    "            value: Valor estimado pelo crítico.\n",
    "        \"\"\"\n",
    "        # O ator retorna ângulo e intensidade\n",
    "        mean = self.actor_mean(state, bola_branca)  # Predição da média das ações\n",
    "        log_std = self.actor_log_std.expand_as(mean)  # Ajusta o desvio padrão ao formato correto\n",
    "        std = torch.exp(log_std)  # Calcula o desvio padrão\n",
    "\n",
    "        # Distribuição Normal para ações contínuas\n",
    "        dist = Normal(mean, std)\n",
    "\n",
    "        if action is None:\n",
    "            action = dist.sample()  # Amostra uma ação da distribuição\n",
    "\n",
    "        log_prob = dist.log_prob(action).sum(axis=-1)  # Log probabilidade\n",
    "        entropy = dist.entropy().sum(axis=-1)  # Entropia\n",
    "\n",
    "        value = self.critic(state, bola_branca)  # Valor estimado pelo crítico\n",
    "\n",
    "        return action, log_prob, entropy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_dim, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action tensor([[ 97.5531,   1.1060],\n",
      "        [162.2326,   1.0374]])\n",
      "log_prob tensor([-2.2460, -2.1543], grad_fn=<SumBackward1>)\n",
      "entropy tensor([2.8379, 2.8379], grad_fn=<SumBackward1>)\n",
      "value tensor([[0.1254],\n",
      "        [0.0573]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "action, log_prob, entropy, value = agent.get_action_and_value(state, bola_branca)\n",
    "\n",
    "print('action', action)\n",
    "print('log_prob', log_prob)\n",
    "print('entropy', entropy)\n",
    "print('value', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui vemos que as ações não estão no range esperado, isso porque estamos passando uma distribuiçao normal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, policy_model, value_model ,lr=1e-3, gamma=0.99, eps_clip=0.2, entropy_coef=0.01):\n",
    "        self.policy = policy_model\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        self.value_model = value_model\n",
    "        self.value_optimizer = optim.Adam(self.value_model.parameters(), lr=lr)\n",
    "        \n",
    "    def compute_advantage(self, rewards, values, masks, gamma):\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for reward, mask in zip(reversed(rewards), reversed(masks)):\n",
    "            R = reward + gamma * R * mask\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        advantage = returns - values\n",
    "        return advantage.detach(), returns\n",
    "    \n",
    "    def update(self, states, bola_branca, actions, log_probs_old, rewards, dones):\n",
    "        states_flat = states.view(states.size(0), -1)\n",
    "        batch_size = states.size(0)\n",
    "\n",
    "        # Calculando valores e vantagens\n",
    "        values = self.value_model(states_flat, bola_branca).squeeze(-1)\n",
    "        advantages, returns = self.compute_advantage(rewards, values, 1 - dones, self.gamma)\n",
    "        \n",
    "        # Atualizar política\n",
    "        for _ in range(4):  # Número de atualizações\n",
    "            outputs = self.policy(states, bola_branca)\n",
    "            dist = Normal(outputs, torch.ones_like(outputs))  # Normal para distribuição\n",
    "            log_probs = dist.log_prob(actions).sum(-1)\n",
    "            entropy = dist.entropy().sum(-1)\n",
    "            \n",
    "            # Clipped Surrogate Objective\n",
    "            ratio = torch.exp(log_probs - log_probs_old)\n",
    "            surrogate1 = ratio * advantages\n",
    "            surrogate2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "            entropy_loss = -self.entropy_coef * entropy.mean()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            (policy_loss + entropy_loss).backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Atualizar função de valor\n",
    "        for _ in range(4):  # Número de atualizações\n",
    "            values = self.value_model(torch.cat((states_flat, bola_branca), dim=1)).squeeze(-1)\n",
    "            value_loss = nn.MSELoss()(values, returns)\n",
    "            \n",
    "            self.value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            self.value_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Transformers(args)\n",
    "value = TransformerValueModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = PPO(policy, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.config as cfg\n",
    "\n",
    "## iniciar o ambiente com a configuração do jogo com apenas uma bola na mesa. \n",
    "# - A bola branca na posição (0, 0)\n",
    "# - se não bater na bola, acabou a jogada\n",
    "\n",
    "cfg.raio_buraco = 13\n",
    "env.reset()\n",
    "env.table.bolas = env.table.bolas[1:2] + [env.table.bola_branca]\n",
    "numero_bola = env.table.bolas[0].numero\n",
    "env.numero_bolas = [[ numero_bola ], [ numero_bola ]]\n",
    "\n",
    "#env.table.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7937, 0.4062, 1.0000, 1.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000]]),\n",
       " tensor([0.2000, 0.5000]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_bolas, state_bola_branca = env.get_observations()\n",
    "# shuffle the balls\n",
    "state_bolas = state_bolas[torch.randperm(state_bolas.size()[0])]\n",
    "state_bolas , state_bola_branca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipyflow)",
   "language": "python",
   "name": "ipyflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
