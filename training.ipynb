{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.11.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from game import GAME\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from torch.distributions import Normal\n",
    "from utils import config as cfg\n",
    "\n",
    "\n",
    "from curriculum.reset import reset_random_one_ball\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from models.model_transformers import Model_args, TransformerValueModel, TransformersAtor\n",
    "\n",
    "from models.agent import Agent\n",
    "\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import math\n",
    "import utils.config as cfg\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_function(information):\n",
    "    colisoes = information['colisoes']\n",
    "    bolas_caidas = information['bolas_caidas']\n",
    "    perdeu = information['perdeu']\n",
    "    ganhou = information['ganhou']\n",
    "    joga_novamente = information['joga_novamente']\n",
    "    bolas_jogador = information.get('bolas_jogador', [])\n",
    "    bolas_adversario = information.get('bolas_adversario', [])\n",
    "    winner = information.get('winner', None)\n",
    "    penalizado = information.get('penalizado', False)\n",
    "\n",
    "    rewards = 1\n",
    "    \n",
    "    #print(information)\n",
    "    \n",
    "    if joga_novamente:\n",
    "        rewards += 1\n",
    "\n",
    "    if perdeu:\n",
    "        rewards -= 1.5\n",
    "\n",
    "    if ganhou:\n",
    "        rewards += 1.5\n",
    "\n",
    "    if penalizado:\n",
    "        rewards -= 1\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = GAME(draw=True)  \n",
    "#observations = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def reset_random_one_ball2(env,raio_buraco = 13, bola_ = (0,0), bola_branca=(0,0)):\n",
    "    ## iniciar o ambiente com a configuração do jogo com apenas uma bola na mesa. \n",
    "    # - A bola branca em uma posição aleatória\n",
    "    # - se não bater na bola, acabou a jogada\n",
    "\n",
    "    cfg.raio_buraco = raio_buraco\n",
    "    \n",
    "    epson = raio_buraco*2\n",
    "\n",
    "    env.reset()\n",
    "    env.table.bolas = env.table.bolas[:2] + [env.table.bola_branca]\n",
    "    numero_bola = env.table.bolas[1].numero\n",
    "    \n",
    "    \n",
    "    for _ ,  bola in enumerate(env.table.bolas[1:]):\n",
    "        bola.posicao[1] = bola_[0] + env.table.x_start\n",
    "        bola.posicao[0] = bola_[1] + env.table.y_start\n",
    "    \n",
    "    env.table.bola_branca.posicao[0] = bola_branca[0]  + env.table.x_start\n",
    "    env.table.bola_branca.posicao[1] = bola_branca[1]  + env.table.y_start\n",
    "    \n",
    "    env.numero_bolas = [[numero_bola ], []]\n",
    "    env.player=0\n",
    "    \n",
    "    \n",
    "    return env.get_observations()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## jogador faz a jogada\n",
    "#\n",
    "#reset_random_one_ball(env)\n",
    "#\n",
    "#env.iniciou_jogada = False\n",
    "#\n",
    "#while not env.iniciou_jogada:\n",
    "#    env.table.draw()\n",
    "#    \n",
    "##env.inicou_jogada_intensidade = 0.42\n",
    "##env.iniciou_jogada_angulo = 0.45\n",
    "#\n",
    "#obs, information, terminations, rewards = env.step((env.iniciou_jogada_angulo, \n",
    "#                                                    env.inicou_jogada_intensidade), rewards_function=rewards_function)\n",
    "#\n",
    "#\n",
    "#\n",
    "##print(env.iniciou_jogada_angulo)\n",
    "#\n",
    "##print('state', obs)\n",
    "##print('termination', terminations)\n",
    "#print('rewards', rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GAME(draw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbola_branca_x = (bola_branca[0] - env.table.x_start) / cfg.display_table_width\\nbola_branca_y = (bola_branca[1] - env.table.y_start) / cfg.display_table_height\\n\\nbola_numero_x = (bola_numero[0] - env.table.x_start) / cfg.display_table_width\\nbola_numero_y = (bola_numero[1] - env.table.y_start) / cfg.display_table_height\\n\\nangulo = calcula_angulo((bola_branca_x, bola_branca_y), (bola_numero_x, bola_numero_y))\\n\\nstate, state_bola_branca = reset_random_one_ball2(env=env, \\n                                                  bola_=bola_numero, \\n                                                  bola_branca=bola_branca)\\n\\nenv.step((angulo, 0.5), rewards_function=rewards_function)\\n\\n#while time.time()  - start < 4:\\n#   env.table.draw()\\n\\n    \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def calcula_angulo(bola_branca, bola_numero):\n",
    "\n",
    "        x, y = bola_branca\n",
    "        x1, y1 = bola_numero \n",
    "\n",
    "        x *= cfg.display_table_width\n",
    "        x1 *= cfg.display_table_width\n",
    "        y *= cfg.display_table_height\n",
    "        y1 *= cfg.display_table_height   \n",
    "\n",
    "\n",
    "        return math.atan2(y1 - y, x1 - x)\n",
    "\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "bola_numero = 200, 400\n",
    "bola_branca = np.random.uniform(20, cfg.display_table_width - 20) , np.random.uniform(20, cfg.display_table_height - 20)\n",
    "#\n",
    "\"\"\"\n",
    "bola_branca_x = (bola_branca[0] - env.table.x_start) / cfg.display_table_width\n",
    "bola_branca_y = (bola_branca[1] - env.table.y_start) / cfg.display_table_height\n",
    "\n",
    "bola_numero_x = (bola_numero[0] - env.table.x_start) / cfg.display_table_width\n",
    "bola_numero_y = (bola_numero[1] - env.table.y_start) / cfg.display_table_height\n",
    "\n",
    "angulo = calcula_angulo((bola_branca_x, bola_branca_y), (bola_numero_x, bola_numero_y))\n",
    "\n",
    "state, state_bola_branca = reset_random_one_ball2(env=env, \n",
    "                                                  bola_=bola_numero, \n",
    "                                                  bola_branca=bola_branca)\n",
    "\n",
    "env.step((angulo, 0.5), rewards_function=rewards_function)\n",
    "\n",
    "#while time.time()  - start < 4:\n",
    "#   env.table.draw()\n",
    "\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def draw_mirrored_pool_tables(bola_branca, bola_numero):\n",
    "#    \"\"\"\n",
    "#    Draws 9 mirrored pool tables based on the given dimensions and ball positions.\n",
    "#\n",
    "#    Args:\n",
    "#        l: Width of the table.\n",
    "#        A: Height of the table.\n",
    "#        balls: List of ball positions as tuples (x, y).\n",
    "#    \"\"\"\n",
    "#    l=cfg.display_table_width \n",
    "#    A=cfg.display_table_height\n",
    "#    posicoes = [ \n",
    "#                    (bola_numero[0], bola_numero[1]),\n",
    "#                    (-bola_numero[0], bola_numero[1]),\n",
    "#                    ( l+(l-bola_numero[0]), bola_numero[1]),#\n",
    "#                    (bola_numero[0], -bola_numero[1]),\n",
    "#                    (bola_numero[0], A+(A-bola_numero[1])),#\n",
    "#                    (-bola_numero[0], A+(A-bola_numero[1])),#\n",
    "#                    (-bola_numero[0], -bola_numero[1]),\n",
    "#                    ( l+(l-bola_numero[0]), A+(A-bola_numero[1])),\n",
    "#                    ( l+(l-bola_numero[0]), -bola_numero[1]),#\n",
    "#                ]\n",
    "#    \n",
    "#    l,A = 1 , 1\n",
    "#    posicoes = [ \n",
    "#                    (bola_numero[0], bola_numero[1]),\n",
    "#                    (-bola_numero[0], bola_numero[1]),\n",
    "#                    ( l+(l-bola_numero[0]), bola_numero[1]),#\n",
    "#                    (bola_numero[0], -bola_numero[1]),\n",
    "#                    (bola_numero[0], A+(A-bola_numero[1])),#\n",
    "#                    (-bola_numero[0], A+(A-bola_numero[1])),#\n",
    "#                    (-bola_numero[0], -bola_numero[1]),\n",
    "#                    ( l+(l-bola_numero[0]), A+(A-bola_numero[1])),\n",
    "#                    ( l+(l-bola_numero[0]), -bola_numero[1]),#\n",
    "#                ]\n",
    "#\n",
    "#    for p in posicoes:\n",
    "#        yield calcula_angulo(bola_branca, p)\n",
    "#        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport math\\nimport utils.config as cfg\\nimport torch\\n\\n\\ncfg.use_clock= False\\nfor p in range(9):\\n    \\n    import time\\n    start = time.time()\\n\\n\\n    bola_numero = 200, 400\\n    bola_branca = np.random.uniform(20, cfg.display_width - 20) , np.random.uniform(20, cfg.display_height - 20)\\n    bola_branca = cfg.bola_branca_posicao_inicial\\n    #\\n    state, state_bola_branca = reset_random_one_ball2(env=env, bola_=bola_numero, bola_branca=bola_branca)\\n\\n    \\n    while time.time()  - start < 0.5:\\n       env.table.draw()\\n    \\n    angulo = list(draw_mirrored_pool_tables(state_bola_branca.numpy().tolist(), \\n                            state[1][:2].numpy().tolist()))[p]\\n    \\n \\n \\n    cfg.forca_maxima = 30\\n    intensidade = 1\\n\\n    obs, information, terminations, rewards = env.step( (angulo, intensidade),  rewards_function=rewards_function)\\n    \\n    print(p,rewards)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import math\n",
    "import utils.config as cfg\n",
    "import torch\n",
    "\n",
    "\n",
    "cfg.use_clock= False\n",
    "for p in range(9):\n",
    "    \n",
    "    import time\n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "    bola_numero = 200, 400\n",
    "    bola_branca = np.random.uniform(20, cfg.display_width - 20) , np.random.uniform(20, cfg.display_height - 20)\n",
    "    bola_branca = cfg.bola_branca_posicao_inicial\n",
    "    #\n",
    "    state, state_bola_branca = reset_random_one_ball2(env=env, bola_=bola_numero, bola_branca=bola_branca)\n",
    "\n",
    "    \n",
    "    while time.time()  - start < 0.5:\n",
    "       env.table.draw()\n",
    "    \n",
    "    angulo = list(draw_mirrored_pool_tables(state_bola_branca.numpy().tolist(), \n",
    "                            state[1][:2].numpy().tolist()))[p]\n",
    "    \n",
    " \n",
    " \n",
    "    cfg.forca_maxima = 30\n",
    "    intensidade = 1\n",
    "\n",
    "    obs, information, terminations, rewards = env.step( (angulo, intensidade),  rewards_function=rewards_function)\n",
    "    \n",
    "    print(p,rewards)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_actions(state, state_bola_branca, bola_numero):\\n    angulos = []\\n    for p in range(9):\\n        angulos.append(list(draw_mirrored_pool_tables(state_bola_branca.numpy().tolist(), \\n                            state[bola_numero][:2].numpy().tolist()))[p])\\n        \\n    return angulos\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def get_actions(state, state_bola_branca, bola_numero):\n",
    "    angulos = []\n",
    "    for p in range(9):\n",
    "        angulos.append(list(draw_mirrored_pool_tables(state_bola_branca.numpy().tolist(), \n",
    "                            state[bola_numero][:2].numpy().tolist()))[p])\n",
    "        \n",
    "    return angulos\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstate, state_bola_branca = env.reset()\\nangulos = get_actions(state, state_bola_branca, 1)\\n\\n\\ncfg.forca_maxima = 30\\nintensidade = 1\\n\\n\\nobs, information, terminations, rewards = env.step( (angulo, intensidade),  rewards_function=rewards_function)\\n    \\nprint(p,rewards)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "state, state_bola_branca = env.reset()\n",
    "angulos = get_actions(state, state_bola_branca, 1)\n",
    "\n",
    "\n",
    "cfg.forca_maxima = 30\n",
    "intensidade = 1\n",
    "\n",
    "\n",
    "obs, information, terminations, rewards = env.step( (angulo, intensidade),  rewards_function=rewards_function)\n",
    "    \n",
    "print(p,rewards)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Começando treinamento\n",
    "                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo política"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ideia é que tenhamos uma rede neural como política, especificamente um transformers. \n",
    "\n",
    "Queremos que ele passe por cada bola e gere uma representação, no fim a representação do nosso estado será o embedding dabola branca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    exp_name: str = \"PPO\"\n",
    "    \"\"\"the name of this experiment\"\"\"\n",
    "    seed: int = 1\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "    torch_deterministic: bool = True\n",
    "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
    "    cuda: bool = torch.cuda.is_available()\n",
    "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    capture_video: bool = False\n",
    "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
    "    save_model: bool = True\n",
    "    \"\"\"whether to save the final model\"\"\"\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    total_timesteps: int = 500000\n",
    "    \"\"\"total timesteps of the experiments\"\"\"\n",
    "    learning_rate: float = 2.5e-4\n",
    "    \"\"\"the learning rate of the optimizer\"\"\"\n",
    "    num_envs: int = 2\n",
    "    \"\"\"the number of parallel game environments\"\"\"\n",
    "    num_steps: int = 10 # 128\n",
    "    \"\"\"the number of steps to run in each environment per policy rollout\"\"\"\n",
    "    anneal_lr: bool = True\n",
    "    \"\"\"Toggle learning rate annealing for policy and value networks\"\"\"\n",
    "    gamma: float = 0.99\n",
    "    \"\"\"the discount factor gamma\"\"\"\n",
    "    gae_lambda: float = 0.95\n",
    "    \"\"\"the lambda for the general advantage estimation\"\"\"\n",
    "    num_minibatches: int = 4\n",
    "    \"\"\"the number of mini-batches\"\"\"\n",
    "    update_epochs: int = 4\n",
    "    \"\"\"the K epochs to update the policy\"\"\"\n",
    "    norm_adv: bool = True\n",
    "    \"\"\"Toggles advantages normalization\"\"\"\n",
    "    clip_coef: float = 0.2\n",
    "    \"\"\"the surrogate clipping coefficient\"\"\"\n",
    "    clip_vloss: bool = True\n",
    "    \"\"\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\"\"\"\n",
    "    ent_coef: float = 0.01\n",
    "    \"\"\"coefficient of the entropy\"\"\"\n",
    "    vf_coef: float = 0.5\n",
    "    \"\"\"coefficient of the value function\"\"\"\n",
    "    max_grad_norm: float = 0.5\n",
    "    \"\"\"the maximum norm for the gradient clipping\"\"\"\n",
    "    target_kl: float = None\n",
    "    \"\"\"the target KL divergence threshold\"\"\"\n",
    "\n",
    "    # to be filled in runtime\n",
    "    batch_size: int = 0\n",
    "    \"\"\"the batch size (computed in runtime)\"\"\"\n",
    "    minibatch_size: int = 0\n",
    "    \"\"\"the mini-batch size (computed in runtime)\"\"\"\n",
    "    num_iterations: int = 0\n",
    "    \"\"\"the number of iterations (computed in runtime)\"\"\"\n",
    "    \n",
    "    model_args  = None\n",
    "    \n",
    "    #model_args: ModelArgs = field(default_factory=ModelArgs)\n",
    "\n",
    "args = Args()\n",
    "#args.model_args = Model_args(\n",
    "#                    embed_dim  = 128,\n",
    "#                    num_heads  = 8,\n",
    "#                    ff_dim  = 128 * 2,\n",
    "#                    num_layers  =  2,\n",
    "#                    dropout  = 0.1,\n",
    "#                    )\n",
    "#\n",
    "args.model_args = Model_args(\n",
    "    embed_dim = 1024,\n",
    "    num_heads = 8,\n",
    "    ff_dim    = 4*1024,\n",
    "    num_layers =1,\n",
    "    dropout  = 0,\n",
    "    mlp_dim=1024,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model_args(embed_dim=1024, num_heads=8, ff_dim=4096, num_layers=1, dropout=0, mlp_dim=1024)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from curriculum.reset import reset_random_one_ball\n",
    "\n",
    "rewards_function\n",
    "\n",
    "class ambientes():\n",
    "    \n",
    "    def __init__(self,n , agent, rewards_function):\n",
    "        self.n = n\n",
    "        self.agent = agent\n",
    "        \n",
    "        self.envs = [ GAME(draw=True) for _ in range(n)]\n",
    "        self.rewards_function = rewards_function\n",
    "        \n",
    " \n",
    "    def format_observations(self, observations : list[tuple] ):\n",
    "        states = []\n",
    "        states_white_ball = []\n",
    "    \n",
    "        for obs in observations:\n",
    "            states += [obs[0].unsqueeze(dim=0)]\n",
    "            states_white_ball += [obs[1].unsqueeze(dim=0)]\n",
    "        return torch.concat(states) , torch.concat(states_white_ball)\n",
    "       \n",
    "    def reset(self):\n",
    "        states = []\n",
    "        for env in self.envs:\n",
    "            states += [reset_random_one_ball(env=env)]\n",
    "        \n",
    "        return self.format_observations(states)\n",
    "            \n",
    "    def single_observation_space(self):\n",
    "        return (15,4)\n",
    "    \n",
    "    def single_observation_space_white(self):\n",
    "        return (1,2)\n",
    "    \n",
    "    def single_action_space(self):\n",
    "        return (2,)\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        angulo =  action[:,0]\n",
    "        intensidade = action[:,1]\n",
    "        \n",
    "        \n",
    "        get_observations, informations , terminations, rewards = [],[],[],[]\n",
    "        \n",
    "        for i, env in enumerate(self.envs):\n",
    "\n",
    "            env_observations, env_informations , env_terminations, env_rewards = env.step( (angulo[i], intensidade[i]), rewards_function =self.rewards_function )\n",
    "            \n",
    "            \n",
    "            if env_terminations:\n",
    "                env_observations = reset_random_one_ball(env=env)\n",
    "\n",
    "            get_observations += [env_observations]\n",
    "            informations += [env_informations]\n",
    "            terminations += [env_terminations]\n",
    "            rewards += [env_rewards]\n",
    "        \n",
    "        \n",
    "        \n",
    "                \n",
    "        return self.format_observations(get_observations), informations , terminations, rewards\n",
    "\n",
    "    \n",
    "    \n",
    "     \n",
    "                \n",
    "#test\n",
    "#agent = Agent(action_dim=2,model_args=args.model_args)\n",
    "#a  =  ambientes(2,agent,rewards_function)\n",
    "#state, balls = a.reset()\n",
    "#action, logprob, _, value = agent.get_action_and_value(state, balls)\n",
    "#print(action)\n",
    "#observations, informations , terminations, rewards = a.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fazzi\\AppData\\Local\\Temp\\ipykernel_83196\\1196441664.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  agent.load_state_dict(torch.load('agent_big.pth'))\n",
      "Iterations:   0%|          | 0/25000 [00:00<?, ?it/s]C:\\Users\\fazzi\\AppData\\Local\\Temp\\ipykernel_83196\\1196441664.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_obs, next_done , next_obs_white = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device), torch.tensor(next_obs_white).to(device)\n",
      "Iterations:   0%|          | 1/25000 [00:22<158:16:23, 22.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 2/25000 [00:43<149:43:31, 21.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 3/25000 [01:08<161:06:21, 23.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 4/25000 [01:36<172:29:34, 24.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 5/25000 [02:02<177:28:28, 25.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 6/25000 [02:27<175:06:03, 25.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 7/25000 [02:52<174:04:55, 25.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 8/25000 [03:17<173:50:50, 25.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 9/25000 [03:43<176:31:28, 25.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 10/25000 [04:08<176:01:49, 25.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 11/25000 [04:31<170:35:30, 24.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 12/25000 [04:58<176:06:06, 25.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 13/25000 [05:27<184:20:57, 26.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 14/25000 [06:01<198:04:41, 28.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 15/25000 [06:27<194:43:47, 28.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 16/25000 [06:52<186:39:44, 26.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 17/25000 [07:12<172:48:14, 24.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 18/25000 [07:35<169:54:31, 24.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 19/25000 [08:07<184:10:56, 26.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 20/25000 [08:37<192:06:02, 27.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 21/25000 [08:56<174:28:33, 25.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 22/25000 [09:23<177:15:59, 25.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 23/25000 [09:47<174:16:49, 25.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 24/25000 [10:12<173:30:42, 25.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 25/25000 [10:37<174:12:23, 25.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 26/25000 [11:07<184:47:55, 26.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 27/25000 [11:33<182:52:10, 26.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 28/25000 [11:56<176:14:00, 25.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 29/25000 [12:18<167:50:33, 24.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 30/25000 [12:40<165:09:55, 23.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 31/25000 [13:04<163:47:26, 23.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 32/25000 [13:31<171:02:59, 24.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 33/25000 [13:54<167:53:41, 24.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 34/25000 [14:21<173:31:13, 25.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 35/25000 [14:44<170:13:02, 24.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 36/25000 [15:10<171:57:40, 24.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 37/25000 [15:33<169:36:36, 24.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 38/25000 [15:58<169:12:44, 24.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 39/25000 [16:18<161:10:02, 23.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 40/25000 [16:47<172:13:16, 24.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 41/25000 [17:16<182:29:22, 26.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 42/25000 [17:43<183:24:21, 26.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 43/25000 [18:14<192:02:34, 27.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 44/25000 [18:37<182:47:40, 26.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 45/25000 [18:56<166:50:27, 24.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 45/25000 [19:13<177:37:12, 25.62s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 277\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;124;03m    writer.close()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;124;03m    return metrics\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    276\u001b[0m cfg\u001b[38;5;241m.\u001b[39muse_clock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 92\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     85\u001b[0m logprobs[step] \u001b[38;5;241m=\u001b[39m logprob\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# TODO: transform ação dada a bola para angulo\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \n\u001b[0;32m     90\u001b[0m \n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# execute the game and log data.\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m (next_obs,next_obs_white), infos, next_done, reward \u001b[38;5;241m=\u001b[39m \u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[0;32m     96\u001b[0m rewards[step] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(reward)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 50\u001b[0m, in \u001b[0;36mambientes.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m get_observations, informations , terminations, rewards \u001b[38;5;241m=\u001b[39m [],[],[],[]\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs):\n\u001b[1;32m---> 50\u001b[0m     env_observations, env_informations , env_terminations, env_rewards \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mangulo\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintensidade\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards_function\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrewards_function\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m env_terminations:\n\u001b[0;32m     54\u001b[0m         env_observations \u001b[38;5;241m=\u001b[39m reset_random_one_ball(env\u001b[38;5;241m=\u001b[39menv)\n",
      "File \u001b[1;32mc:\\CEIA\\RL\\8pool-with-reinforcement-learning\\game.py:271\u001b[0m, in \u001b[0;36mGAME.step\u001b[1;34m(self, actions, rewards_function)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, actions, rewards_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    270\u001b[0m     angulo, forca \u001b[38;5;241m=\u001b[39m actions\n\u001b[1;32m--> 271\u001b[0m     informations  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mangulo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforca\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m     informations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_step(informations)\n\u001b[0;32m    274\u001b[0m     terminations \u001b[38;5;241m=\u001b[39m informations\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperdeu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m informations\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mganhou\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m informations\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwinner\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\CEIA\\RL\\8pool-with-reinforcement-learning\\utils\\Table.py:285\u001b[0m, in \u001b[0;36mTable.step\u001b[1;34m(self, angulo, intensidade)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexec_physics():\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraw_game:\n\u001b[1;32m--> 285\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m############################## REALIZAR as REGRAS DE SINUCA APOS A JOGADA ##############################\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minformations[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbolas\n",
      "File \u001b[1;32mc:\\CEIA\\RL\\8pool-with-reinforcement-learning\\utils\\Table.py:258\u001b[0m, in \u001b[0;36mTable.draw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;66;03m# Se não houver imagem, desenha a bola como um círculo de cor\u001b[39;00m\n\u001b[0;32m    256\u001b[0m         pygame\u001b[38;5;241m.\u001b[39mdraw\u001b[38;5;241m.\u001b[39mcircle(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen, bola\u001b[38;5;241m.\u001b[39mcor, (\u001b[38;5;28mint\u001b[39m(bola\u001b[38;5;241m.\u001b[39mposicao[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()), \u001b[38;5;28mint\u001b[39m(bola\u001b[38;5;241m.\u001b[39mposicao[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem())), \u001b[38;5;28mint\u001b[39m(bola\u001b[38;5;241m.\u001b[39mraio))\n\u001b[1;32m--> 258\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtaco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39muse_clock:\n",
      "File \u001b[1;32mc:\\CEIA\\RL\\8pool-with-reinforcement-learning\\utils\\Cue.py:86\u001b[0m, in \u001b[0;36mCue.draw\u001b[1;34m(self, screen)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw\u001b[39m(\u001b[38;5;28mself\u001b[39m, screen: pygame\u001b[38;5;241m.\u001b[39mSurface):\n\u001b[0;32m     80\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03m    Desenha o taco e as linhas pontilhadas na direção da bola até o fim da mesa.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m        screen (pygame.Surface): A superfície de jogo onde os elementos serão desenhados.\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_enable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlance_travado:\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;66;03m# Posição da bola branca\u001b[39;00m\n\u001b[0;32m     89\u001b[0m             x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtable\u001b[38;5;241m.\u001b[39mbola_branca\u001b[38;5;241m.\u001b[39mposicao\n",
      "File \u001b[1;32mc:\\CEIA\\RL\\8pool-with-reinforcement-learning\\utils\\Cue.py:75\u001b[0m, in \u001b[0;36mCue.is_enable\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03mVerifica se o taco está pronto para ser usado, ou seja, se a bola branca está parada.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m    bool: True se a bola branca estiver parada, indicando que o taco pode ser usado.\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ball \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtable\u001b[38;5;241m.\u001b[39mbolas:\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mball\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvelocidade\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m :\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "metrics = defaultdict(list)\n",
    "    \n",
    "def run(args: Args):\n",
    "    args.batch_size = int(args.num_envs * args.num_steps)\n",
    "    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "    args.num_iterations = args.total_timesteps // args.batch_size\n",
    "    args.run_name = f\"__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "\n",
    "    writer = SummaryWriter(f\"runs/{args.run_name}\")\n",
    "    writer.add_text(\n",
    "        \"hyperparameters\",\n",
    "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    "    )\n",
    "\n",
    "    # seeding\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "    # env setup\n",
    "    agent = Agent(action_dim=2,model_args=args.model_args)\n",
    "    agent.load_state_dict(torch.load('agent_big.pth'))\n",
    "    agent.to(device)\n",
    "\n",
    "    \n",
    "    \n",
    "    envs = ambientes(n=args.num_envs, agent=agent,rewards_function=rewards_function)\n",
    "\n",
    "\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
    "\n",
    "\n",
    "    # ALGO Logic: Storage setup\n",
    "    obs = torch.zeros((args.num_steps, args.num_envs) + (15,4)).to(device)\n",
    "    obs_white_ball = torch.zeros((args.num_steps, args.num_envs) + (2,)).to(device)\n",
    "    \n",
    "    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space() ).to(device)\n",
    "    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # start the game\n",
    "    global_step = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #next_obs, _ = envs.reset(seed=args.seed)\n",
    "    \n",
    "    next_obs, next_obs_white = envs.reset()\n",
    "    next_done = torch.zeros(args.num_envs).to(device)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    pbar = tqdm(range(1, args.num_iterations + 1), desc=\"Iterations\")\n",
    "    for iteration in pbar:\n",
    "        # annealing the rate if instructed to do so.\n",
    "        if args.anneal_lr:\n",
    "            frac = 1.0 - (iteration - 1.0) / args.num_iterations\n",
    "            lrnow = frac * args.learning_rate\n",
    "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "        for step in range(0, args.num_steps):\n",
    "            global_step += args.num_envs\n",
    "            obs[step] = next_obs\n",
    "            obs_white_ball[step] = next_obs_white\n",
    "            \n",
    "            dones[step] = next_done\n",
    "\n",
    "            # action logic\n",
    "            with torch.no_grad():\n",
    "                action, logprob, _, value = agent.get_action_and_value(next_obs, next_obs_white)\n",
    "                values[step] = value.flatten()\n",
    "                \n",
    "            actions[step] = action\n",
    "            logprobs[step] = logprob\n",
    "\n",
    "            \n",
    "            # TODO: transform ação dada a bola para angulo\n",
    "            \n",
    "            \n",
    "            # execute the game and log data.\n",
    "            (next_obs,next_obs_white), infos, next_done, reward = envs.step(action)\n",
    "\n",
    "            # \n",
    "\n",
    "            rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "            next_obs, next_done , next_obs_white = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device), torch.tensor(next_obs_white).to(device)\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # TODO: IMPLEMENTAR NO AMBIENTE\n",
    "            \n",
    "            #if \"final_info\" in infos:\n",
    "            #    for info in infos[\"final_info\"]:\n",
    "            #        if info and \"episode\" in info:\n",
    "            #            writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "            #            writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "            #            metrics[\"charts/episodic_return\"].append(info[\"episode\"][\"r\"])\n",
    "            #            metrics[\"charts/episodic_length\"].append(info[\"episode\"][\"l\"])\n",
    "            #            pbar.set_postfix_str(f\"step={global_step}, return={info['episode']['r']}\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \"\"\"\n",
    "            PASS OK\n",
    "        \"\"\"  \n",
    "\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # bootstrap value if not done\n",
    "            next_value = agent.get_value(next_obs,next_obs_white).reshape(1, -1)\n",
    "            advantages = torch.zeros_like(rewards).to(device)\n",
    "            lastgaelam = 0\n",
    "\n",
    "            # Calculate returns and advantages using GAE\n",
    "            for t in reversed(range(args.num_steps)):\n",
    "                if t == args.num_steps - 1:\n",
    "                    nextnonterminal = 1.0 - next_done\n",
    "                    nextvalues = next_value\n",
    "                else:\n",
    "                    nextnonterminal = 1.0 - dones[t + 1]\n",
    "                    nextvalues = values[t + 1]\n",
    "                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * lastgaelam\n",
    "            returns = advantages + values\n",
    "\n",
    "        # flatten the batch\n",
    "        \n",
    "        b_obs = obs.reshape((-1,) + envs.single_observation_space())\n",
    "        b_obs_white = obs_white_ball.reshape((-1,) + envs.single_observation_space_white()).squeeze(1)\n",
    "        \n",
    "        \n",
    "        b_logprobs = logprobs.reshape(-1)\n",
    "        b_actions = actions.reshape((-1,) + envs.single_action_space())\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Optimizing the policy and value network\n",
    "        b_inds = np.arange(args.batch_size)\n",
    "        clipfracs = []\n",
    "        for epoch in range(args.update_epochs):\n",
    "            np.random.shuffle(b_inds)\n",
    "            for start in range(0, args.batch_size, args.minibatch_size):\n",
    "                end = start + args.minibatch_size\n",
    "                mb_inds = b_inds[start:end]                \n",
    "\n",
    "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_obs_white[mb_inds], b_actions[mb_inds])\n",
    "                \n",
    "                \n",
    "                logratio = b_logprobs[mb_inds] - newlogprob\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "\n",
    "                mb_advantages = b_advantages[mb_inds]\n",
    "                if args.norm_adv:\n",
    "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = mb_advantages * ratio\n",
    "                pg_loss2 = mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "                pg_loss = torch.min(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                newvalue = newvalue.view(-1)\n",
    "                if args.clip_vloss:\n",
    "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                        newvalue - b_values[mb_inds],\n",
    "                        -args.clip_coef,\n",
    "                        args.clip_coef,\n",
    "                    )\n",
    "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "                else:\n",
    "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "                # Entropy Loss\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss + args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                \n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "            if args.target_kl is not None and approx_kl > args.target_kl:\n",
    "                break\n",
    "\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "        \n",
    "        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "        metrics[\"losses/value_loss\"].append(v_loss.item())\n",
    "        metrics[\"losses/policy_loss\"].append(pg_loss.item())\n",
    "        metrics[\"losses/entropy\"].append(entropy_loss.item())\n",
    "        metrics[\"losses/SPS\"].append(int(global_step / (time.time() - start_time)))\n",
    "        \n",
    "        \n",
    "        if args.save_model:\n",
    "            \n",
    "            model_path = f\"runs/{args.run_name}/{args.exp_name}_model_iteration_{iteration}\"\n",
    "            torch.save(agent.state_dict(), model_path)\n",
    "            print('epoch', epoch, 'saving model at\n",
    "        \n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    if args.save_model:\n",
    "        model_path = f\"runs/{args.run_name}/{args.exp_name}_model\"\n",
    "        torch.save(agent.state_dict(), model_path)\n",
    "\n",
    "    #envs.close()\n",
    "    \"\"\"\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    # final evaluation\n",
    "    eval_episodes = 3\n",
    "    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, 0, True, args.run_name)])\n",
    "    agent.eval()\n",
    "    obs, _ = envs.reset()\n",
    "    episodic_returns = []\n",
    "    while len(episodic_returns) < eval_episodes:\n",
    "        action, logprob, entropy, value = agent.get_action_and_value(torch.Tensor(obs).to(device))\n",
    "        action = action.cpu().numpy()\n",
    "        next_obs, _, _, _, infos = envs.step(action)\n",
    "        obs = next_obs\n",
    "        if \"final_info\" in infos:\n",
    "            for info in infos[\"final_info\"]:\n",
    "                if \"episode\" not in info:\n",
    "                    continue\n",
    "                print(f\"eval_episode={len(episodic_returns)}, episodic_return={info['episode']['r']}\")\n",
    "                episodic_returns += [info[\"episode\"][\"r\"]]\n",
    "    \n",
    "    return metrics\n",
    "    \"\"\"\n",
    "\n",
    "cfg.use_clock = False\n",
    "run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mmetrics\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcharts/episodic_return\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(metrics[\"charts/episodic_return\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics[\"losses/policy_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics[\"losses/value_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics[\"losses/entropy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
