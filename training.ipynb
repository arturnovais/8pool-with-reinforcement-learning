{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.11.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from game import GAME\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GAME(draw=True)  \n",
    "observations = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_function(information):\n",
    "    colisoes = information['colisoes']\n",
    "    bolas_caidas = information['bolas_caidas']\n",
    "    perdeu = information['perdeu']\n",
    "    ganhou = information['ganhou']\n",
    "    joga_novamente = information['joga_novamente']\n",
    "    bolas_jogador = information.get('bolas_jogador', [])\n",
    "    bolas_adversario = information.get('bolas_adversario', [])\n",
    "    winner = information.get('winner', None)\n",
    "    penalizado = information.get('penalizado', False)\n",
    "\n",
    "    rewards = 0\n",
    "\n",
    "    if joga_novamente:\n",
    "        rewards += 1\n",
    "\n",
    "    if perdeu:\n",
    "        rewards -= 1.5\n",
    "\n",
    "    if ganhou:\n",
    "        rewards += 1.5\n",
    "\n",
    "    if penalizado:\n",
    "        rewards -= 1\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.table.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state (tensor([[0.0137, 0.5000, 1.0000, 0.0000],\n",
      "        [0.7708, 0.2246, 1.0000, 0.0000],\n",
      "        [0.6730, 0.4139, 1.0000, 0.0000],\n",
      "        [0.8060, 0.4679, 1.0000, 0.0000],\n",
      "        [0.7746, 0.8149, 1.0000, 0.0000],\n",
      "        [0.8508, 0.5478, 1.0000, 0.0000],\n",
      "        [0.4874, 0.4273, 1.0000, 0.0000],\n",
      "        [0.7255, 0.6092, 1.0000, 0.0000],\n",
      "        [0.7863, 0.5857, 1.0000, 0.0000],\n",
      "        [0.7450, 0.8495, 1.0000, 0.0000],\n",
      "        [0.7739, 0.5051, 1.0000, 0.0000],\n",
      "        [0.8250, 0.6250, 1.0000, 0.0000],\n",
      "        [0.8031, 0.5470, 1.0000, 0.0000],\n",
      "        [0.7468, 0.0736, 1.0000, 0.0000],\n",
      "        [0.7899, 0.1778, 1.0000, 0.0000]]), tensor([0.2000, 0.5000]))\n",
      "termination False\n",
      "rewards 0\n"
     ]
    }
   ],
   "source": [
    "## jogador faz a jogada\n",
    "\n",
    "\n",
    "env.iniciou_jogada = False\n",
    "while not env.iniciou_jogada:\n",
    "    env.table.draw()\n",
    "\n",
    "obs, information, terminations, rewards = env.step((env.iniciou_jogada_angulo, \n",
    "                                                    env.inicou_jogada_intensidade), rewards_function=rewards_function)\n",
    "\n",
    "print('state', obs)\n",
    "print('termination', terminations)\n",
    "print('rewards', rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando jogada True\n",
      "Recompensa da jogada: 0\n"
     ]
    }
   ],
   "source": [
    "# computador faz a jogada\n",
    "print(\"Iniciando jogada\", env.jogador_atual)\n",
    "\n",
    "\n",
    "\n",
    "angulo = 10\n",
    "intensidade = 0.9\n",
    "\n",
    "env.iniciou_jogada = False\n",
    "obs, information, terminations, rewards = env.step( (angulo, intensidade),  rewards_function=rewards_function)\n",
    "\n",
    "print(\"Recompensa da jogada:\", rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Começando treinamento\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gymnasium as gym\n",
    "#from stable_baselines3 import PPO \n",
    "#from stable_baselines3.common.env_checker import check_env\n",
    "#from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, num_layers, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        # make a linear\n",
    "        self.embedding = nn.Linear(4, embed_dim)\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer( d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        \n",
    "        self.mlp = nn.Sequential( \n",
    "                nn.Linear(embed_dim, ff_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(ff_dim, 3)\n",
    "        )\n",
    "        \n",
    "        self.head_position = nn.Tanh()\n",
    "        self.head_intensity = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, bola_branca):\n",
    "        \n",
    "        # adiciona a dimensão da bolinha branca -> batch, (x,y,1,-1)\n",
    "        b = bola_branca.shape[0]\n",
    "        \n",
    "        t_concat = torch.zeros(b,2, device=bola_branca.device)\n",
    "        t_concat[:,0] = 1\n",
    "        t_concat[:,1] = -1\n",
    "        bola_branca = torch.concat( (bola_branca , t_concat), dim=-1).unsqueeze(1)\n",
    "        \n",
    "        x = torch.concat((bola_branca,x),dim=1)\n",
    "        \n",
    "                \n",
    "        \n",
    "        x = self.embedding(x)        \n",
    "        \n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "    \n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        # bola branca é nosso Value\n",
    "        x = x[:,0,:]        \n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        \n",
    "        # transforma o x em angulo e intensidade\n",
    "        position = self.head_position(x[:,:2])\n",
    "        intensity = self.head_intensity(x[:,2:])\n",
    "        \n",
    "        x_sin = position[:,0:1]\n",
    "        x_cos = position[:,1:2]\n",
    "        \n",
    "        angles_deg = torch.rad2deg(torch.atan2(x_sin, x_cos))\n",
    "        angles_deg = torch.where(angles_deg < 0, angles_deg + 360, angles_deg)\n",
    "        \n",
    "        \n",
    "        # concatena o resultado\n",
    "        return torch.concat((angles_deg,intensity),dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "# Parameters\n",
    "embed_dim = 128         # Embedding dimension\n",
    "num_heads = 8           # Number of attention heads\n",
    "ff_dim = embed_dim * 2  # Feedforward dimension\n",
    "num_layers = 2          # Number of encoder layers\n",
    "dropout = 0.1           # Dropout rate\n",
    "\n",
    "\n",
    "model = TransformerEncoder(\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    ff_dim=ff_dim,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[278.0536,   0.6743],\n",
       "        [290.7160,   0.6681]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, bola_branca = (torch.tensor([\n",
    "        [[0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.8250, 0.3750, 1.0000, 1.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
    "        [[0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.8250, 0.3750, 1.0000, 1.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000]]\n",
    "        ]),\n",
    "                      \n",
    " torch.tensor([[0.2000, 0.5000],[0.2000, 0.5000]]))\n",
    "\n",
    "model(state,bola_branca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # Convolução para os dados de posição (x, y)\n",
    "        self.conv_position = nn.Conv1d(\n",
    "            in_channels=1, out_channels=16, kernel_size=2, stride=2\n",
    "        )\n",
    "        \n",
    "        # Convolução para os dados de estado (se é do adversário e se está na mesa)\n",
    "        self.conv_state = nn.Conv1d(\n",
    "            in_channels=1, out_channels=16, kernel_size=2, stride=2\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(482, 128)  # Ajustar com base nas características concatenadas\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Garantir que o tensor de entrada seja do tamanho correto\n",
    "        assert x.numel() == 62, f\"Expected input of size 62, got {x.numel()} elements.\"\n",
    "\n",
    "        # Separar dados de posição e estado\n",
    "        position_data = x[0:60].reshape(-1, 4)[:, 0:2].reshape(1, 1, -1)  # Extrai (x, y) pares consecutivos\n",
    "        state_data = x[0:60].reshape(-1, 4)[:, 2:4].reshape(1, 1, -1)  # Extrai (adversário, na mesa)\n",
    "        white_ball_data = x[60:62]  # Dados da bola branca (x, y)\n",
    "\n",
    "        # Aplicar convoluções\n",
    "        position_features = torch.relu(self.conv_position(position_data))\n",
    "        state_features = torch.relu(self.conv_state(state_data))\n",
    "\n",
    "        # Flatten convoluções\n",
    "        position_features = position_features.view(-1)  # Achatar\n",
    "        state_features = state_features.view(-1)        # Achatar\n",
    "        \n",
    "        \n",
    "        #print(f'Position features shape: {position_features.shape}')\n",
    "        #print(f'State features shape: {state_features.shape}')\n",
    "\n",
    "        # Concatenar características das convoluções e dados da bola branca\n",
    "        combined_features = torch.cat([position_features, state_features, white_ball_data], dim=0)\n",
    "        \n",
    "        #print(f'Combined features shape: {combined_features.shape}')\n",
    "\n",
    "        # Passar pelas fully connected layers\n",
    "        x = torch.relu(self.fc1(combined_features))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.config as cfg\n",
    "\n",
    "## iniciar o ambiente com a configuração do jogo com apenas uma bola na mesa. \n",
    "# - A bola branca na posição (0, 0)\n",
    "# - se não bater na bola, acabou a jogada\n",
    "\n",
    "cfg.raio_buraco = 13\n",
    "env.reset()\n",
    "env.table.bolas = env.table.bolas[1:2] + [env.table.bola_branca]\n",
    "numero_bola = env.table.bolas[0].numero\n",
    "env.numero_bolas = [[ numero_bola ], [ numero_bola ]]\n",
    "\n",
    "#env.table.draw()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8250, 0.3750, 1.0000, 1.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000]]),\n",
       " tensor([0.2000, 0.5000]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_bolas, state_bola_branca = env.get_observations()\n",
    "# shuffle the balls\n",
    "state_bolas = state_bolas[torch.randperm(state_bolas.size()[0])]\n",
    "state_bolas , state_bola_branca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.99  # Fator de desconto\n",
    "        self.epsilon = 1.0  # Taxa de exploração\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.batch_size = 64\n",
    "\n",
    "        self.model = DQN(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # Exploração: escolher valores aleatórios dentro do intervalo permitido\n",
    "            angle = np.random.uniform(0, 360)  # Ângulo contínuo entre 0° e 360°\n",
    "            intensity = np.random.uniform(0, 1)  # Intensidade contínua entre 0 e 1\n",
    "            #print(f\"Explorando: Ângulo={angle:.2f}, Intensidade={intensity:.2f}\")\n",
    "            return np.array([angle, intensity])\n",
    "            \n",
    "        else:\n",
    "            # Exploração: usar a política aprendida\n",
    "            state = torch.FloatTensor(state)\n",
    "            q_values = self.model(state)\n",
    "            action = q_values.detach().numpy()  # Retorna o vetor [ângulo, intensidade]\n",
    "            #print(f\"Explotando: Ângulo={action[0]:.2f}, Intensidade={action[1]:.2f}\")\n",
    "            return action\n",
    "\n",
    "\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            next_state = torch.FloatTensor(next_state).view(-1)  # Garantir formato correto\n",
    "            \n",
    "            if not done:\n",
    "                target += self.gamma * torch.max(self.model(next_state).detach())\n",
    "            \n",
    "            state = torch.FloatTensor(state).view(-1)  # Garantir formato correto\n",
    "            target_f = self.model(state)\n",
    "\n",
    "            # Calculando a perda diretamente\n",
    "            loss = self.criterion(target_f, torch.FloatTensor(action))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnvWrapper(GAME(draw=False))\n",
    "\n",
    "# Inicializar o tamanho do estado e das ações\n",
    "state_size = 62  # Tamanho do estado processado\n",
    "action_size = 2  # Ângulo e Intensidade\n",
    "\n",
    "# Inicializar o agente\n",
    "agent = DQNAgent(state_size=state_size, action_size=action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnvWrapper(GAME(draw=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/500 - Total Reward: -105 - Moving Average: 30.00\n",
      "Episode 2/500 - Total Reward: 9970 - Moving Average: 10105.00\n",
      "Episode 3/500 - Total Reward: 550 - Moving Average: 685.00\n",
      "Episode 4/500 - Total Reward: 3565 - Moving Average: 3700.00\n",
      "Episode 5/500 - Total Reward: 3235 - Moving Average: 3370.00\n",
      "Episode 6/500 - Total Reward: -100 - Moving Average: 31.50\n",
      "Episode 7/500 - Total Reward: 670 - Moving Average: 805.00\n",
      "Episode 8/500 - Total Reward: -30 - Moving Average: 105.00\n",
      "Episode 9/500 - Total Reward: 3160 - Moving Average: 3263.50\n",
      "Episode 10/500 - Total Reward: 250 - Moving Average: 227.50\n",
      "Episode 11/500 - Total Reward: 1150 - Moving Average: 1285.00\n",
      "Episode 12/500 - Total Reward: 3225 - Moving Average: 3360.00\n",
      "Episode 13/500 - Total Reward: 55 - Moving Average: 186.50\n",
      "Episode 14/500 - Total Reward: 5980 - Moving Average: 5957.50\n",
      "Episode 15/500 - Total Reward: 9370 - Moving Average: 9347.50\n",
      "Episode 16/500 - Total Reward: 14950 - Moving Average: 14927.50\n",
      "Episode 17/500 - Total Reward: 2695 - Moving Average: 2830.00\n",
      "Episode 18/500 - Total Reward: 4745 - Moving Average: 4877.00\n",
      "Episode 19/500 - Total Reward: 4690 - Moving Average: 4667.50\n",
      "Episode 20/500 - Total Reward: 445 - Moving Average: 422.50\n",
      "Episode 21/500 - Total Reward: 4330 - Moving Average: 4465.00\n",
      "Episode 22/500 - Total Reward: 1615 - Moving Average: 1592.50\n",
      "Episode 23/500 - Total Reward: 1170 - Moving Average: 1299.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m angle, intensity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(action[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mfloat\u001b[39m(action[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Realizar a ação no ambiente\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m obs, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mangle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintensity\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Usar diretamente o estado retornado\u001b[39;00m\n\u001b[1;32m     24\u001b[0m next_state \u001b[38;5;241m=\u001b[39m obs\n",
      "Cell \u001b[0;32mIn[113], line 25\u001b[0m, in \u001b[0;36mCustomEnvWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     24\u001b[0m     angle, intensity \u001b[38;5;241m=\u001b[39m action\n\u001b[0;32m---> 25\u001b[0m     obs, info, done, reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mangle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintensity\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrewards_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Processar cada parte de 'obs'\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     obs_main \u001b[38;5;241m=\u001b[39m obs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()  \u001b[38;5;66;03m# Primeiro tensor (posição das bolas)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/VsCode/8pool-with-reinforcement-learning/game.py:271\u001b[0m, in \u001b[0;36mGAME.step\u001b[0;34m(self, actions, rewards_function)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, actions, rewards_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    270\u001b[0m     angulo, forca \u001b[38;5;241m=\u001b[39m actions\n\u001b[0;32m--> 271\u001b[0m     informations  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mangulo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforca\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     informations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_step(informations)\n\u001b[1;32m    274\u001b[0m     terminations \u001b[38;5;241m=\u001b[39m informations\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperdeu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m informations\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mganhou\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m informations\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwinner\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/VsCode/8pool-with-reinforcement-learning/utils/Table.py:270\u001b[0m, in \u001b[0;36mTable.step\u001b[0;34m(self, angulo, intensidade)\u001b[0m\n\u001b[1;32m    266\u001b[0m forca \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtaco\u001b[38;5;241m.\u001b[39mcalcular_forca(intensidade, angulo)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbola_branca\u001b[38;5;241m.\u001b[39maplicar_forca(forca)\n\u001b[0;32m--> 270\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_physics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraw_game:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraw()\n",
      "File \u001b[0;32m~/Documents/VsCode/8pool-with-reinforcement-learning/utils/Table.py:166\u001b[0m, in \u001b[0;36mTable.exec_physics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbolas)):\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbolas)):\n\u001b[0;32m--> 166\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetecttor_colisao\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetectar_colisao_bolas\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbolas\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbolas\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    167\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minformations[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolisoes\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbolas[i], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbolas[j]))\n\u001b[1;32m    168\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetecttor_colisao\u001b[38;5;241m.\u001b[39mresolver_colisao_bolas(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbolas[i], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbolas[j])\n",
      "File \u001b[0;32m~/Documents/VsCode/8pool-with-reinforcement-learning/utils/CollisionDetector.py:33\u001b[0m, in \u001b[0;36mCollisionDetector.detectar_colisao_bolas\u001b[0;34m(self, bola1, bola2)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    Verifica se duas bolas colidiram.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    A colisão é detectada se a distância entre os centros das bolas for menor ou igual à soma de seus raios.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m        bool: True se as bolas colidirem, False caso contrário.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     dist_entre_bolas \u001b[38;5;241m=\u001b[39m bola1\u001b[38;5;241m.\u001b[39mposicao \u001b[38;5;241m-\u001b[39m bola2\u001b[38;5;241m.\u001b[39mposicao\n\u001b[0;32m---> 33\u001b[0m     distancia \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist_entre_bolas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m distancia \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m (bola1\u001b[38;5;241m.\u001b[39mraio \u001b[38;5;241m+\u001b[39m bola2\u001b[38;5;241m.\u001b[39mraio)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03mdef resolver_colisao_bolas(self, bola1: Ball, bola2: Ball):\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m        )\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Definir o número de episódios para o treinamento\n",
    "episodes = 500\n",
    "\n",
    "window_size = 10  # Tamanho da janela para a média móvel\n",
    "reward_window = deque(maxlen=window_size) \n",
    "\n",
    "for e in range(episodes):\n",
    "    state, _ = env.reset()  # Obter o estado inicial como array NumPy\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    c = 0  # Contador para print debug\n",
    "\n",
    "    while not done:\n",
    "        c += 1\n",
    "        # Escolher uma ação contínua\n",
    "        action = agent.act(state)  # Retorna [angle, intensity]\n",
    "        angle, intensity = float(action[0]), float(action[1])\n",
    "\n",
    "        # Realizar a ação no ambiente\n",
    "        obs, reward, done, _ = env.step((angle, intensity))\n",
    "        \n",
    "        # Usar diretamente o estado retornado\n",
    "        next_state = obs\n",
    "\n",
    "        # Memorizar a transição\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        # Atualizar o estado atual\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        # Debug a cada 10 iterações\n",
    "        #if c % 10 == 0:\n",
    "            #print(f'Action: Angle={angle}, Intensity={intensity}, Reward={reward}, Total Reward={total_reward}')\n",
    "            \n",
    "            \n",
    "        reward_window.append(total_reward)\n",
    "\n",
    "        if done:\n",
    "            #print()\n",
    "            moving_avg = sum(reward_window) / len(reward_window)\n",
    "\n",
    "            print(f\"Episode {e+1}/{episodes} - Total Reward: {total_reward} - Moving Average: {moving_avg:.2f}\")\n",
    "            #print()\n",
    "            break\n",
    "\n",
    "    # Treinar o agente\n",
    "    agent.replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "lista[2:-1: 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo salvo em dqn_agent_model6.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "save_path = \"dqn_agent_model6.pth\"\n",
    "\n",
    "# Salvar o modelo\n",
    "torch.save({\n",
    "    'model_state_dict': agent.model.state_dict(),\n",
    "    'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "    'epsilon': agent.epsilon,  # Salvar o epsilon atual\n",
    "}, save_path)\n",
    "\n",
    "print(f\"Modelo salvo em {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo carregado com sucesso!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ys/qvr7rwln3yjg1zwjrrspn9gc0000gn/T/ipykernel_62796/3586500898.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n"
     ]
    }
   ],
   "source": [
    "model_path = \"dqn_agent_model.pth\"\n",
    "\n",
    "# Inicializar o agente\n",
    "agent = DQNAgent(state_size=62, action_size=2)\n",
    "\n",
    "# Carregar os pesos do modelo e do otimizador\n",
    "checkpoint = torch.load(model_path)\n",
    "agent.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "agent.epsilon = checkpoint['epsilon']  # Restaurar o epsilon atual\n",
    "\n",
    "print(\"Modelo carregado com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (fc1): Linear(in_features=62, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GAME(draw=False)  \n",
    "observations = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando jogada 0\n",
      "Ação realizada: Ângulo=114.47925567626953, Intensidade=0.5772977471351624\n",
      "Recompensa da jogada: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# O agente escolhe uma ação\u001b[39;00m\n\u001b[1;32m     11\u001b[0m state \u001b[38;5;241m=\u001b[39m obs  \u001b[38;5;66;03m# Assumimos que 'obs' é o estado retornado do ambiente\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Agente escolhe o ângulo e a intensidade\u001b[39;00m\n\u001b[1;32m     13\u001b[0m angle, intensity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(action[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mfloat\u001b[39m(action[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Realizar a jogada no ambiente\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 30\u001b[0m, in \u001b[0;36mDQNAgent.act\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([angle, intensity])\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Exploração: usar a política aprendida\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(state)\n\u001b[1;32m     32\u001b[0m     action \u001b[38;5;241m=\u001b[39m q_values\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Retorna o vetor [ângulo, intensidade]\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "print(\"Iniciando jogada\", env.jogador_atual)\n",
    "\n",
    "done = False  # Variável para controlar o término do jogo\n",
    "total_reward = 0  # Variável para acompanhar a recompensa total\n",
    "\n",
    "while not done:\n",
    "    while not env.iniciou_jogada:\n",
    "        env.table.draw()  # Renderizar o estado atual do jogo (se draw=True)\n",
    "\n",
    "    # O agente escolhe uma ação\n",
    "    state = obs  # Assumimos que 'obs' é o estado retornado do ambiente\n",
    "    action = agent.act(state)  # Agente escolhe o ângulo e a intensidade\n",
    "    angle, intensity = float(action[0]), float(action[1])\n",
    "\n",
    "    # Realizar a jogada no ambiente\n",
    "    obs, information, terminations, rewards = env.step(\n",
    "        (angle, intensity), \n",
    "        rewards_function=rewards_function\n",
    "    )\n",
    "\n",
    "    env.iniciou_jogada = False  # Resetar o estado da jogada\n",
    "    total_reward += rewards  # Atualizar a recompensa total\n",
    "    print(f\"Ação realizada: Ângulo={angle}, Intensidade={intensity}\")\n",
    "    print(f\"Recompensa da jogada: {rewards}\")\n",
    "\n",
    "    # Verificar se o jogo terminou\n",
    "    done = terminations\n",
    "\n",
    "print(f\"Fim do jogo! Recompensa total: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
