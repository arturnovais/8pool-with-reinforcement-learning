{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from game import GAME\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from torch.distributions import Normal\n",
    "from utils import config as cfg\n",
    "\n",
    "\n",
    "from curriculum.reset import reset_random_one_ball\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from models.model_transformers import Model_args, TransformerValueModel, TransformersAtor\n",
    "\n",
    "from models.agent import Agent\n",
    "\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from curriculum.reset import reset_random_one_ball\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import math\n",
    "import utils.config as cfg\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_function(information):\n",
    "    colisoes = information['colisoes']\n",
    "    bolas_caidas = information['bolas_caidas']\n",
    "    perdeu = information['perdeu']\n",
    "    ganhou = information['ganhou']\n",
    "    joga_novamente = information['joga_novamente']\n",
    "    bolas_jogador = information.get('bolas_jogador', [])\n",
    "    bolas_adversario = information.get('bolas_adversario', [])\n",
    "    winner = information.get('winner', None)\n",
    "    penalizado = information.get('penalizado', False)\n",
    "\n",
    "    rewards = 1\n",
    "    \n",
    "    #print(information)\n",
    "    \n",
    "    if joga_novamente:\n",
    "        rewards += 1\n",
    "\n",
    "    if perdeu:\n",
    "        rewards -= 1.5\n",
    "\n",
    "    if ganhou:\n",
    "        rewards += 1.5\n",
    "\n",
    "    if penalizado:\n",
    "        rewards -= 1\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GAME(draw=True)  \n",
    "observations = env.reset()\n",
    "\n",
    "\n",
    "def reset_random_one_ball2(env,raio_buraco = 13, bola_ = (0,0), bola_branca=(0,0)):\n",
    "    ## iniciar o ambiente com a configuração do jogo com apenas uma bola na mesa. \n",
    "    # - A bola branca em uma posição aleatória\n",
    "    # - se não bater na bola, acabou a jogada\n",
    "\n",
    "    cfg.raio_buraco = raio_buraco\n",
    "    \n",
    "    epson = raio_buraco*2\n",
    "\n",
    "    env.reset()\n",
    "    env.table.bolas = env.table.bolas[:2] + [env.table.bola_branca]\n",
    "    numero_bola = env.table.bolas[1].numero\n",
    "    \n",
    "    \n",
    "    for _ ,  bola in enumerate(env.table.bolas[1:]):\n",
    "        bola.posicao[1] = bola_[0] + env.table.x_start\n",
    "        bola.posicao[0] = bola_[1] + env.table.y_start\n",
    "    \n",
    "    env.table.bola_branca.posicao[0] = bola_branca[0]  + env.table.x_start\n",
    "    env.table.bola_branca.posicao[1] = bola_branca[1]  + env.table.y_start\n",
    "    \n",
    "    env.numero_bolas = [[numero_bola ], []]\n",
    "    env.player=0\n",
    "    \n",
    "    \n",
    "    return env.get_observations()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## jogador faz a jogada\n",
    "\n",
    "reset_random_one_ball2(env)\n",
    "\n",
    "env.iniciou_jogada = False\n",
    "\n",
    "#while not env.iniciou_jogada:\n",
    "#    env.table.draw()\n",
    "    \n",
    "env.inicou_jogada_intensidade = 0.42\n",
    "\n",
    "\n",
    "\n",
    "env.iniciou_jogada_angulo = 0.45\n",
    "\n",
    "obs, information, terminations, rewards = env.step((env.iniciou_jogada_angulo, \n",
    "                                                    env.inicou_jogada_intensidade), rewards_function=rewards_function)\n",
    "\n",
    "\n",
    "\n",
    "#print(env.iniciou_jogada_angulo)\n",
    "\n",
    "#print('state', obs)\n",
    "#print('termination', terminations)\n",
    "#print('rewards', rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcula_angulo(bola_branca, bola_numero):\n",
    "\n",
    "        x, y = bola_branca\n",
    "        x1, y1 = bola_numero \n",
    "\n",
    "        x *= cfg.display_table_width\n",
    "        x1 *= cfg.display_table_width\n",
    "        y *= cfg.display_table_height\n",
    "        y1 *= cfg.display_table_height   \n",
    "\n",
    "\n",
    "        return math.atan2(y1 - y, x1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_mirrored_pool_tables(bola_branca, bola_numero):\n",
    "    \"\"\"\n",
    "    Draws 9 mirrored pool tables based on the given dimensions and ball positions.\n",
    "\n",
    "    Args:\n",
    "        l: Width of the table.\n",
    "        A: Height of the table.\n",
    "        balls: List of ball positions as tuples (x, y).\n",
    "    \"\"\"\n",
    "    l=cfg.display_table_width \n",
    "    A=cfg.display_table_height\n",
    "    posicoes = [ \n",
    "                    (bola_numero[0], bola_numero[1]),\n",
    "                    (-bola_numero[0], bola_numero[1]),\n",
    "                    ( l+(l-bola_numero[0]), bola_numero[1]),#\n",
    "                    (bola_numero[0], -bola_numero[1]),\n",
    "                    (bola_numero[0], A+(A-bola_numero[1])),#\n",
    "                    (-bola_numero[0], A+(A-bola_numero[1])),#\n",
    "                    (-bola_numero[0], -bola_numero[1]),\n",
    "                    ( l+(l-bola_numero[0]), A+(A-bola_numero[1])),\n",
    "                    ( l+(l-bola_numero[0]), -bola_numero[1]),#\n",
    "                ]\n",
    "    \n",
    "    l,A = 1 , 1\n",
    "    posicoes = [ \n",
    "                    (bola_numero[0], bola_numero[1]),\n",
    "                    (-bola_numero[0], bola_numero[1]),\n",
    "                    ( l+(l-bola_numero[0]), bola_numero[1]),#\n",
    "                    (bola_numero[0], -bola_numero[1]),\n",
    "                    (bola_numero[0], A+(A-bola_numero[1])),#\n",
    "                    (-bola_numero[0], A+(A-bola_numero[1])),#\n",
    "                    (-bola_numero[0], -bola_numero[1]),\n",
    "                    ( l+(l-bola_numero[0]), A+(A-bola_numero[1])),\n",
    "                    ( l+(l-bola_numero[0]), -bola_numero[1]),#\n",
    "                ]\n",
    "\n",
    "    for p in posicoes:\n",
    "        yield calcula_angulo(bola_branca, p)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "1 1\n",
      "2 1\n",
      "3 1\n",
      "4 1\n",
      "5 1\n",
      "6 1\n",
      "7 1\n",
      "8 1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import utils.config as cfg\n",
    "import torch\n",
    "\n",
    "\n",
    "cfg.use_clock= False\n",
    "for p in range(9):\n",
    "    \n",
    "    import time\n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "    bola_numero = 200, 400\n",
    "    bola_branca = np.random.uniform(20, cfg.display_width - 20) , np.random.uniform(20, cfg.display_height - 20)\n",
    "    bola_branca = cfg.bola_branca_posicao_inicial\n",
    "    #\n",
    "    state, state_bola_branca = reset_random_one_ball2(env=env, bola_=bola_numero, bola_branca=bola_branca)\n",
    "\n",
    "    \n",
    "    while time.time()  - start < 0.5:\n",
    "       env.table.draw()\n",
    "    \n",
    "    angulo = list(draw_mirrored_pool_tables(state_bola_branca.numpy().tolist(), \n",
    "                            state[1][:2].numpy().tolist()))[p]\n",
    "    \n",
    " \n",
    " \n",
    "    cfg.forca_maxima = 30\n",
    "    intensidade = 1\n",
    "\n",
    "    obs, information, terminations, rewards = env.step( (angulo, intensidade),  rewards_function=rewards_function)\n",
    "    \n",
    "    print(p,rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actions(state, state_bola_branca, bola_numero):\n",
    "    angulos = []\n",
    "    for p in range(9):\n",
    "        angulos.append(list(draw_mirrored_pool_tables(state_bola_branca.numpy().tolist(), \n",
    "                            state[bola_numero][:2].numpy().tolist()))[p])\n",
    "        \n",
    "    return angulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, state_bola_branca = env.reset()\n",
    "angulos = get_actions(state, state_bola_branca, 1)\n",
    "\n",
    "\n",
    "cfg.forca_maxima = 30\n",
    "intensidade = 1\n",
    "\n",
    "\n",
    "obs, information, terminations, rewards = env.step( (angulo, intensidade),  rewards_function=rewards_function)\n",
    "    \n",
    "print(p,rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2.582993311457625,\n",
       " -3.010765255892737,\n",
       " -0.1184899578254459,\n",
       " -1.7955074822620603,\n",
       " 2.582993311457625,\n",
       " 3.010765255892737,\n",
       " -2.3972672625297826,\n",
       " 0.1184899578254459,\n",
       " -0.69473827061313]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_actions(state, state_bola_branca, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Começando treinamento\n",
    "                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo política"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ideia é que tenhamos uma rede neural como política, especificamente um transformers. \n",
    "\n",
    "Queremos que ele passe por cada bola e gere uma representação, no fim a representação do nosso estado será o embedding dabola branca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    exp_name: str = \"PPO\"\n",
    "    \"\"\"the name of this experiment\"\"\"\n",
    "    seed: int = 1\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "    torch_deterministic: bool = True\n",
    "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
    "    cuda: bool = torch.cuda.is_available()\n",
    "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    capture_video: bool = False\n",
    "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
    "    save_model: bool = True\n",
    "    \"\"\"whether to save the final model\"\"\"\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    total_timesteps: int = 500000\n",
    "    \"\"\"total timesteps of the experiments\"\"\"\n",
    "    learning_rate: float = 2.5e-4\n",
    "    \"\"\"the learning rate of the optimizer\"\"\"\n",
    "    num_envs: int = 2\n",
    "    \"\"\"the number of parallel game environments\"\"\"\n",
    "    num_steps: int = 10 # 128\n",
    "    \"\"\"the number of steps to run in each environment per policy rollout\"\"\"\n",
    "    anneal_lr: bool = True\n",
    "    \"\"\"Toggle learning rate annealing for policy and value networks\"\"\"\n",
    "    gamma: float = 0.99\n",
    "    \"\"\"the discount factor gamma\"\"\"\n",
    "    gae_lambda: float = 0.95\n",
    "    \"\"\"the lambda for the general advantage estimation\"\"\"\n",
    "    num_minibatches: int = 4\n",
    "    \"\"\"the number of mini-batches\"\"\"\n",
    "    update_epochs: int = 4\n",
    "    \"\"\"the K epochs to update the policy\"\"\"\n",
    "    norm_adv: bool = True\n",
    "    \"\"\"Toggles advantages normalization\"\"\"\n",
    "    clip_coef: float = 0.2\n",
    "    \"\"\"the surrogate clipping coefficient\"\"\"\n",
    "    clip_vloss: bool = True\n",
    "    \"\"\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\"\"\"\n",
    "    ent_coef: float = 0.01\n",
    "    \"\"\"coefficient of the entropy\"\"\"\n",
    "    vf_coef: float = 0.5\n",
    "    \"\"\"coefficient of the value function\"\"\"\n",
    "    max_grad_norm: float = 0.5\n",
    "    \"\"\"the maximum norm for the gradient clipping\"\"\"\n",
    "    target_kl: float = None\n",
    "    \"\"\"the target KL divergence threshold\"\"\"\n",
    "\n",
    "    # to be filled in runtime\n",
    "    batch_size: int = 0\n",
    "    \"\"\"the batch size (computed in runtime)\"\"\"\n",
    "    minibatch_size: int = 0\n",
    "    \"\"\"the mini-batch size (computed in runtime)\"\"\"\n",
    "    num_iterations: int = 0\n",
    "    \"\"\"the number of iterations (computed in runtime)\"\"\"\n",
    "    \n",
    "    model_args  = None\n",
    "    \n",
    "    #model_args: ModelArgs = field(default_factory=ModelArgs)\n",
    "\n",
    "args = Args()\n",
    "args.model_args = Model_args(\n",
    "                    embed_dim  = 128,\n",
    "                    num_heads  = 8,\n",
    "                    ff_dim  = 128 * 2,\n",
    "                    num_layers  =  2,\n",
    "                    dropout  = 0.1,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from curriculum.reset import reset_random_one_ball\n",
    "\n",
    "rewards_function\n",
    "\n",
    "class ambientes():\n",
    "    \n",
    "    def __init__(self,n , agent, rewards_function):\n",
    "        self.n = n\n",
    "        self.agent = agent\n",
    "        \n",
    "        self.envs = [ GAME(draw=True) for _ in range(n)]\n",
    "        self.rewards_function = rewards_function\n",
    "        \n",
    " \n",
    "    def format_observations(self, observations : list[tuple] ):\n",
    "        states = []\n",
    "        states_white_ball = []\n",
    "    \n",
    "        for obs in observations:\n",
    "            states += [obs[0].unsqueeze(dim=0)]\n",
    "            states_white_ball += [obs[1].unsqueeze(dim=0)]\n",
    "        return torch.concat(states) , torch.concat(states_white_ball)\n",
    "       \n",
    "    def reset(self):\n",
    "        states = []\n",
    "        for env in self.envs:\n",
    "            states += [reset_random_one_ball(env=env)]\n",
    "        \n",
    "        return self.format_observations(states)\n",
    "            \n",
    "    def single_observation_space(self):\n",
    "        return (15,4)\n",
    "    \n",
    "    def single_observation_space_white(self):\n",
    "        return (1,2)\n",
    "    \n",
    "    def single_action_space(self):\n",
    "        return (2,)\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        angulo =  action[:,0]\n",
    "        intensidade = action[:,1]\n",
    "        \n",
    "        \n",
    "        get_observations, informations , terminations, rewards = [],[],[],[]\n",
    "        \n",
    "        for i, env in enumerate(self.envs):\n",
    "\n",
    "            env_observations, env_informations , env_terminations, env_rewards = env.step( (angulo[i], intensidade[i]), rewards_function =self.rewards_function )\n",
    "            \n",
    "            \n",
    "            if env_terminations:\n",
    "                env_observations = reset_random_one_ball(env=env)\n",
    "\n",
    "            get_observations += [env_observations]\n",
    "            informations += [env_informations]\n",
    "            terminations += [env_terminations]\n",
    "            rewards += [env_rewards]\n",
    "        \n",
    "        \n",
    "        \n",
    "                \n",
    "        return self.format_observations(get_observations), informations , terminations, rewards\n",
    "\n",
    "    \n",
    "    \n",
    "     \n",
    "                \n",
    "#test\n",
    "#agent = Agent(action_dim=2,model_args=args.model_args)\n",
    "#a  =  ambientes(2,agent,rewards_function)\n",
    "#state, balls = a.reset()\n",
    "#action, logprob, _, value = agent.get_action_and_value(state, balls)\n",
    "#print(action)\n",
    "#observations, informations , terminations, rewards = a.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 268\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;124;03m    writer.close()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    return metrics\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    267\u001b[0m cfg\u001b[38;5;241m.\u001b[39muse_clock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 32\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     28\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(action_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,model_args\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmodel_args)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     29\u001b[0m envs \u001b[38;5;241m=\u001b[39m ambientes(n\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_envs, agent\u001b[38;5;241m=\u001b[39magent,rewards_function\u001b[38;5;241m=\u001b[39mrewards_function)\n\u001b[1;32m---> 32\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# ALGO Logic: Storage setup\u001b[39;00m\n\u001b[0;32m     36\u001b[0m obs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((args\u001b[38;5;241m.\u001b[39mnum_steps, args\u001b[38;5;241m.\u001b[39mnum_envs) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m4\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\CEIA\\RL\\8pool-with-reinforcement-learning\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:78\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     66\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     67\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m     68\u001b[0m     betas\u001b[38;5;241m=\u001b[39mbetas,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m     fused\u001b[38;5;241m=\u001b[39mfused,\n\u001b[0;32m     77\u001b[0m )\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[1;32mc:\\CEIA\\RL\\8pool-with-reinforcement-learning\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:371\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    368\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: param_groups}]\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[1;32m--> 371\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\CEIA\\RL\\8pool-with-reinforcement-learning\\.venv\\Lib\\site-packages\\torch\\_compile.py:27\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m disable_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dynamo_disable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m disable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[0;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n",
      "File \u001b[1;32mc:\\CEIA\\RL\\8pool-with-reinforcement-learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\__init__.py:39\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m graph_break_reasons, guard_failures, orig_code_map, reset_frame_count\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Register polyfill functions\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolyfills\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m loader \u001b[38;5;28;01mas\u001b[39;00m _  \u001b[38;5;66;03m# usort: skip # noqa: F401\u001b[39;00m\n\u001b[0;32m     42\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_in_graph\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massume_constant_result\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlookup_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     66\u001b[0m ]\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmanual_seed \u001b[38;5;129;01mis\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mmanual_seed:\n",
      "File \u001b[1;32mc:\\CEIA\\RL\\8pool-with-reinforcement-learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\polyfills\\loader.py:22\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# See also the TYPE_CHECKING block in torch/_dynamo/polyfills/__init__.py\u001b[39;00m\n\u001b[0;32m     15\u001b[0m POLYFILLED_MODULE_NAMES: Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuiltins\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctools\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msys\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m )\n\u001b[1;32m---> 22\u001b[0m POLYFILLED_MODULES: Tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModuleType\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msubmodule\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolyfills\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubmodule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mPOLYFILLED_MODULE_NAMES\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Unregister the builtin functions from _builtin_function_ids to let them to be\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# dispatched with the appropriate VariableTracker type. Otherwise, they will be\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# dispatched with BuiltinVariable if present in _builtin_function_ids.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m polyfill_module \u001b[38;5;129;01min\u001b[39;00m POLYFILLED_MODULES:\n",
      "File \u001b[1;32mc:\\CEIA\\RL\\8pool-with-reinforcement-learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\polyfills\\loader.py:23\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# See also the TYPE_CHECKING block in torch/_dynamo/polyfills/__init__.py\u001b[39;00m\n\u001b[0;32m     15\u001b[0m POLYFILLED_MODULE_NAMES: Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuiltins\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctools\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msys\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m POLYFILLED_MODULES: Tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModuleType\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m---> 23\u001b[0m     \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msubmodule\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolyfills\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m submodule \u001b[38;5;129;01min\u001b[39;00m POLYFILLED_MODULE_NAMES\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Unregister the builtin functions from _builtin_function_ids to let them to be\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# dispatched with the appropriate VariableTracker type. Otherwise, they will be\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# dispatched with BuiltinVariable if present in _builtin_function_ids.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m polyfill_module \u001b[38;5;129;01min\u001b[39;00m POLYFILLED_MODULES:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\CEIA\\RL\\8pool-with-reinforcement-learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\polyfills\\builtins.py:23\u001b[0m\n\u001b[0;32m     13\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menumerate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m ]\n\u001b[0;32m     20\u001b[0m _T \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_T\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;129;43m@substitute_in_graph\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_constant_fold_through\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\CEIA\\RL\\8pool-with-reinforcement-learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\decorators.py:312\u001b[0m, in \u001b[0;36msubstitute_in_graph.<locals>.wrapper\u001b[1;34m(traceable_fn)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m(original_fn) \u001b[38;5;129;01min\u001b[39;00m id_dispatch_map:\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuplicate dispatch rule for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malready registered in VariableBuilder\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms id dispatch map\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 312\u001b[0m rule_map: Dict[Any, Type[VariableTracker]] \u001b[38;5;241m=\u001b[39m \u001b[43mget_torch_obj_rule_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original_fn \u001b[38;5;129;01min\u001b[39;00m rule_map:\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuplicate object \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with different rules: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    316\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPolyfilledFunctionVariable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrule_map[original_fn]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    317\u001b[0m     )\n",
      "File \u001b[1;32mc:\\CEIA\\RL\\8pool-with-reinforcement-learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py:2860\u001b[0m, in \u001b[0;36mget_torch_obj_rule_map\u001b[1;34m()\u001b[0m\n\u001b[0;32m   2858\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m m\u001b[38;5;241m.\u001b[39mitems():  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   2859\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py#\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m k:\n\u001b[1;32m-> 2860\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mload_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2861\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2862\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _module_dir(torch) \u001b[38;5;241m+\u001b[39m k[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch/\u001b[39m\u001b[38;5;124m\"\u001b[39m) :]\n",
      "File \u001b[1;32mc:\\CEIA\\RL\\8pool-with-reinforcement-learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py:2891\u001b[0m, in \u001b[0;36mload_object\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m   2889\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2890\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid obj name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 2891\u001b[0m         val \u001b[38;5;241m=\u001b[39m \u001b[43m_load_obj_from_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2892\u001b[0m     val \u001b[38;5;241m=\u001b[39m unwrap_if_wrapper(val)\n\u001b[0;32m   2893\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\CEIA\\RL\\8pool-with-reinforcement-learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py:2875\u001b[0m, in \u001b[0;36m_load_obj_from_str\u001b[1;34m(fully_qualified_name)\u001b[0m\n\u001b[0;32m   2873\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_obj_from_str\u001b[39m(fully_qualified_name):\n\u001b[0;32m   2874\u001b[0m     module, obj_name \u001b[38;5;241m=\u001b[39m fully_qualified_name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, maxsplit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m-> 2875\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m, obj_name)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\CEIA\\RL\\8pool-with-reinforcement-learning\\.venv\\Lib\\site-packages\\torch\\_higher_order_ops\\map.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DispatchKey\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dispatch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m suspend_functionalization\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maot_autograd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AOTConfig, create_joint\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_higher_order_ops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     _has_potential_branch_input_alias,\n\u001b[0;32m      9\u001b[0m     _has_potential_branch_input_mutation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     UnsupportedAliasMutationException,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HigherOrderOperator\n",
      "File \u001b[1;32mc:\\CEIA\\RL\\8pool-with-reinforcement-learning\\.venv\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py:32\u001b[0m\n\u001b[0;32m     27\u001b[0m static_inputs_log \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_logging\u001b[38;5;241m.\u001b[39mgetArtifactLogger(\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcudagraph_static_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_aot_autograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd_cache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     AOTAutogradCache,\n\u001b[0;32m     34\u001b[0m     autograd_cache_key,\n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_aot_autograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollect_metadata_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     run_functionalized_fw_and_collect_metadata,\n\u001b[0;32m     38\u001b[0m )\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_aot_autograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     _check_if_mutation_can_be_in_graph,\n\u001b[0;32m     41\u001b[0m     are_all_mutations_hidden_from_autograd,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m     to_fun,\n\u001b[0;32m     51\u001b[0m )\n",
      "File \u001b[1;32mc:\\CEIA\\RL\\8pool-with-reinforcement-learning\\.venv\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\autograd_cache.py:32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mruntime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mruntime_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cache_dir\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_logging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyString\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mruntime_wrappers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     33\u001b[0m     AOTDispatchAutograd,\n\u001b[0;32m     34\u001b[0m     AOTDispatchSubclassWrapper,\n\u001b[0;32m     35\u001b[0m     CompilerWrapper,\n\u001b[0;32m     36\u001b[0m     FunctionalizedRngRuntimeWrapper,\n\u001b[0;32m     37\u001b[0m     post_compile,\n\u001b[0;32m     38\u001b[0m     RuntimeWrapper,\n\u001b[0;32m     39\u001b[0m     SubclassMeta,\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschemas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AOTConfig, ViewAndMutationMeta  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[1;32mc:\\CEIA\\RL\\8pool-with-reinforcement-learning\\.venv\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py:1001\u001b[0m\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m debugged_compiled_fn\n\u001b[0;32m    993\u001b[0m \u001b[38;5;66;03m# This layer handles the situation where you have two inputs that alias each other,\u001b[39;00m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;66;03m# and one of the inputs is mutated.\u001b[39;00m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;66;03m# We need to take special care to ensure that the mutation is applied to the other aliases in the graph.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;66;03m# However, the synthetic base code path is a bit sub-optimal, and running with dupe'd inputs\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;66;03m# would cause us to hit that path more frequently).\u001b[39;00m\n\u001b[1;32m-> 1001\u001b[0m \u001b[38;5;129;43m@dataclass\u001b[39;49m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mAOTSyntheticBaseWrapper\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mCompilerWrapper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Currently, the only reason we need to plumb this bool is because\u001b[39;49;00m\n\u001b[0;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# the synthetic base code prohibits more cases in the autograd case than the inference case.\u001b[39;49;00m\n\u001b[0;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrace_joint\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# TODO: refactor trace_joint\u001b[39;49;00m\n\u001b[0;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneeds_post_compile\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\dataclasses.py:1232\u001b[0m, in \u001b[0;36mdataclass\u001b[1;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[0m\n\u001b[0;32m   1229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrap\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;66;03m# We're called as @dataclass without parens.\u001b[39;00m\n\u001b[1;32m-> 1232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\dataclasses.py:1222\u001b[0m, in \u001b[0;36mdataclass.<locals>.wrap\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m   1221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[1;32m-> 1222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_process_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munsafe_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mfrozen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatch_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mweakref_slot\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\dataclasses.py:1047\u001b[0m, in \u001b[0;36m_process_class\u001b[1;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mrepr\u001b[39m:\n\u001b[0;32m   1046\u001b[0m     flds \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m field_list \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mrepr]\n\u001b[1;32m-> 1047\u001b[0m     _set_new_attribute(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43m_repr_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eq:\n\u001b[0;32m   1050\u001b[0m     \u001b[38;5;66;03m# Create __eq__ method.  There's no need for a __ne__ method,\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;66;03m# since python will call __eq__ and negate it.\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     flds \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m field_list \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mcompare]\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\dataclasses.py:589\u001b[0m, in \u001b[0;36m_repr_fn\u001b[1;34m(fields, globals)\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_repr_fn\u001b[39m(fields, \u001b[38;5;28mglobals\u001b[39m):\n\u001b[1;32m--> 589\u001b[0m     fn \u001b[38;5;241m=\u001b[39m \u001b[43m_create_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m__repr__\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mself\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreturn self.__class__.__qualname__ + f\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[0;32m    592\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m=\u001b[39;49m\u001b[38;5;130;43;01m{{\u001b[39;49;00m\u001b[38;5;124;43mself.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m!r\u001b[39;49m\u001b[38;5;130;43;01m}}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    593\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[0;32m    594\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _recursive_repr(fn)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\dataclasses.py:433\u001b[0m, in \u001b[0;36m_create_fn\u001b[1;34m(name, args, body, globals, locals, return_type)\u001b[0m\n\u001b[0;32m    431\u001b[0m txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdef __create_fn__(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtxt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m return \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    432\u001b[0m ns \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 433\u001b[0m exec(txt, \u001b[38;5;28mglobals\u001b[39m, ns)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__create_fn__\u001b[39m\u001b[38;5;124m'\u001b[39m](\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlocals\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "metrics = defaultdict(list)\n",
    "    \n",
    "def run(args: Args):\n",
    "    args.batch_size = int(args.num_envs * args.num_steps)\n",
    "    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "    args.num_iterations = args.total_timesteps // args.batch_size\n",
    "    args.run_name = f\"__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "\n",
    "    writer = SummaryWriter(f\"runs/{args.run_name}\")\n",
    "    writer.add_text(\n",
    "        \"hyperparameters\",\n",
    "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    "    )\n",
    "\n",
    "    # seeding\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "    # env setup\n",
    "    agent = Agent(action_dim=2,model_args=args.model_args).to(device)\n",
    "    envs = ambientes(n=args.num_envs, agent=agent,rewards_function=rewards_function)\n",
    "\n",
    "\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
    "\n",
    "\n",
    "    # ALGO Logic: Storage setup\n",
    "    obs = torch.zeros((args.num_steps, args.num_envs) + (15,4)).to(device)\n",
    "    obs_white_ball = torch.zeros((args.num_steps, args.num_envs) + (2,)).to(device)\n",
    "    \n",
    "    actions = torch.zeros((args.num_steps, args.num_envs) + (2,)).to(device)\n",
    "    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # start the game\n",
    "    global_step = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #next_obs, _ = envs.reset(seed=args.seed)\n",
    "    \n",
    "    next_obs, next_obs_white = envs.reset()\n",
    "    next_done = torch.zeros(args.num_envs).to(device)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    pbar = tqdm(range(1, args.num_iterations + 1), desc=\"Iterations\")\n",
    "    for iteration in pbar:\n",
    "        # annealing the rate if instructed to do so.\n",
    "        if args.anneal_lr:\n",
    "            frac = 1.0 - (iteration - 1.0) / args.num_iterations\n",
    "            lrnow = frac * args.learning_rate\n",
    "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "        for step in range(0, args.num_steps):\n",
    "            global_step += args.num_envs\n",
    "            obs[step] = next_obs\n",
    "            obs_white_ball[step] = next_obs_white\n",
    "            \n",
    "            dones[step] = next_done\n",
    "\n",
    "            # action logic\n",
    "            with torch.no_grad():\n",
    "                action, logprob, _, value = agent.get_action_and_value(next_obs, next_obs_white)\n",
    "                values[step] = value.flatten()\n",
    "                \n",
    "            actions[step] = action\n",
    "            logprobs[step] = logprob\n",
    "\n",
    "            # execute the game and log data.\n",
    "            (next_obs,next_obs_white), infos, next_done, reward = envs.step(action)\n",
    "\n",
    "            # \n",
    "\n",
    "            rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "            next_obs, next_done , next_obs_white = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device), torch.tensor(next_obs_white).to(device)\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # TODO: IMPLEMENTAR NO AMBIENTE\n",
    "            \n",
    "            #if \"final_info\" in infos:\n",
    "            #    for info in infos[\"final_info\"]:\n",
    "            #        if info and \"episode\" in info:\n",
    "            #            writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "            #            writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "            #            metrics[\"charts/episodic_return\"].append(info[\"episode\"][\"r\"])\n",
    "            #            metrics[\"charts/episodic_length\"].append(info[\"episode\"][\"l\"])\n",
    "            #            pbar.set_postfix_str(f\"step={global_step}, return={info['episode']['r']}\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \"\"\"\n",
    "            PASS OK\n",
    "        \"\"\"  \n",
    "\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # bootstrap value if not done\n",
    "            next_value = agent.get_value(next_obs,next_obs_white).reshape(1, -1)\n",
    "            advantages = torch.zeros_like(rewards).to(device)\n",
    "            lastgaelam = 0\n",
    "\n",
    "            # Calculate returns and advantages using GAE\n",
    "            for t in reversed(range(args.num_steps)):\n",
    "                if t == args.num_steps - 1:\n",
    "                    nextnonterminal = 1.0 - next_done\n",
    "                    nextvalues = next_value\n",
    "                else:\n",
    "                    nextnonterminal = 1.0 - dones[t + 1]\n",
    "                    nextvalues = values[t + 1]\n",
    "                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * lastgaelam\n",
    "            returns = advantages + values\n",
    "\n",
    "        # flatten the batch\n",
    "        \n",
    "        b_obs = obs.reshape((-1,) + envs.single_observation_space())\n",
    "        b_obs_white = obs_white_ball.reshape((-1,) + envs.single_observation_space_white()).squeeze(1)\n",
    "        \n",
    "        \n",
    "        b_logprobs = logprobs.reshape(-1)\n",
    "        b_actions = actions.reshape((-1,) + envs.single_action_space())\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Optimizing the policy and value network\n",
    "        b_inds = np.arange(args.batch_size)\n",
    "        clipfracs = []\n",
    "        for epoch in range(args.update_epochs):\n",
    "            np.random.shuffle(b_inds)\n",
    "            for start in range(0, args.batch_size, args.minibatch_size):\n",
    "                end = start + args.minibatch_size\n",
    "                mb_inds = b_inds[start:end]                \n",
    "\n",
    "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_obs_white[mb_inds], b_actions[mb_inds])\n",
    "                \n",
    "                \n",
    "                logratio = b_logprobs[mb_inds] - newlogprob\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "\n",
    "                mb_advantages = b_advantages[mb_inds]\n",
    "                if args.norm_adv:\n",
    "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = mb_advantages * ratio\n",
    "                pg_loss2 = mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "                pg_loss = torch.min(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                newvalue = newvalue.view(-1)\n",
    "                if args.clip_vloss:\n",
    "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                        newvalue - b_values[mb_inds],\n",
    "                        -args.clip_coef,\n",
    "                        args.clip_coef,\n",
    "                    )\n",
    "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "                else:\n",
    "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "                # Entropy Loss\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss + args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                \n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "            if args.target_kl is not None and approx_kl > args.target_kl:\n",
    "                break\n",
    "\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "        \n",
    "        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "        metrics[\"losses/value_loss\"].append(v_loss.item())\n",
    "        metrics[\"losses/policy_loss\"].append(pg_loss.item())\n",
    "        metrics[\"losses/entropy\"].append(entropy_loss.item())\n",
    "        metrics[\"losses/SPS\"].append(int(global_step / (time.time() - start_time)))\n",
    "        \n",
    "        \n",
    "        if args.save_model:\n",
    "            print('epoch', epoch, 'saving model')\n",
    "            model_path = f\"runs/{args.run_name}/{args.exp_name}_model_iteration_{iteration}\"\n",
    "            torch.save(agent.state_dict(), model_path)\n",
    "        \n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    if args.save_model:\n",
    "        model_path = f\"runs/{args.run_name}/{args.exp_name}_model\"\n",
    "        torch.save(agent.state_dict(), model_path)\n",
    "\n",
    "    #envs.close()\n",
    "    \"\"\"\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    # final evaluation\n",
    "    eval_episodes = 3\n",
    "    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, 0, True, args.run_name)])\n",
    "    agent.eval()\n",
    "    obs, _ = envs.reset()\n",
    "    episodic_returns = []\n",
    "    while len(episodic_returns) < eval_episodes:\n",
    "        action, logprob, entropy, value = agent.get_action_and_value(torch.Tensor(obs).to(device))\n",
    "        action = action.cpu().numpy()\n",
    "        next_obs, _, _, _, infos = envs.step(action)\n",
    "        obs = next_obs\n",
    "        if \"final_info\" in infos:\n",
    "            for info in infos[\"final_info\"]:\n",
    "                if \"episode\" not in info:\n",
    "                    continue\n",
    "                print(f\"eval_episode={len(episodic_returns)}, episodic_return={info['episode']['r']}\")\n",
    "                episodic_returns += [info[\"episode\"][\"r\"]]\n",
    "    \n",
    "    return metrics\n",
    "    \"\"\"\n",
    "\n",
    "cfg.use_clock = False\n",
    "run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mmetrics\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcharts/episodic_return\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(metrics[\"charts/episodic_return\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics[\"losses/policy_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics[\"losses/value_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics[\"losses/entropy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipyflow)",
   "language": "python",
   "name": "ipyflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
