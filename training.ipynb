{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.11.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from game import GAME\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from torch.distributions import Normal\n",
    "from utils import config as cfg\n",
    "\n",
    "\n",
    "from curriculum.reset import reset_random_one_ball"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciando game e exemplificando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GAME(draw=True)  \n",
    "observations = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_function(information):\n",
    "    colisoes = information['colisoes']\n",
    "    bolas_caidas = information['bolas_caidas']\n",
    "    perdeu = information['perdeu']\n",
    "    ganhou = information['ganhou']\n",
    "    joga_novamente = information['joga_novamente']\n",
    "    bolas_jogador = information.get('bolas_jogador', [])\n",
    "    bolas_adversario = information.get('bolas_adversario', [])\n",
    "    winner = information.get('winner', None)\n",
    "    penalizado = information.get('penalizado', False)\n",
    "\n",
    "    rewards = 0\n",
    "\n",
    "    if joga_novamente:\n",
    "        rewards += 1\n",
    "\n",
    "    if perdeu:\n",
    "        rewards -= 1.5\n",
    "\n",
    "    if ganhou:\n",
    "        rewards += 1.5\n",
    "\n",
    "    if penalizado:\n",
    "        rewards -= 1\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.table.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## jogador faz a jogada\n",
    "\n",
    "\n",
    "#env.iniciou_jogada = False\n",
    "#while not env.iniciou_jogada:\n",
    "#    env.table.draw()\n",
    "#\n",
    "#obs, information, terminations, rewards = env.step((env.iniciou_jogada_angulo, \n",
    "#                                                    env.inicou_jogada_intensidade), rewards_function=rewards_function)\n",
    "#\n",
    "#print('state', obs)\n",
    "#print('termination', terminations)\n",
    "#print('rewards', rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando jogada 0\n",
      "Recompensa da jogada: 0\n"
     ]
    }
   ],
   "source": [
    "# computador faz a jogada\n",
    "print(\"Iniciando jogada\", env.jogador_atual)\n",
    "\n",
    "\n",
    "\n",
    "angulo = 10\n",
    "intensidade = 0.9\n",
    "\n",
    "env.iniciou_jogada = False\n",
    "obs, information, terminations, rewards = env.step( (angulo, intensidade),  rewards_function=rewards_function)\n",
    "\n",
    "print(\"Recompensa da jogada:\", rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Começando treinamento\n",
    "                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model_transformers import Model_args, TransformerValueModel, TransformersAtor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo política"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ideia é que tenhamos uma rede neural como política, especificamente um transformers. \n",
    "\n",
    "Queremos que ele passe por cada bola e gere uma representação, no fim a representação do nosso estado será o embedding dabola branca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model_transformers import Model_args, TransformerValueModel, TransformersAtor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instanciando e testando modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo de foward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, bola_branca = (torch.tensor([\n",
    "        [[0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.8250, 0.3750, 1.0000, 1.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
    "        [[0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.8250, 0.3750, 1.0000, 1.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000]]\n",
    "        ]),\n",
    "                      \n",
    " torch.tensor([[0.2000, 0.5000],[0.2000, 0.5000]]))\n",
    "\n",
    "#model(state,bola_branca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15, 4]) torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "state, bola_branca = (torch.tensor([[[0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.7312, 0.4688, 1.0000, 1.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000]]]), torch.tensor([[0.6916, 0.8642]]))\n",
    "\n",
    "print(state.shape, bola_branca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    exp_name: str = \"PPO\"\n",
    "    \"\"\"the name of this experiment\"\"\"\n",
    "    seed: int = 1\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "    torch_deterministic: bool = True\n",
    "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
    "    cuda: bool = torch.cuda.is_available()\n",
    "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    capture_video: bool = False\n",
    "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
    "    save_model: bool = True\n",
    "    \"\"\"whether to save the final model\"\"\"\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    total_timesteps: int = 500000\n",
    "    \"\"\"total timesteps of the experiments\"\"\"\n",
    "    learning_rate: float = 2.5e-4\n",
    "    \"\"\"the learning rate of the optimizer\"\"\"\n",
    "    num_envs: int = 1\n",
    "    \"\"\"the number of parallel game environments\"\"\"\n",
    "    num_steps: int = 128\n",
    "    \"\"\"the number of steps to run in each environment per policy rollout\"\"\"\n",
    "    anneal_lr: bool = True\n",
    "    \"\"\"Toggle learning rate annealing for policy and value networks\"\"\"\n",
    "    gamma: float = 0.99\n",
    "    \"\"\"the discount factor gamma\"\"\"\n",
    "    gae_lambda: float = 0.95\n",
    "    \"\"\"the lambda for the general advantage estimation\"\"\"\n",
    "    num_minibatches: int = 4\n",
    "    \"\"\"the number of mini-batches\"\"\"\n",
    "    update_epochs: int = 4\n",
    "    \"\"\"the K epochs to update the policy\"\"\"\n",
    "    norm_adv: bool = True\n",
    "    \"\"\"Toggles advantages normalization\"\"\"\n",
    "    clip_coef: float = 0.2\n",
    "    \"\"\"the surrogate clipping coefficient\"\"\"\n",
    "    clip_vloss: bool = True\n",
    "    \"\"\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\"\"\"\n",
    "    ent_coef: float = 0.01\n",
    "    \"\"\"coefficient of the entropy\"\"\"\n",
    "    vf_coef: float = 0.5\n",
    "    \"\"\"coefficient of the value function\"\"\"\n",
    "    max_grad_norm: float = 0.5\n",
    "    \"\"\"the maximum norm for the gradient clipping\"\"\"\n",
    "    target_kl: float = None\n",
    "    \"\"\"the target KL divergence threshold\"\"\"\n",
    "\n",
    "    # to be filled in runtime\n",
    "    batch_size: int = 0\n",
    "    \"\"\"the batch size (computed in runtime)\"\"\"\n",
    "    minibatch_size: int = 0\n",
    "    \"\"\"the mini-batch size (computed in runtime)\"\"\"\n",
    "    num_iterations: int = 0\n",
    "    \"\"\"the number of iterations (computed in runtime)\"\"\"\n",
    "    \n",
    "    model_args  = None\n",
    "    \n",
    "    #model_args: ModelArgs = field(default_factory=ModelArgs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, action_dim, model_args):\n",
    "        super().__init__()\n",
    "        self.critic = TransformerValueModel(model_args)  # Modelo para estimar o valor\n",
    "        self.actor_mean = TransformersAtor(model_args)  # Modelo para estimar a média das ações\n",
    "        self.actor_log_std = nn.Parameter(torch.zeros(action_dim))  # Desvio padrão fixo\n",
    "\n",
    "    def get_value(self, state, bola_branca):\n",
    "        \"\"\"Obtém o valor estimado pelo modelo crítico.\"\"\"\n",
    "        return self.critic(state, bola_branca)\n",
    "\n",
    "    def get_action_and_value(self, state, bola_branca, action=None):\n",
    "        \"\"\"\n",
    "        Obtém a ação, log_prob, entropia e valor.\n",
    "        \n",
    "        Args:\n",
    "            x: Entrada para o modelo.\n",
    "            bola_branca: Dados da bola branca.\n",
    "            action: Ação opcional para avaliar log_prob e entropia.\n",
    "        \n",
    "        Returns:\n",
    "            action: Ação amostrada ou fornecida.\n",
    "            log_prob: Logaritmo da probabilidade da ação.\n",
    "            entropy: Entropia da distribuição.\n",
    "            value: Valor estimado pelo crítico.\n",
    "        \"\"\"\n",
    "        # O ator retorna ângulo e intensidade\n",
    "        \n",
    "        print('state',state.shape)\n",
    "        print('state',type(state))\n",
    "        print('bola_branca',bola_branca.shape)\n",
    "        print('bola_branca',bola_branca.dtype)\n",
    "        print('bola_branca',bola_branca)\n",
    "        \n",
    "        mean = self.actor_mean(state, bola_branca)  # Predição da média das ações\n",
    "        \n",
    "        \n",
    "        log_std = self.actor_log_std.expand_as(mean)  # Ajusta o desvio padrão ao formato correto\n",
    "        std = torch.exp(log_std)  # Calcula o desvio padrão\n",
    "\n",
    "        # Distribuição Normal para ações contínuas\n",
    "        \n",
    "        dist = Normal(mean, std)\n",
    "\n",
    "        if action is None:\n",
    "            action = dist.sample()  # Amostra uma ação da distribuição\n",
    "            print('action', action.shape)\n",
    "            \n",
    "        action[..., 0] = torch.clamp(action[..., 0], 0, 360)\n",
    "        action[..., 1] = torch.clamp(action[..., 1], 0, 1)\n",
    "    \n",
    "        log_prob = dist.log_prob(action).sum(axis=-1)  # Log probabilidade\n",
    "        entropy = dist.entropy().sum(axis=-1)  # Entropia\n",
    "\n",
    "        value = self.critic(state, bola_branca)  # Valor estimado pelo crítico\n",
    "\n",
    "        return action, log_prob, entropy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 1, 60, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(200, 1) + (60,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 1, 15, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((200, 1) + (15,4)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "args.model_args = Model_args(\n",
    "                    embed_dim  = 128,\n",
    "                    num_heads  = 8,\n",
    "                    ff_dim  = 128 * 2,\n",
    "                    num_layers  =  2,\n",
    "                    dropout  = 0.1,\n",
    "                    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from curriculum.reset import reset_random_one_ball\n",
    "\n",
    "rewards_function\n",
    "\n",
    "class ambientes():\n",
    "    \n",
    "    def __init__(self,n , agent, rewards_function):\n",
    "        self.n = n\n",
    "        self.agent = agent\n",
    "        \n",
    "        self.envs = [ GAME(draw=True) for _ in range(n)]\n",
    "        self.rewards_function = rewards_function\n",
    "        \n",
    " \n",
    "    def format_observations(self, observations : list[tuple] ):\n",
    "        states = []\n",
    "        states_white_ball = []\n",
    "    \n",
    "        for obs in observations:\n",
    "            states += [obs[0].unsqueeze(dim=0)]\n",
    "            states_white_ball += [obs[1].unsqueeze(dim=0)]\n",
    "        return torch.concat(states) , torch.concat(states_white_ball)\n",
    "       \n",
    "    def reset(self):\n",
    "        states = []\n",
    "        for env in self.envs:\n",
    "            states += [reset_random_one_ball(env=env)]\n",
    "        \n",
    "        return self.format_observations(states)\n",
    "            \n",
    "    def single_observation_space(self):\n",
    "        return (15,4)\n",
    "    \n",
    "    def single_observation_space_white(self):\n",
    "        return (1,2)\n",
    "    \n",
    "    def single_action_space(self):\n",
    "        return (2,)\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        angulo =  action[:,0]\n",
    "        intensidade = action[:,1]\n",
    "        \n",
    "        \n",
    "        get_observations, informations , terminations, rewards = [],[],[],[]\n",
    "        \n",
    "        for i, env in enumerate(self.envs):\n",
    "\n",
    "            env_observations, env_informations , env_terminations, env_rewards = env.step( (angulo[i], intensidade[i]), rewards_function =self.rewards_function )\n",
    "            \n",
    "            get_observations += [env_observations]\n",
    "            informations += [env_informations]\n",
    "            terminations += [env_terminations]\n",
    "            rewards += [env_rewards]\n",
    "            \n",
    "        return self.format_observations(get_observations), informations , terminations, rewards\n",
    "\n",
    "    \n",
    "    #def verify_finish_and_reset(self, terminations):\n",
    "    #    for done , env in zip(terminations, self.envs):\n",
    "    #        if done:\n",
    "     \n",
    "                \n",
    "#test\n",
    "#agent = Agent(action_dim=2,model_args=args.model_args)\n",
    "#a  =  ambientes(2,agent,rewards_function)\n",
    "#state, balls = a.reset()\n",
    "#action, logprob, _, value = agent.get_action_and_value(state, balls)\n",
    "#print(action)\n",
    "#observations, informations , terminations, rewards = a.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_envs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\CEIA\\RL\\8pool-with-reinforcement-learning\\utils\\Ball.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.posicao = torch.tensor(posicao, device=cfg.device, dtype=torch.float32)\n",
      "Iterations:   0%|          | 0/1953 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state torch.Size([2, 15, 4])\n",
      "state <class 'torch.Tensor'>\n",
      "bola_branca torch.Size([2, 2])\n",
      "bola_branca torch.float32\n",
      "bola_branca tensor([[0.8506, 0.5864],\n",
      "        [0.8942, 0.1138]])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ENTROU NA REDE\n",
      "action torch.Size([2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fazzi\\AppData\\Local\\Temp\\ipykernel_36732\\3007374644.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_obs, next_done , next_obs_white = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device), torch.tensor(next_obs_white).to(device)\n",
      "Iterations:   0%|          | 0/1953 [00:03<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state torch.Size([64, 15, 4])\n",
      "state <class 'torch.Tensor'>\n",
      "bola_branca torch.Size([64, 2])\n",
      "bola_branca torch.float32\n",
      "bola_branca tensor([[0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.8506, 0.5864],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000]])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ENTROU NA REDE\n",
      "cheguei\n",
      "state torch.Size([64, 15, 4])\n",
      "state <class 'torch.Tensor'>\n",
      "bola_branca torch.Size([64, 2])\n",
      "bola_branca torch.float32\n",
      "bola_branca tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ENTROU NA REDE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (64, 2)) of distribution Normal(loc: torch.Size([64, 2]), scale: torch.Size([64, 2])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan]], grad_fn=<CatBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 266\u001b[0m\n\u001b[0;32m    264\u001b[0m cfg\u001b[38;5;241m.\u001b[39muse_clock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    265\u001b[0m args\u001b[38;5;241m.\u001b[39mnum_envs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 161\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    158\u001b[0m end \u001b[38;5;241m=\u001b[39m start \u001b[38;5;241m+\u001b[39m args\u001b[38;5;241m.\u001b[39mminibatch_size\n\u001b[0;32m    159\u001b[0m mb_inds \u001b[38;5;241m=\u001b[39m b_inds[start:end]                \n\u001b[1;32m--> 161\u001b[0m _, newlogprob, entropy, newvalue \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action_and_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_obs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmb_inds\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_obs_white\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmb_inds\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_actions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmb_inds\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheguei\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    165\u001b[0m logratio \u001b[38;5;241m=\u001b[39m b_logprobs[mb_inds] \u001b[38;5;241m-\u001b[39m newlogprob\n",
      "Cell \u001b[1;32mIn[12], line 43\u001b[0m, in \u001b[0;36mAgent.get_action_and_value\u001b[1;34m(self, state, bola_branca, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(log_std)  \u001b[38;5;66;03m# Calcula o desvio padrão\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Distribuição Normal para ações contínuas\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()  \u001b[38;5;66;03m# Amostra uma ação da distribuição\u001b[39;00m\n",
      "File \u001b[1;32mc:\\CEIA\\RL\\8pool-with-reinforcement-learning\\.venv\\Lib\\site-packages\\torch\\distributions\\normal.py:59\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[1;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\CEIA\\RL\\8pool-with-reinforcement-learning\\.venv\\Lib\\site-packages\\torch\\distributions\\distribution.py:71\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     69\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m---> 71\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m             )\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (64, 2)) of distribution Normal(loc: torch.Size([64, 2]), scale: torch.Size([64, 2])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan]], grad_fn=<CatBackward0>)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "def run(args: Args):\n",
    "    #metrics = defaultdict(list)\n",
    "    args.batch_size = int(args.num_envs * args.num_steps)\n",
    "    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "    args.num_iterations = args.total_timesteps // args.batch_size\n",
    "    args.run_name = f\"__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "\n",
    "    #writer = SummaryWriter(f\"runs/{args.run_name}\")\n",
    "    #writer.add_text(\n",
    "    #    \"hyperparameters\",\n",
    "    #    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    "    #)\n",
    "\n",
    "    # seeding\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "    # env setup\n",
    "    #envs = gym.vector.SyncVectorEnv(\n",
    "    #    [make_env(args.env_id, i, args.capture_video, args.run_name) for i in range(args.num_envs)],\n",
    "    #)\n",
    "    \n",
    "    agent = Agent(action_dim=2,model_args=args.model_args).to(device)\n",
    "    \n",
    "    envs = ambientes(n=args.num_envs, agent=agent,rewards_function=rewards_function)\n",
    "\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
    "\n",
    "\n",
    "    # ALGO Logic: Storage setup\n",
    "    obs = torch.zeros((args.num_steps, args.num_envs) + (15,4)).to(device)\n",
    "    obs_white_ball = torch.zeros((args.num_steps, args.num_envs) + (2,)).to(device)\n",
    "    \n",
    "    actions = torch.zeros((args.num_steps, args.num_envs) + (2,)).to(device)\n",
    "    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # start the game\n",
    "    global_step = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #next_obs, _ = envs.reset(seed=args.seed)\n",
    "    \n",
    "    next_obs, next_obs_white = envs.reset()\n",
    "    next_done = torch.zeros(args.num_envs).to(device)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    pbar = tqdm(range(1, args.num_iterations + 1), desc=\"Iterations\")\n",
    "    for iteration in pbar:\n",
    "        # annealing the rate if instructed to do so.\n",
    "        if args.anneal_lr:\n",
    "            frac = 1.0 - (iteration - 1.0) / args.num_iterations\n",
    "            lrnow = frac * args.learning_rate\n",
    "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "        for step in range(0, args.num_steps):\n",
    "            global_step += args.num_envs\n",
    "            obs[step] = next_obs\n",
    "            obs_white_ball[step] = next_obs_white\n",
    "            \n",
    "            dones[step] = next_done\n",
    "\n",
    "            # action logic\n",
    "            with torch.no_grad():\n",
    "                action, logprob, _, value = agent.get_action_and_value(next_obs, next_obs_white)\n",
    "                values[step] = value.flatten()\n",
    "                \n",
    "            actions[step] = action\n",
    "            logprobs[step] = logprob\n",
    "\n",
    "            # execute the game and log data.\n",
    "            (next_obs,next_obs_white), infos, next_done, reward = envs.step(action)\n",
    "\n",
    "\n",
    "            rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "            next_obs, next_done , next_obs_white = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device), torch.tensor(next_obs_white).to(device)\n",
    "            \n",
    "            break\n",
    "        \n",
    "            \n",
    "            # TODO: implementar reset se necessario\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "            #if \"final_info\" in infos:\n",
    "            #    for info in infos[\"final_info\"]:\n",
    "            #        if info and \"episode\" in info:\n",
    "            #            writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "            #            writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "            #            metrics[\"charts/episodic_return\"].append(info[\"episode\"][\"r\"])\n",
    "            #            metrics[\"charts/episodic_length\"].append(info[\"episode\"][\"l\"])\n",
    "            #            pbar.set_postfix_str(f\"step={global_step}, return={info['episode']['r']}\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \"\"\"\n",
    "            PASS OK\n",
    "        \"\"\"  \n",
    "\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # bootstrap value if not done\n",
    "            next_value = agent.get_value(next_obs,next_obs_white).reshape(1, -1)\n",
    "            advantages = torch.zeros_like(rewards).to(device)\n",
    "            lastgaelam = 0\n",
    "\n",
    "            # Calculate returns and advantages using GAE\n",
    "            for t in reversed(range(args.num_steps)):\n",
    "                if t == args.num_steps - 1:\n",
    "                    nextnonterminal = 1.0 - next_done\n",
    "                    nextvalues = next_value\n",
    "                else:\n",
    "                    nextnonterminal = 1.0 - dones[t + 1]\n",
    "                    nextvalues = values[t + 1]\n",
    "                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * lastgaelam\n",
    "            returns = advantages + values\n",
    "\n",
    "        # flatten the batch\n",
    "        \n",
    "        b_obs = obs.reshape((-1,) + envs.single_observation_space())\n",
    "        b_obs_white = obs_white_ball.reshape((-1,) + envs.single_observation_space_white()).squeeze(1)\n",
    "        \n",
    "        \n",
    "        b_logprobs = logprobs.reshape(-1)\n",
    "        b_actions = actions.reshape((-1,) + envs.single_action_space())\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Optimizing the policy and value network\n",
    "        b_inds = np.arange(args.batch_size)\n",
    "        clipfracs = []\n",
    "        for epoch in range(args.update_epochs):\n",
    "            np.random.shuffle(b_inds)\n",
    "            for start in range(0, args.batch_size, args.minibatch_size):\n",
    "                end = start + args.minibatch_size\n",
    "                mb_inds = b_inds[start:end]                \n",
    "\n",
    "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_obs_white[mb_inds], b_actions[mb_inds])\n",
    "                \n",
    "                print('cheguei')\n",
    "                \n",
    "                logratio = b_logprobs[mb_inds] - newlogprob\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "\n",
    "                mb_advantages = b_advantages[mb_inds]\n",
    "                if args.norm_adv:\n",
    "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = mb_advantages * ratio\n",
    "                pg_loss2 = mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "                pg_loss = torch.min(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                newvalue = newvalue.view(-1)\n",
    "                if args.clip_vloss:\n",
    "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                        newvalue - b_values[mb_inds],\n",
    "                        -args.clip_coef,\n",
    "                        args.clip_coef,\n",
    "                    )\n",
    "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "                else:\n",
    "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "                # Entropy Loss\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss + args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                \n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "            if args.target_kl is not None and approx_kl > args.target_kl:\n",
    "                break\n",
    "\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "        \"\"\"\n",
    "        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "        metrics[\"losses/value_loss\"].append(v_loss.item())\n",
    "        metrics[\"losses/policy_loss\"].append(pg_loss.item())\n",
    "        metrics[\"losses/entropy\"].append(entropy_loss.item())\n",
    "        metrics[\"losses/SPS\"].append(int(global_step / (time.time() - start_time)))\n",
    "        \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    if args.save_model:\n",
    "        model_path = f\"runs/{args.run_name}/{args.exp_name}_model\"\n",
    "        torch.save(agent.state_dict(), model_path)\n",
    "\n",
    "    #envs.close()\n",
    "    \"\"\"\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    # final evaluation\n",
    "    eval_episodes = 3\n",
    "    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, 0, True, args.run_name)])\n",
    "    agent.eval()\n",
    "    obs, _ = envs.reset()\n",
    "    episodic_returns = []\n",
    "    while len(episodic_returns) < eval_episodes:\n",
    "        action, logprob, entropy, value = agent.get_action_and_value(torch.Tensor(obs).to(device))\n",
    "        action = action.cpu().numpy()\n",
    "        next_obs, _, _, _, infos = envs.step(action)\n",
    "        obs = next_obs\n",
    "        if \"final_info\" in infos:\n",
    "            for info in infos[\"final_info\"]:\n",
    "                if \"episode\" not in info:\n",
    "                    continue\n",
    "                print(f\"eval_episode={len(episodic_returns)}, episodic_return={info['episode']['r']}\")\n",
    "                episodic_returns += [info[\"episode\"][\"r\"]]\n",
    "    \n",
    "    return metrics\n",
    "    \"\"\"\n",
    "\n",
    "cfg.use_clock = False\n",
    "args.num_envs = 2\n",
    "run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fazzi\\AppData\\Local\\Temp\\ipykernel_69568\\1093790985.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  b_obs = torch.load('b_obs.pt')\n",
      "C:\\Users\\fazzi\\AppData\\Local\\Temp\\ipykernel_69568\\1093790985.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  b_obs_white = torch.load('b_obs_white.pt')\n",
      "C:\\Users\\fazzi\\AppData\\Local\\Temp\\ipykernel_69568\\1093790985.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  b_actions = torch.load('b_actions.pt')\n"
     ]
    }
   ],
   "source": [
    "#torch.save(b_obs[mb_inds], 'b_obs.pt')\n",
    "#torch.save(b_obs_white[mb_inds], 'b_obs_white.pt')\n",
    "#torch.save(b_actions[mb_inds], 'b_actions.pt')\n",
    "#\n",
    "#\n",
    "#_, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_obs_white[mb_inds], b_actions[mb_inds])\n",
    "#print('cheguei')\n",
    "                \n",
    "                \n",
    "                \n",
    "# load\n",
    "\n",
    "b_obs = torch.load('b_obs.pt')\n",
    "b_obs_white = torch.load('b_obs_white.pt')\n",
    "b_actions = torch.load('b_actions.pt')\n",
    "\n",
    "agent = Agent(action_dim=2,model_args=args.model_args)\n",
    "_, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs, b_obs_white, b_actions)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_dim, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9769, 0.2516, 1.0000, 0.0000],\n",
       "         [0.7535, 0.7476, 1.0000, 0.0000],\n",
       "         [0.7545, 0.4865, 1.0000, 0.0000],\n",
       "         [0.9280, 0.2381, 1.0000, 0.0000],\n",
       "         [0.7937, 0.4062, 1.0000, 0.0000],\n",
       "         [0.8259, 0.5281, 1.0000, 0.0000],\n",
       "         [0.8699, 0.6038, 1.0000, 0.0000],\n",
       "         [0.9671, 0.7087, 1.0000, 0.0000],\n",
       "         [0.0372, 0.3679, 1.0000, 0.0000],\n",
       "         [0.7445, 0.2964, 1.0000, 0.0000],\n",
       "         [0.6428, 0.0969, 1.0000, 0.0000],\n",
       "         [0.8062, 0.4619, 1.0000, 0.0000],\n",
       "         [0.7291, 0.4671, 1.0000, 0.0000],\n",
       "         [0.9655, 0.8755, 1.0000, 0.0000],\n",
       "         [0.8250, 0.3750, 1.0000, 0.0000]]),\n",
       " tensor([0.4498, 0.4073]))"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action tensor([[ 32.0625,   0.5783],\n",
      "        [101.6929,   0.6640]])\n",
      "log_prob tensor([-3.2769, -1.8512], grad_fn=<SumBackward1>)\n",
      "entropy tensor([2.8379, 2.8379], grad_fn=<SumBackward1>)\n",
      "value tensor([[0.0828],\n",
      "        [0.0119]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "action, log_prob, entropy, value = agent.get_action_and_value(torch.concat((state,state)),torch.concat((bola_branca,bola_branca)))\n",
    "\n",
    "print('action', action)\n",
    "print('log_prob', log_prob)\n",
    "print('entropy', entropy)\n",
    "print('value', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui vemos que as ações não estão no range esperado, isso porque estamos passando uma distribuiçao normal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, agent, value_model ,lr=1e-3, gamma=0.99, eps_clip=0.2, entropy_coef=0.01):\n",
    "        self.policy = agent.policy_model\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        self.value_model = value_model\n",
    "        self.value_optimizer = optim.Adam(self.value_model.parameters(), lr=lr)\n",
    "        \n",
    "    def compute_advantage(self, rewards, values, masks, gamma):\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for reward, mask in zip(reversed(rewards), reversed(masks)):\n",
    "            R = reward + gamma * R * mask\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        advantage = returns - values\n",
    "        return advantage.detach(), returns\n",
    "    \n",
    "    def update(self, states, bola_branca, actions, log_probs_old, rewards, dones):\n",
    "        states_flat = states.view(states.size(0), -1)\n",
    "        batch_size = states.size(0)\n",
    "\n",
    "        # Calculando valores e vantagens\n",
    "        values = self.value_model(states_flat, bola_branca).squeeze(-1)\n",
    "        advantages, returns = self.compute_advantage(rewards, values, 1 - dones, self.gamma)\n",
    "        \n",
    "        # Atualizar política\n",
    "        for _ in range(4):  # Número de atualizações\n",
    "            outputs = self.policy(states, bola_branca)\n",
    "            dist = Normal(outputs, torch.ones_like(outputs))  # Normal para distribuição\n",
    "            log_probs = dist.log_prob(actions).sum(-1)\n",
    "            entropy = dist.entropy().sum(-1)\n",
    "            \n",
    "            # Clipped Surrogate Objective\n",
    "            ratio = torch.exp(log_probs - log_probs_old)\n",
    "            surrogate1 = ratio * advantages\n",
    "            surrogate2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "            entropy_loss = -self.entropy_coef * entropy.mean()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            (policy_loss + entropy_loss).backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Atualizar função de valor\n",
    "        for _ in range(4):  # Número de atualizações\n",
    "            values = self.value_model(torch.cat((states_flat, bola_branca), dim=1)).squeeze(-1)\n",
    "            value_loss = nn.MSELoss()(values, returns)\n",
    "            \n",
    "            self.value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            self.value_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Transformers(args)\n",
    "value = TransformerValueModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = PPO(policy, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "\n",
    "cfg.table_x_max - cfg.table_x_min\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.config as cfg\n",
    "\n",
    "\n",
    "reset_random_one_ball(env)\n",
    "env.table.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env.table.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8250, 0.5000, 1.0000, 1.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000]]),\n",
       " tensor([0.7068, 0.3645]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_bolas, state_bola_branca = env.get_observations()\n",
    "# shuffle the balls\n",
    "state_bolas = state_bolas[torch.randperm(state_bolas.size()[0])]\n",
    "state_bolas , state_bola_branca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_function(information):\n",
    "    colisoes = information['colisoes']\n",
    "    bolas_caidas = information['bolas_caidas']\n",
    "    perdeu = information['perdeu']\n",
    "    ganhou = information['ganhou']\n",
    "    joga_novamente = information['joga_novamente']\n",
    "    bolas_jogador = information.get('bolas_jogador', [])\n",
    "    bolas_adversario = information.get('bolas_adversario', [])\n",
    "    winner = information.get('winner', None)\n",
    "    penalizado = information.get('penalizado', False)\n",
    "\n",
    "    rewards = 0\n",
    "\n",
    "    if penalizado:\n",
    "        rewards -= 1\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1000  # Número de episódios para o treinamento\n",
    "max_steps = 200  # Passos máximos por episódio\n",
    "gamma = 0.99  # Fator de desconto\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_dim=2, args=args)  # Dois valores de ação (ângulo, força)\n",
    "ppo = PPO(policy_model=agent, value_model=agent.critic, lr=learning_rate, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = {\n",
    "    \"states\": [],\n",
    "    \"actions\": [],\n",
    "    \"log_probs\": [],\n",
    "    \"rewards\": [],\n",
    "    \"dones\": [],\n",
    "    \"bola_branca\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.8250, 0.5625, 1.0000, 1.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000]]),\n",
       " tensor([0.0312, 0.7628]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[135.2115,   2.5012]]),\n",
       " tensor([-5.0424], grad_fn=<SumBackward1>),\n",
       " tensor([2.8379], grad_fn=<SumBackward1>),\n",
       " tensor([[-0.3033]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.get_action_and_value(state, bola_branca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.7312, 0.4688, 1.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " tensor([[0.6916, 0.8642]]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, bola_branca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7625, 0.5625, 1.0000, 0.0000],\n",
       "         [0.7937, 0.4688, 1.0000, 0.0000],\n",
       "         [0.8250, 0.5625, 1.0000, 0.0000],\n",
       "         [0.7937, 0.5938, 1.0000, 0.0000],\n",
       "         [0.8250, 0.6250, 1.0000, 0.0000],\n",
       "         [0.0137, 0.3837, 1.0000, 0.0000],\n",
       "         [0.8250, 0.4375, 1.0000, 0.0000],\n",
       "         [0.7625, 0.4375, 1.0000, 0.0000],\n",
       "         [0.8250, 0.5000, 1.0000, 0.0000],\n",
       "         [0.7937, 0.5312, 1.0000, 0.0000],\n",
       "         [0.7625, 0.5000, 1.0000, 0.0000],\n",
       "         [0.7312, 0.4688, 1.0000, 0.0000],\n",
       "         [0.8250, 0.3750, 1.0000, 0.0000],\n",
       "         [0.7312, 0.5312, 1.0000, 0.0000],\n",
       "         [0.7937, 0.4062, 1.0000, 0.0000]]),\n",
       " tensor([0.8709, 0.3837]))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_bolas, state_bola_branca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 4]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(state_bolas.shape, state_bola_branca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        \n",
    "        reset_random_one_ball(env=env, raio_buraco=13)\n",
    "        \n",
    "        rewards_sum = 0\n",
    "        for step in range(max_steps):\n",
    "            # Amostra uma ação do agente\n",
    "            \n",
    "            \n",
    "            state_bolas , state_bola_branca = env.get_observations()\n",
    "            \n",
    "            action, log_prob, entropy, value = agent.get_action_and_value(state_bolas, state_bola_branca)\n",
    "            # Executa a ação no ambiente\n",
    "            \n",
    "            print(action)\n",
    "            \n",
    "            angulo = action[0, 0].item()\n",
    "            intensidade = action[0, 1].item()\n",
    "            \n",
    "            next_obs, information, done, reward = env.step((angulo, intensidade), rewards_function=rewards_function)\n",
    "\n",
    "            next_state_bolas, next_state_bola_branca = next_obs\n",
    "            \n",
    "            # Armazenar dados no buffer\n",
    "            replay_buffer[\"states\"].append(state_bolas)\n",
    "            replay_buffer[\"actions\"].append(action)\n",
    "            replay_buffer[\"log_probs\"].append(log_prob)\n",
    "            replay_buffer[\"rewards\"].append(reward)\n",
    "            replay_buffer[\"dones\"].append(done)\n",
    "            replay_buffer[\"bola_branca\"].append(state_bola_branca)\n",
    "\n",
    "            print(rewards_sum, reward)\n",
    "            rewards_sum += reward\n",
    "\n",
    "            # Atualizar estados\n",
    "            state_bolas = next_state_bolas\n",
    "            state_bola_branca = next_state_bola_branca\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Atualizar a política usando os dados do episódio\n",
    "        ppo.update(\n",
    "            states=torch.stack(replay_buffer[\"states\"]),\n",
    "            bola_branca=torch.stack(replay_buffer[\"bola_branca\"]),\n",
    "            actions=torch.stack(replay_buffer[\"actions\"]),\n",
    "            log_probs_old=torch.stack(replay_buffer[\"log_probs\"]),\n",
    "            rewards=torch.tensor(replay_buffer[\"rewards\"]),\n",
    "            dones=torch.tensor(replay_buffer[\"dones\"])\n",
    "        )\n",
    "        \n",
    "        # Limpar o buffer para o próximo episódio\n",
    "        for key in replay_buffer.keys():\n",
    "            replay_buffer[key] = []\n",
    "\n",
    "        print(f\"Episódio {episode + 1}/{n_episodes}, Recompensa Total: {rewards_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/artur/Documents/VsCode/8pool-with-reinforcement-learning/utils/Ball.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.posicao = torch.tensor(posicao, device=cfg.device, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.1050e+02, 1.0058e-01]])\n",
      "0 -1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[130], line 59\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Atualizar a política usando os dados do episódio\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstates\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbola_branca\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbola_branca\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_probs_old\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog_probs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrewards\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdones\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdones\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Limpar o buffer para o próximo episódio\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m replay_buffer\u001b[38;5;241m.\u001b[39mkeys():\n",
      "Cell \u001b[0;32mIn[54], line 27\u001b[0m, in \u001b[0;36mPPO.update\u001b[0;34m(self, states, bola_branca, actions, log_probs_old, rewards, dones)\u001b[0m\n\u001b[1;32m     24\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m states\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Calculando valores e vantagens\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbola_branca\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m advantages, returns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_advantage(rewards, values, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Atualizar política\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/VsCode/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/VsCode/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[11], line 30\u001b[0m, in \u001b[0;36mTransformerValueModel.forward\u001b[0;34m(self, x, bola_branca)\u001b[0m\n\u001b[1;32m     28\u001b[0m t_concat[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     29\u001b[0m t_concat[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 30\u001b[0m bola_branca \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbola_branca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_concat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat((bola_branca, x), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Passa pela camada de embedding\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "state_bolas, state_bola_branca = env.get_observations()\n",
    "env.table.bolas = env.table.bolas[1:2] + [env.table.bola_branca]\n",
    "numero_bola = env.table.bolas[0].numero\n",
    "state_bolas = state_bolas[torch.randperm(state_bolas.size()[0])] \n",
    "\n",
    "state_bolas = state_bolas[torch.randperm(state_bolas.size()[0])]        \n",
    "        \n",
    "env.table.bolas = env.table.bolas[1:2] + [env.table.bola_branca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7625, 0.5625, 1.0000, 0.0000],\n",
       "         [0.7937, 0.4688, 1.0000, 0.0000],\n",
       "         [0.8250, 0.5625, 1.0000, 0.0000],\n",
       "         [0.7937, 0.5938, 1.0000, 0.0000],\n",
       "         [0.8250, 0.6250, 1.0000, 0.0000],\n",
       "         [0.0137, 0.3837, 1.0000, 0.0000],\n",
       "         [0.8250, 0.4375, 1.0000, 0.0000],\n",
       "         [0.7625, 0.4375, 1.0000, 0.0000],\n",
       "         [0.8250, 0.5000, 1.0000, 0.0000],\n",
       "         [0.7937, 0.5312, 1.0000, 0.0000],\n",
       "         [0.7625, 0.5000, 1.0000, 0.0000],\n",
       "         [0.7312, 0.4688, 1.0000, 0.0000],\n",
       "         [0.8250, 0.3750, 1.0000, 0.0000],\n",
       "         [0.7312, 0.5312, 1.0000, 0.0000],\n",
       "         [0.7937, 0.4062, 1.0000, 0.0000]]),\n",
       " tensor([0.8709, 0.3837]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_bolas, state_bola_branca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "values expected sparse tensor layout but got Strided",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m352.0620\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m1.3407\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: values expected sparse tensor layout but got Strided"
     ]
    }
   ],
   "source": [
    "torch.tensor([[352.0620,   1.3407]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensas no buffer: [-1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Recompensas no buffer:\", replay_buffer[\"rewards\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
