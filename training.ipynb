{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.11.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from game import GAME\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from torch.distributions import Normal\n",
    "\n",
    "\n",
    "\n",
    "from curriculum.reset import reset_random_one_ball"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciando game e exemplificando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GAME(draw=True)  \n",
    "observations = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_function(information):\n",
    "    colisoes = information['colisoes']\n",
    "    bolas_caidas = information['bolas_caidas']\n",
    "    perdeu = information['perdeu']\n",
    "    ganhou = information['ganhou']\n",
    "    joga_novamente = information['joga_novamente']\n",
    "    bolas_jogador = information.get('bolas_jogador', [])\n",
    "    bolas_adversario = information.get('bolas_adversario', [])\n",
    "    winner = information.get('winner', None)\n",
    "    penalizado = information.get('penalizado', False)\n",
    "\n",
    "    rewards = 0\n",
    "\n",
    "    if joga_novamente:\n",
    "        rewards += 1\n",
    "\n",
    "    if perdeu:\n",
    "        rewards -= 1.5\n",
    "\n",
    "    if ganhou:\n",
    "        rewards += 1.5\n",
    "\n",
    "    if penalizado:\n",
    "        rewards -= 1\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.table.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state (tensor([[0.0137, 0.5000, 1.0000, 0.0000],\n",
      "        [0.7535, 0.7476, 1.0000, 0.0000],\n",
      "        [0.7545, 0.4865, 1.0000, 0.0000],\n",
      "        [0.9280, 0.2381, 1.0000, 0.0000],\n",
      "        [0.7937, 0.4062, 1.0000, 0.0000],\n",
      "        [0.8259, 0.5281, 1.0000, 0.0000],\n",
      "        [0.8699, 0.6038, 1.0000, 0.0000],\n",
      "        [0.9671, 0.7087, 1.0000, 0.0000],\n",
      "        [0.5665, 0.3341, 1.0000, 0.0000],\n",
      "        [0.7445, 0.2964, 1.0000, 0.0000],\n",
      "        [0.6428, 0.0969, 1.0000, 0.0000],\n",
      "        [0.8062, 0.4619, 1.0000, 0.0000],\n",
      "        [0.7291, 0.4671, 1.0000, 0.0000],\n",
      "        [0.9655, 0.8755, 1.0000, 0.0000],\n",
      "        [0.8250, 0.3750, 1.0000, 0.0000]]), tensor([0.7510, 0.5762]))\n",
      "termination False\n",
      "rewards 0\n"
     ]
    }
   ],
   "source": [
    "## jogador faz a jogada\n",
    "\n",
    "\n",
    "env.iniciou_jogada = False\n",
    "while not env.iniciou_jogada:\n",
    "    env.table.draw()\n",
    "\n",
    "obs, information, terminations, rewards = env.step((env.iniciou_jogada_angulo, \n",
    "                                                    env.inicou_jogada_intensidade), rewards_function=rewards_function)\n",
    "\n",
    "print('state', obs)\n",
    "print('termination', terminations)\n",
    "print('rewards', rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando jogada True\n",
      "Recompensa da jogada: 0\n"
     ]
    }
   ],
   "source": [
    "# computador faz a jogada\n",
    "print(\"Iniciando jogada\", env.jogador_atual)\n",
    "\n",
    "\n",
    "\n",
    "angulo = 10\n",
    "intensidade = 0.9\n",
    "\n",
    "env.iniciou_jogada = False\n",
    "obs, information, terminations, rewards = env.step( (angulo, intensidade),  rewards_function=rewards_function)\n",
    "\n",
    "print(\"Recompensa da jogada:\", rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Começando treinamento\n",
    "                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model_transformers import Args, TransformerValueModel, TransformersAtor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo política"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ideia é que tenhamos uma rede neural como política, especificamente um transformers. \n",
    "\n",
    "Queremos que ele passe por cada bola e gere uma representação, no fim a representação do nosso estado será o embedding dabola branca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instanciando e testando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args( )\n",
    "model = TransformersAtor( args )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo de foward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[303.6469,   0.5058],\n",
       "        [313.9930,   0.4833]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, bola_branca = (torch.tensor([\n",
    "        [[0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.8250, 0.3750, 1.0000, 1.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
    "        [[0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.8250, 0.3750, 1.0000, 1.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000]]\n",
    "        ]),\n",
    "                      \n",
    " torch.tensor([[0.2000, 0.5000],[0.2000, 0.5000]]))\n",
    "\n",
    "model(state,bola_branca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15, 4]) torch.Size([1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[325.0546,   0.4613]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, bola_branca = (torch.tensor([[[0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.7312, 0.4688, 1.0000, 1.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000]]]), torch.tensor([[0.6916, 0.8642]]))\n",
    "\n",
    "print(state.shape, bola_branca.shape)\n",
    "model(state,bola_branca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerValueModel( args )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, action_dim, args):\n",
    "        super().__init__()\n",
    "        self.critic = TransformerValueModel(args)  # Modelo para estimar o valor\n",
    "        self.actor_mean = TransformersAtor(args)  # Modelo para estimar a média das ações\n",
    "        self.actor_log_std = nn.Parameter(torch.zeros(action_dim))  # Desvio padrão fixo\n",
    "\n",
    "    def get_value(self, state, bola_branca):\n",
    "        \"\"\"Obtém o valor estimado pelo modelo crítico.\"\"\"\n",
    "        return self.critic(state, bola_branca)\n",
    "\n",
    "    def get_action_and_value(self, state, bola_branca, action=None):\n",
    "        \"\"\"\n",
    "        Obtém a ação, log_prob, entropia e valor.\n",
    "        \n",
    "        Args:\n",
    "            x: Entrada para o modelo.\n",
    "            bola_branca: Dados da bola branca.\n",
    "            action: Ação opcional para avaliar log_prob e entropia.\n",
    "        \n",
    "        Returns:\n",
    "            action: Ação amostrada ou fornecida.\n",
    "            log_prob: Logaritmo da probabilidade da ação.\n",
    "            entropy: Entropia da distribuição.\n",
    "            value: Valor estimado pelo crítico.\n",
    "        \"\"\"\n",
    "        # O ator retorna ângulo e intensidade\n",
    "        mean = self.actor_mean(state, bola_branca)  # Predição da média das ações\n",
    "                \n",
    "        log_std = self.actor_log_std.expand_as(mean)  # Ajusta o desvio padrão ao formato correto\n",
    "        std = torch.exp(log_std)  # Calcula o desvio padrão\n",
    "\n",
    "        # Distribuição Normal para ações contínuas\n",
    "        dist = Normal(mean, std)\n",
    "\n",
    "        if action is None:\n",
    "            action = dist.sample()  # Amostra uma ação da distribuição\n",
    "\n",
    "        action[..., 0] = torch.clamp(action[..., 0], 0, 360)\n",
    "        action[..., 1] = torch.clamp(action[..., 1], 0, 1)\n",
    "    \n",
    "        log_prob = dist.log_prob(action).sum(axis=-1)  # Log probabilidade\n",
    "        entropy = dist.entropy().sum(axis=-1)  # Entropia\n",
    "\n",
    "        value = self.critic(state, bola_branca)  # Valor estimado pelo crítico\n",
    "\n",
    "        return action, log_prob, entropy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_dim, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action tensor([[ 32.0625,   0.5783],\n",
      "        [101.6929,   0.6640]])\n",
      "log_prob tensor([-3.2769, -1.8512], grad_fn=<SumBackward1>)\n",
      "entropy tensor([2.8379, 2.8379], grad_fn=<SumBackward1>)\n",
      "value tensor([[0.0828],\n",
      "        [0.0119]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "action, log_prob, entropy, value = agent.get_action_and_value(torch.concat((state,state)),torch.concat((bola_branca,bola_branca)))\n",
    "\n",
    "print('action', action)\n",
    "print('log_prob', log_prob)\n",
    "print('entropy', entropy)\n",
    "print('value', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui vemos que as ações não estão no range esperado, isso porque estamos passando uma distribuiçao normal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, policy_model, value_model ,lr=1e-3, gamma=0.99, eps_clip=0.2, entropy_coef=0.01):\n",
    "        self.policy = policy_model\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        self.value_model = value_model\n",
    "        self.value_optimizer = optim.Adam(self.value_model.parameters(), lr=lr)\n",
    "        \n",
    "    def compute_advantage(self, rewards, values, masks, gamma):\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for reward, mask in zip(reversed(rewards), reversed(masks)):\n",
    "            R = reward + gamma * R * mask\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        advantage = returns - values\n",
    "        return advantage.detach(), returns\n",
    "    \n",
    "    def update(self, states, bola_branca, actions, log_probs_old, rewards, dones):\n",
    "        states_flat = states.view(states.size(0), -1)\n",
    "        batch_size = states.size(0)\n",
    "\n",
    "        # Calculando valores e vantagens\n",
    "        values = self.value_model(states_flat, bola_branca).squeeze(-1)\n",
    "        advantages, returns = self.compute_advantage(rewards, values, 1 - dones, self.gamma)\n",
    "        \n",
    "        # Atualizar política\n",
    "        for _ in range(4):  # Número de atualizações\n",
    "            outputs = self.policy(states, bola_branca)\n",
    "            dist = Normal(outputs, torch.ones_like(outputs))  # Normal para distribuição\n",
    "            log_probs = dist.log_prob(actions).sum(-1)\n",
    "            entropy = dist.entropy().sum(-1)\n",
    "            \n",
    "            # Clipped Surrogate Objective\n",
    "            ratio = torch.exp(log_probs - log_probs_old)\n",
    "            surrogate1 = ratio * advantages\n",
    "            surrogate2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "            entropy_loss = -self.entropy_coef * entropy.mean()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            (policy_loss + entropy_loss).backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Atualizar função de valor\n",
    "        for _ in range(4):  # Número de atualizações\n",
    "            values = self.value_model(torch.cat((states_flat, bola_branca), dim=1)).squeeze(-1)\n",
    "            value_loss = nn.MSELoss()(values, returns)\n",
    "            \n",
    "            self.value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            self.value_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Transformers(args)\n",
    "value = TransformerValueModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = PPO(policy, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "\n",
    "cfg.table_x_max - cfg.table_x_min\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.config as cfg\n",
    "\n",
    "\n",
    "reset_random_one_ball(env)\n",
    "env.table.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env.table.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8250, 0.5000, 1.0000, 1.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000]]),\n",
       " tensor([0.7068, 0.3645]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_bolas, state_bola_branca = env.get_observations()\n",
    "# shuffle the balls\n",
    "state_bolas = state_bolas[torch.randperm(state_bolas.size()[0])]\n",
    "state_bolas , state_bola_branca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_function(information):\n",
    "    colisoes = information['colisoes']\n",
    "    bolas_caidas = information['bolas_caidas']\n",
    "    perdeu = information['perdeu']\n",
    "    ganhou = information['ganhou']\n",
    "    joga_novamente = information['joga_novamente']\n",
    "    bolas_jogador = information.get('bolas_jogador', [])\n",
    "    bolas_adversario = information.get('bolas_adversario', [])\n",
    "    winner = information.get('winner', None)\n",
    "    penalizado = information.get('penalizado', False)\n",
    "\n",
    "    rewards = 0\n",
    "\n",
    "    if penalizado:\n",
    "        rewards -= 1\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1000  # Número de episódios para o treinamento\n",
    "max_steps = 200  # Passos máximos por episódio\n",
    "gamma = 0.99  # Fator de desconto\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_dim=2, args=args)  # Dois valores de ação (ângulo, força)\n",
    "ppo = PPO(policy_model=agent, value_model=agent.critic, lr=learning_rate, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = {\n",
    "    \"states\": [],\n",
    "    \"actions\": [],\n",
    "    \"log_probs\": [],\n",
    "    \"rewards\": [],\n",
    "    \"dones\": [],\n",
    "    \"bola_branca\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.8250, 0.5625, 1.0000, 1.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000]]),\n",
       " tensor([0.0312, 0.7628]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[135.2115,   2.5012]]),\n",
       " tensor([-5.0424], grad_fn=<SumBackward1>),\n",
       " tensor([2.8379], grad_fn=<SumBackward1>),\n",
       " tensor([[-0.3033]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.get_action_and_value(state, bola_branca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.7312, 0.4688, 1.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " tensor([[0.6916, 0.8642]]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, bola_branca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7625, 0.5625, 1.0000, 0.0000],\n",
       "         [0.7937, 0.4688, 1.0000, 0.0000],\n",
       "         [0.8250, 0.5625, 1.0000, 0.0000],\n",
       "         [0.7937, 0.5938, 1.0000, 0.0000],\n",
       "         [0.8250, 0.6250, 1.0000, 0.0000],\n",
       "         [0.0137, 0.3837, 1.0000, 0.0000],\n",
       "         [0.8250, 0.4375, 1.0000, 0.0000],\n",
       "         [0.7625, 0.4375, 1.0000, 0.0000],\n",
       "         [0.8250, 0.5000, 1.0000, 0.0000],\n",
       "         [0.7937, 0.5312, 1.0000, 0.0000],\n",
       "         [0.7625, 0.5000, 1.0000, 0.0000],\n",
       "         [0.7312, 0.4688, 1.0000, 0.0000],\n",
       "         [0.8250, 0.3750, 1.0000, 0.0000],\n",
       "         [0.7312, 0.5312, 1.0000, 0.0000],\n",
       "         [0.7937, 0.4062, 1.0000, 0.0000]]),\n",
       " tensor([0.8709, 0.3837]))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_bolas, state_bola_branca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 4]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(state_bolas.shape, state_bola_branca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        \n",
    "        reset_random_one_ball(env=env, raio_buraco=13)\n",
    "        \n",
    "        rewards_sum = 0\n",
    "        for step in range(max_steps):\n",
    "            # Amostra uma ação do agente\n",
    "            \n",
    "            \n",
    "            state_bolas , state_bola_branca = env.get_observations()\n",
    "            \n",
    "            action, log_prob, entropy, value = agent.get_action_and_value(state_bolas, state_bola_branca)\n",
    "            # Executa a ação no ambiente\n",
    "            \n",
    "            print(action)\n",
    "            \n",
    "            angulo = action[0, 0].item()\n",
    "            intensidade = action[0, 1].item()\n",
    "            \n",
    "            next_obs, information, done, reward = env.step((angulo, intensidade), rewards_function=rewards_function)\n",
    "\n",
    "            next_state_bolas, next_state_bola_branca = next_obs\n",
    "            \n",
    "            # Armazenar dados no buffer\n",
    "            replay_buffer[\"states\"].append(state_bolas)\n",
    "            replay_buffer[\"actions\"].append(action)\n",
    "            replay_buffer[\"log_probs\"].append(log_prob)\n",
    "            replay_buffer[\"rewards\"].append(reward)\n",
    "            replay_buffer[\"dones\"].append(done)\n",
    "            replay_buffer[\"bola_branca\"].append(state_bola_branca)\n",
    "\n",
    "            print(rewards_sum, reward)\n",
    "            rewards_sum += reward\n",
    "\n",
    "            # Atualizar estados\n",
    "            state_bolas = next_state_bolas\n",
    "            state_bola_branca = next_state_bola_branca\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Atualizar a política usando os dados do episódio\n",
    "        ppo.update(\n",
    "            states=torch.stack(replay_buffer[\"states\"]),\n",
    "            bola_branca=torch.stack(replay_buffer[\"bola_branca\"]),\n",
    "            actions=torch.stack(replay_buffer[\"actions\"]),\n",
    "            log_probs_old=torch.stack(replay_buffer[\"log_probs\"]),\n",
    "            rewards=torch.tensor(replay_buffer[\"rewards\"]),\n",
    "            dones=torch.tensor(replay_buffer[\"dones\"])\n",
    "        )\n",
    "        \n",
    "        # Limpar o buffer para o próximo episódio\n",
    "        for key in replay_buffer.keys():\n",
    "            replay_buffer[key] = []\n",
    "\n",
    "        print(f\"Episódio {episode + 1}/{n_episodes}, Recompensa Total: {rewards_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/artur/Documents/VsCode/8pool-with-reinforcement-learning/utils/Ball.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.posicao = torch.tensor(posicao, device=cfg.device, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.1050e+02, 1.0058e-01]])\n",
      "0 -1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[130], line 59\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Atualizar a política usando os dados do episódio\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstates\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbola_branca\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbola_branca\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_probs_old\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog_probs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrewards\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdones\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdones\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Limpar o buffer para o próximo episódio\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m replay_buffer\u001b[38;5;241m.\u001b[39mkeys():\n",
      "Cell \u001b[0;32mIn[54], line 27\u001b[0m, in \u001b[0;36mPPO.update\u001b[0;34m(self, states, bola_branca, actions, log_probs_old, rewards, dones)\u001b[0m\n\u001b[1;32m     24\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m states\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Calculando valores e vantagens\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbola_branca\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m advantages, returns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_advantage(rewards, values, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Atualizar política\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/VsCode/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/VsCode/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[11], line 30\u001b[0m, in \u001b[0;36mTransformerValueModel.forward\u001b[0;34m(self, x, bola_branca)\u001b[0m\n\u001b[1;32m     28\u001b[0m t_concat[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     29\u001b[0m t_concat[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 30\u001b[0m bola_branca \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbola_branca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_concat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat((bola_branca, x), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Passa pela camada de embedding\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "state_bolas, state_bola_branca = env.get_observations()\n",
    "env.table.bolas = env.table.bolas[1:2] + [env.table.bola_branca]\n",
    "numero_bola = env.table.bolas[0].numero\n",
    "state_bolas = state_bolas[torch.randperm(state_bolas.size()[0])] \n",
    "\n",
    "state_bolas = state_bolas[torch.randperm(state_bolas.size()[0])]        \n",
    "        \n",
    "env.table.bolas = env.table.bolas[1:2] + [env.table.bola_branca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7625, 0.5625, 1.0000, 0.0000],\n",
       "         [0.7937, 0.4688, 1.0000, 0.0000],\n",
       "         [0.8250, 0.5625, 1.0000, 0.0000],\n",
       "         [0.7937, 0.5938, 1.0000, 0.0000],\n",
       "         [0.8250, 0.6250, 1.0000, 0.0000],\n",
       "         [0.0137, 0.3837, 1.0000, 0.0000],\n",
       "         [0.8250, 0.4375, 1.0000, 0.0000],\n",
       "         [0.7625, 0.4375, 1.0000, 0.0000],\n",
       "         [0.8250, 0.5000, 1.0000, 0.0000],\n",
       "         [0.7937, 0.5312, 1.0000, 0.0000],\n",
       "         [0.7625, 0.5000, 1.0000, 0.0000],\n",
       "         [0.7312, 0.4688, 1.0000, 0.0000],\n",
       "         [0.8250, 0.3750, 1.0000, 0.0000],\n",
       "         [0.7312, 0.5312, 1.0000, 0.0000],\n",
       "         [0.7937, 0.4062, 1.0000, 0.0000]]),\n",
       " tensor([0.8709, 0.3837]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_bolas, state_bola_branca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "values expected sparse tensor layout but got Strided",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m352.0620\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m1.3407\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: values expected sparse tensor layout but got Strided"
     ]
    }
   ],
   "source": [
    "torch.tensor([[352.0620,   1.3407]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensas no buffer: [-1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Recompensas no buffer:\", replay_buffer[\"rewards\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
