{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from game import GAME\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciando game e exemplificando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GAME(draw=True)  \n",
    "observations = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_function(information):\n",
    "    colisoes = information['colisoes']\n",
    "    bolas_caidas = information['bolas_caidas']\n",
    "    perdeu = information['perdeu']\n",
    "    ganhou = information['ganhou']\n",
    "    joga_novamente = information['joga_novamente']\n",
    "    bolas_jogador = information.get('bolas_jogador', [])\n",
    "    bolas_adversario = information.get('bolas_adversario', [])\n",
    "    winner = information.get('winner', None)\n",
    "    penalizado = information.get('penalizado', False)\n",
    "\n",
    "    rewards = 0\n",
    "\n",
    "    if joga_novamente:\n",
    "        rewards += 1\n",
    "\n",
    "    if perdeu:\n",
    "        rewards -= 1.5\n",
    "\n",
    "    if ganhou:\n",
    "        rewards += 1.5\n",
    "\n",
    "    if penalizado:\n",
    "        rewards -= 1\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.table.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state (tensor([[0.0137, 0.5000, 1.0000, 0.0000],\n",
      "        [0.9322, 0.3294, 1.0000, 0.0000],\n",
      "        [0.8604, 0.0964, 1.0000, 1.0000],\n",
      "        [0.9359, 0.5480, 1.0000, 0.0000],\n",
      "        [0.8154, 0.1850, 1.0000, 0.0000],\n",
      "        [0.8259, 0.3654, 1.0000, 1.0000],\n",
      "        [0.5060, 0.1848, 1.0000, 0.0000],\n",
      "        [0.6799, 0.7674, 1.0000, 1.0000],\n",
      "        [0.8243, 0.7367, 1.0000, 0.0000],\n",
      "        [0.9181, 0.7098, 1.0000, 1.0000],\n",
      "        [0.8112, 0.9580, 1.0000, 0.0000],\n",
      "        [0.9094, 0.1063, 1.0000, 0.0000],\n",
      "        [0.9554, 0.9162, 1.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000]]), tensor([0.4263, 0.7625]))\n",
      "termination False\n",
      "rewards 1\n"
     ]
    }
   ],
   "source": [
    "## jogador faz a jogada\n",
    "\n",
    "\n",
    "env.iniciou_jogada = False\n",
    "while not env.iniciou_jogada:\n",
    "    env.table.draw()\n",
    "\n",
    "obs, information, terminations, rewards = env.step((env.iniciou_jogada_angulo, \n",
    "                                                    env.inicou_jogada_intensidade), rewards_function=rewards_function)\n",
    "\n",
    "print('state', obs)\n",
    "print('termination', terminations)\n",
    "print('rewards', rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando jogada 0\n",
      "Recompensa da jogada: 1\n"
     ]
    }
   ],
   "source": [
    "# computador faz a jogada\n",
    "print(\"Iniciando jogada\", env.jogador_atual)\n",
    "\n",
    "\n",
    "\n",
    "angulo = 10\n",
    "intensidade = 0.9\n",
    "\n",
    "env.iniciou_jogada = False\n",
    "obs, information, terminations, rewards = env.step( (angulo, intensidade),  rewards_function=rewards_function)\n",
    "\n",
    "print(\"Recompensa da jogada:\", rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Começando treinamento\n",
    "                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    embed_dim : int = 128\n",
    "    num_heads : int = 8         \n",
    "    ff_dim : int = embed_dim * 2\n",
    "    num_layers : int =  2       \n",
    "    dropout : float = 0.1         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo política"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ideia é que tenhamos uma rede neural como política, especificamente um transformers. \n",
    "\n",
    "Queremos que ele passe por cada bola e gere uma representação, no fim a representação do nosso estado será o embedding dabola branca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transformers(nn.Module):\n",
    "    def __init__(self, args : Args):\n",
    "        super(Transformers, self).__init__()\n",
    "        \n",
    "        # Layer de embedding (entra uma bola com (x,y,z, w) e sai um vetor de 128 dimensões que representa a bola)\n",
    "        self.embedding = nn.Linear(4, args.embed_dim)\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer( d_model=args.embed_dim, \n",
    "                                       nhead=args.num_heads, \n",
    "                                       dim_feedforward=args.ff_dim, \n",
    "                                       dropout=args.dropout) for _ in range(args.num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(args.embed_dim)\n",
    "        \n",
    "        #MlP para gerar regressão de angulo e intensidade\n",
    "        self.mlp = nn.Sequential( \n",
    "                nn.Linear(args.embed_dim, args.ff_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(args.ff_dim, 3) # Vamos sair com (sen, cos, intensidade)\n",
    "        )\n",
    "        \n",
    "        self.head_position = nn.Tanh() # Tanh para garantir que o angulo fique entre -1 e 1\n",
    "        self.head_intensity = nn.Sigmoid() # Sigmoid para garantir que a intensidade fique entre 0 e 1\n",
    "\n",
    "    def forward(self, x, bola_branca): \n",
    "        # adiciona a dimensão da bolinha branca -> batch, (x,y,1,-1)\n",
    "        b = bola_branca.shape[0]\n",
    "        t_concat = torch.zeros(b,2, device=bola_branca.device)\n",
    "        t_concat[:,0] = 1\n",
    "        t_concat[:,1] = -1\n",
    "        bola_branca = torch.concat( (bola_branca , t_concat), dim=-1).unsqueeze(1)\n",
    "        x = torch.concat((bola_branca,x),dim=1)\n",
    "        \n",
    "        x = self.embedding(x)        \n",
    "        \n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "    \n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        # bola branca é nosso Value, que ira representar o estado do jogo\n",
    "        x = x[:,0,:]        \n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        # transforma o x em angulo e intensidade\n",
    "        position = self.head_position(x[:,:2])\n",
    "        intensity = self.head_intensity(x[:,2:])\n",
    "        \n",
    "        x_sin = position[:,0:1] # seno\n",
    "        x_cos = position[:,1:2] # cosseno\n",
    "        \n",
    "        angles_deg = torch.rad2deg(torch.atan2(x_sin, x_cos)) # converte para graus\n",
    "        angles_deg = torch.where(angles_deg < 0, angles_deg + 360, angles_deg)\n",
    "        \n",
    "        # concatena o resultado\n",
    "        return torch.concat((angles_deg,intensity),dim=-1) # Retorna o angulo e a intensidade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instanciando e testando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(\n",
    ")\n",
    "\n",
    "model = Transformers(\n",
    "    args\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo de foward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[193.8388,   0.3252],\n",
       "        [214.9594,   0.3166]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, bola_branca = (torch.tensor([\n",
    "        [[0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.8250, 0.3750, 1.0000, 1.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
    "        [[0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.8250, 0.3750, 1.0000, 1.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000]]\n",
    "        ]),\n",
    "                      \n",
    " torch.tensor([[0.2000, 0.5000],[0.2000, 0.5000]]))\n",
    "\n",
    "model(state,bola_branca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15, 4]) torch.Size([1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[219.0895,   0.2755]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, bola_branca = (torch.tensor([[[0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.7312, 0.4688, 1.0000, 1.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "         [0.0000, 0.0000, 0.0000, 0.0000]]]), torch.tensor([[0.6916, 0.8642]]))\n",
    "\n",
    "print(state.shape, bola_branca.shape)\n",
    "model(state,bola_branca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerValueModel(nn.Module):\n",
    "    def __init__(self, args : Args):\n",
    "        super(TransformerValueModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Linear(4, args.embed_dim)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=args.embed_dim,\n",
    "                nhead=args.num_heads,\n",
    "                dim_feedforward=args.ff_dim,\n",
    "                dropout=args.dropout\n",
    "            ) for _ in range(args.num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(args.embed_dim)\n",
    "        \n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(args.embed_dim, args.ff_dim),  # Projeta para dimensão intermediária\n",
    "            nn.ReLU(),                    # Ativação não-linear\n",
    "            nn.Linear(args.ff_dim, 1)          # Saída escalar (valor do estado)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, bola_branca):\n",
    "        # Adiciona a dimensão da bolinha branca -> batch, (x,y,1,-1)\n",
    "        b = bola_branca.shape[0]\n",
    "        t_concat = torch.zeros(b, 2, device=bola_branca.device)\n",
    "        t_concat[:, 0] = 1\n",
    "        t_concat[:, 1] = -1\n",
    "        bola_branca = torch.concat((bola_branca, t_concat), dim=-1).unsqueeze(1)\n",
    "        x = torch.concat((bola_branca, x), dim=1)\n",
    "        \n",
    "        # Passa pela camada de embedding\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Passa pelas camadas de encoder\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Normalização final\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        # Seleciona a representação do estado do jogo (primeiro token)\n",
    "        x = x[:, 0, :]\n",
    "        \n",
    "        # Passa pela cabeça de valor para prever V(s)\n",
    "        value = self.value_head(x)\n",
    "        \n",
    "        # Garante que a saída está no formato (batch_size,)\n",
    "        return value  # Remove apenas a última dimensão\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(\n",
    "    embed_dim=128   \n",
    ")\n",
    "\n",
    "model = TransformerValueModel(\n",
    "    args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1637],\n",
       "        [0.2640]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(state,bola_branca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, action_dim, args):\n",
    "        super().__init__()\n",
    "        self.critic = TransformerValueModel(args)  # Modelo para estimar o valor\n",
    "        self.actor_mean = Transformers(args)  # Modelo para estimar a média das ações\n",
    "        self.actor_log_std = nn.Parameter(torch.zeros(action_dim))  # Desvio padrão fixo\n",
    "\n",
    "    def get_value(self, state, bola_branca):\n",
    "        \"\"\"Obtém o valor estimado pelo modelo crítico.\"\"\"\n",
    "        return self.critic(state, bola_branca)\n",
    "\n",
    "    def get_action_and_value(self, state, bola_branca, action=None):\n",
    "        \"\"\"\n",
    "        Obtém a ação, log_prob, entropia e valor.\n",
    "        \n",
    "        Args:\n",
    "            x: Entrada para o modelo.\n",
    "            bola_branca: Dados da bola branca.\n",
    "            action: Ação opcional para avaliar log_prob e entropia.\n",
    "        \n",
    "        Returns:\n",
    "            action: Ação amostrada ou fornecida.\n",
    "            log_prob: Logaritmo da probabilidade da ação.\n",
    "            entropy: Entropia da distribuição.\n",
    "            value: Valor estimado pelo crítico.\n",
    "        \"\"\"\n",
    "        # O ator retorna ângulo e intensidade\n",
    "        mean = self.actor_mean(state, bola_branca)  # Predição da média das ações\n",
    "        log_std = self.actor_log_std.expand_as(mean)  # Ajusta o desvio padrão ao formato correto\n",
    "        std = torch.exp(log_std)  # Calcula o desvio padrão\n",
    "\n",
    "        # Distribuição Normal para ações contínuas\n",
    "        dist = Normal(mean, std)\n",
    "\n",
    "        if action is None:\n",
    "            action = dist.sample()  # Amostra uma ação da distribuição\n",
    "\n",
    "        log_prob = dist.log_prob(action).sum(axis=-1)  # Log probabilidade\n",
    "        entropy = dist.entropy().sum(axis=-1)  # Entropia\n",
    "\n",
    "        value = self.critic(state, bola_branca)  # Valor estimado pelo crítico\n",
    "\n",
    "        return action, log_prob, entropy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_dim, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action tensor([[68.5015,  0.5418]])\n",
      "log_prob tensor([-2.9486], grad_fn=<SumBackward1>)\n",
      "entropy tensor([2.8379], grad_fn=<SumBackward1>)\n",
      "value tensor([[-0.2299]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "action, log_prob, entropy, value = agent.get_action_and_value(state, bola_branca)\n",
    "\n",
    "print('action', action)\n",
    "print('log_prob', log_prob)\n",
    "print('entropy', entropy)\n",
    "print('value', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui vemos que as ações não estão no range esperado, isso porque estamos passando uma distribuiçao normal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, policy_model, value_model ,lr=1e-3, gamma=0.99, eps_clip=0.2, entropy_coef=0.01):\n",
    "        self.policy = policy_model\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        self.value_model = value_model\n",
    "        self.value_optimizer = optim.Adam(self.value_model.parameters(), lr=lr)\n",
    "        \n",
    "    def compute_advantage(self, rewards, values, masks, gamma):\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for reward, mask in zip(reversed(rewards), reversed(masks)):\n",
    "            R = reward + gamma * R * mask\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        advantage = returns - values\n",
    "        return advantage.detach(), returns\n",
    "    \n",
    "    def update(self, states, bola_branca, actions, log_probs_old, rewards, dones):\n",
    "        states_flat = states.view(states.size(0), -1)\n",
    "        batch_size = states.size(0)\n",
    "\n",
    "        # Calculando valores e vantagens\n",
    "        values = self.value_model(states_flat, bola_branca).squeeze(-1)\n",
    "        advantages, returns = self.compute_advantage(rewards, values, 1 - dones, self.gamma)\n",
    "        \n",
    "        # Atualizar política\n",
    "        for _ in range(4):  # Número de atualizações\n",
    "            outputs = self.policy(states, bola_branca)\n",
    "            dist = Normal(outputs, torch.ones_like(outputs))  # Normal para distribuição\n",
    "            log_probs = dist.log_prob(actions).sum(-1)\n",
    "            entropy = dist.entropy().sum(-1)\n",
    "            \n",
    "            # Clipped Surrogate Objective\n",
    "            ratio = torch.exp(log_probs - log_probs_old)\n",
    "            surrogate1 = ratio * advantages\n",
    "            surrogate2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "            entropy_loss = -self.entropy_coef * entropy.mean()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            (policy_loss + entropy_loss).backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Atualizar função de valor\n",
    "        for _ in range(4):  # Número de atualizações\n",
    "            values = self.value_model(torch.cat((states_flat, bola_branca), dim=1)).squeeze(-1)\n",
    "            value_loss = nn.MSELoss()(values, returns)\n",
    "            \n",
    "            self.value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            self.value_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Transformers(args)\n",
    "value = TransformerValueModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = PPO(policy, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/artur/Documents/VsCode/8pool-with-reinforcement-learning/utils/Ball.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.posicao = torch.tensor(posicao, device=cfg.device, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import utils.config as cfg\n",
    "\n",
    "## iniciar o ambiente com a configuração do jogo com apenas uma bola na mesa. \n",
    "# - A bola branca em uma posição aleatória\n",
    "# - se não bater na bola, acabou a jogada\n",
    "\n",
    "cfg.raio_buraco = 13\n",
    "cfg.bola_branca_posicao_inicial = (1000, 1000)\n",
    "\n",
    "\n",
    "cfg.bola_branca_posicao_inicial = torch.tensor([\n",
    "    torch.rand(1).item() * (cfg.table_x_max - cfg.table_x_min) + cfg.table_x_min,\n",
    "    torch.rand(1).item() * (cfg.table_y_max - cfg.table_y_min) + cfg.table_y_min\n",
    "])\n",
    "\n",
    "env.reset()\n",
    "env.table.bolas = env.table.bolas[1:2] + [env.table.bola_branca]\n",
    "numero_bola = env.table.bolas[0].numero\n",
    "env.numero_bolas = [[ numero_bola ], [ numero_bola ]]\n",
    "\n",
    "#env.table.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.table.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8250, 0.5000, 1.0000, 1.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000]]),\n",
       " tensor([0.7068, 0.3645]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_bolas, state_bola_branca = env.get_observations()\n",
    "# shuffle the balls\n",
    "state_bolas = state_bolas[torch.randperm(state_bolas.size()[0])]\n",
    "state_bolas , state_bola_branca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_function(information):\n",
    "    colisoes = information['colisoes']\n",
    "    bolas_caidas = information['bolas_caidas']\n",
    "    perdeu = information['perdeu']\n",
    "    ganhou = information['ganhou']\n",
    "    joga_novamente = information['joga_novamente']\n",
    "    bolas_jogador = information.get('bolas_jogador', [])\n",
    "    bolas_adversario = information.get('bolas_adversario', [])\n",
    "    winner = information.get('winner', None)\n",
    "    penalizado = information.get('penalizado', False)\n",
    "\n",
    "    rewards = 0\n",
    "\n",
    "    if penalizado:\n",
    "        rewards -= 1\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1000  # Número de episódios para o treinamento\n",
    "max_steps = 200  # Passos máximos por episódio\n",
    "gamma = 0.99  # Fator de desconto\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_dim=2, args=args)  # Dois valores de ação (ângulo, força)\n",
    "ppo = PPO(policy_model=agent, value_model=agent.critic, lr=learning_rate, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = {\n",
    "    \"states\": [],\n",
    "    \"actions\": [],\n",
    "    \"log_probs\": [],\n",
    "    \"rewards\": [],\n",
    "    \"dones\": [],\n",
    "    \"bola_branca\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[135.2115,   2.5012]]),\n",
       " tensor([-5.0424], grad_fn=<SumBackward1>),\n",
       " tensor([2.8379], grad_fn=<SumBackward1>),\n",
       " tensor([[-0.3033]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.get_action_and_value(state, bola_branca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.7312, 0.4688, 1.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " tensor([[0.6916, 0.8642]]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, bola_branca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7625, 0.5625, 1.0000, 0.0000],\n",
       "         [0.7937, 0.4688, 1.0000, 0.0000],\n",
       "         [0.8250, 0.5625, 1.0000, 0.0000],\n",
       "         [0.7937, 0.5938, 1.0000, 0.0000],\n",
       "         [0.8250, 0.6250, 1.0000, 0.0000],\n",
       "         [0.0137, 0.3837, 1.0000, 0.0000],\n",
       "         [0.8250, 0.4375, 1.0000, 0.0000],\n",
       "         [0.7625, 0.4375, 1.0000, 0.0000],\n",
       "         [0.8250, 0.5000, 1.0000, 0.0000],\n",
       "         [0.7937, 0.5312, 1.0000, 0.0000],\n",
       "         [0.7625, 0.5000, 1.0000, 0.0000],\n",
       "         [0.7312, 0.4688, 1.0000, 0.0000],\n",
       "         [0.8250, 0.3750, 1.0000, 0.0000],\n",
       "         [0.7312, 0.5312, 1.0000, 0.0000],\n",
       "         [0.7937, 0.4062, 1.0000, 0.0000]]),\n",
       " tensor([0.8709, 0.3837]))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_bolas, state_bola_branca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 4]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(state_bolas.shape, state_bola_branca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for episode in range(n_episodes):\n",
    "        \n",
    "        cfg.bola_branca_posicao_inicial = torch.tensor([\n",
    "        torch.rand(1).item() * (cfg.table_x_max - cfg.table_x_min) + cfg.table_x_min,\n",
    "        torch.rand(1).item() * (cfg.table_y_max - cfg.table_y_min) + cfg.table_y_min\n",
    "        ])\n",
    "\n",
    "        env.reset()\n",
    "        env.table.bolas = env.table.bolas[1:2] + [env.table.bola_branca]\n",
    "        numero_bola = env.table.bolas[0].numero\n",
    "        env.numero_bolas = [[ numero_bola ], [ numero_bola ]]\n",
    "\n",
    "        state_bolas, state_bola_branca = env.get_observations()\n",
    "        # shuffle the balls\n",
    "        state_bolas = state_bolas[torch.randperm(state_bolas.size()[0])]        \n",
    "        \n",
    "        env.table.bolas = env.table.bolas[1:2] + [env.table.bola_branca]\n",
    "        \n",
    "        rewards_sum = 0\n",
    "        for step in range(max_steps):\n",
    "            # Amostra uma ação do agente\n",
    "            \n",
    "            #Adiciona a dimensão do batch\n",
    "            state_bolas = state_bolas.unsqueeze(0)\n",
    "            state_bola_branca = state_bola_branca.unsqueeze(0)\n",
    "    \n",
    "            action, log_prob, entropy, value = agent.get_action_and_value(state_bolas, state_bola_branca)\n",
    "            # Executa a ação no ambiente\n",
    "            \n",
    "            print(action)\n",
    "            \n",
    "            angulo = action[0, 0].item()\n",
    "            intensidade = action[0, 1].item()\n",
    "            \n",
    "            next_obs, information, done, reward = env.step((angulo, intensidade), rewards_function=rewards_function)\n",
    "\n",
    "            next_state_bolas, next_state_bola_branca = next_obs\n",
    "            \n",
    "            # Armazenar dados no buffer\n",
    "            replay_buffer[\"states\"].append(state_bolas)\n",
    "            replay_buffer[\"actions\"].append(action)\n",
    "            replay_buffer[\"log_probs\"].append(log_prob)\n",
    "            replay_buffer[\"rewards\"].append(reward)\n",
    "            replay_buffer[\"dones\"].append(done)\n",
    "            replay_buffer[\"bola_branca\"].append(state_bola_branca)\n",
    "\n",
    "            print(rewards_sum, reward)\n",
    "            rewards_sum += reward\n",
    "\n",
    "            # Atualizar estados\n",
    "            state_bolas = next_state_bolas\n",
    "            state_bola_branca = next_state_bola_branca\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Atualizar a política usando os dados do episódio\n",
    "        ppo.update(\n",
    "            states=torch.stack(replay_buffer[\"states\"]),\n",
    "            bola_branca=torch.stack(replay_buffer[\"bola_branca\"]),\n",
    "            actions=torch.stack(replay_buffer[\"actions\"]),\n",
    "            log_probs_old=torch.stack(replay_buffer[\"log_probs\"]),\n",
    "            rewards=torch.tensor(replay_buffer[\"rewards\"]),\n",
    "            dones=torch.tensor(replay_buffer[\"dones\"])\n",
    "        )\n",
    "        \n",
    "        # Limpar o buffer para o próximo episódio\n",
    "        for key in replay_buffer.keys():\n",
    "            replay_buffer[key] = []\n",
    "\n",
    "        print(f\"Episódio {episode + 1}/{n_episodes}, Recompensa Total: {rewards_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/artur/Documents/VsCode/8pool-with-reinforcement-learning/utils/Ball.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.posicao = torch.tensor(posicao, device=cfg.device, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.1050e+02, 1.0058e-01]])\n",
      "0 -1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[130], line 59\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Atualizar a política usando os dados do episódio\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstates\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbola_branca\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbola_branca\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_probs_old\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog_probs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrewards\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdones\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdones\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Limpar o buffer para o próximo episódio\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m replay_buffer\u001b[38;5;241m.\u001b[39mkeys():\n",
      "Cell \u001b[0;32mIn[54], line 27\u001b[0m, in \u001b[0;36mPPO.update\u001b[0;34m(self, states, bola_branca, actions, log_probs_old, rewards, dones)\u001b[0m\n\u001b[1;32m     24\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m states\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Calculando valores e vantagens\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbola_branca\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m advantages, returns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_advantage(rewards, values, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Atualizar política\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/VsCode/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/VsCode/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[11], line 30\u001b[0m, in \u001b[0;36mTransformerValueModel.forward\u001b[0;34m(self, x, bola_branca)\u001b[0m\n\u001b[1;32m     28\u001b[0m t_concat[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     29\u001b[0m t_concat[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 30\u001b[0m bola_branca \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbola_branca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_concat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat((bola_branca, x), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Passa pela camada de embedding\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "state_bolas, state_bola_branca = env.get_observations()\n",
    "env.table.bolas = env.table.bolas[1:2] + [env.table.bola_branca]\n",
    "numero_bola = env.table.bolas[0].numero\n",
    "state_bolas = state_bolas[torch.randperm(state_bolas.size()[0])] \n",
    "\n",
    "state_bolas = state_bolas[torch.randperm(state_bolas.size()[0])]        \n",
    "        \n",
    "env.table.bolas = env.table.bolas[1:2] + [env.table.bola_branca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7625, 0.5625, 1.0000, 0.0000],\n",
       "         [0.7937, 0.4688, 1.0000, 0.0000],\n",
       "         [0.8250, 0.5625, 1.0000, 0.0000],\n",
       "         [0.7937, 0.5938, 1.0000, 0.0000],\n",
       "         [0.8250, 0.6250, 1.0000, 0.0000],\n",
       "         [0.0137, 0.3837, 1.0000, 0.0000],\n",
       "         [0.8250, 0.4375, 1.0000, 0.0000],\n",
       "         [0.7625, 0.4375, 1.0000, 0.0000],\n",
       "         [0.8250, 0.5000, 1.0000, 0.0000],\n",
       "         [0.7937, 0.5312, 1.0000, 0.0000],\n",
       "         [0.7625, 0.5000, 1.0000, 0.0000],\n",
       "         [0.7312, 0.4688, 1.0000, 0.0000],\n",
       "         [0.8250, 0.3750, 1.0000, 0.0000],\n",
       "         [0.7312, 0.5312, 1.0000, 0.0000],\n",
       "         [0.7937, 0.4062, 1.0000, 0.0000]]),\n",
       " tensor([0.8709, 0.3837]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_bolas, state_bola_branca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "values expected sparse tensor layout but got Strided",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m352.0620\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m1.3407\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: values expected sparse tensor layout but got Strided"
     ]
    }
   ],
   "source": [
    "torch.tensor([[352.0620,   1.3407]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensas no buffer: [-1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Recompensas no buffer:\", replay_buffer[\"rewards\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipyflow)",
   "language": "python",
   "name": "ipyflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
