{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import GAME\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from torch.distributions import Normal\n",
    "from utils import config as cfg\n",
    "\n",
    "\n",
    "from curriculum.reset import reset_random_one_ball\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from models.model_transformers import Model_args, TransformerValueModel, TransformersAtor\n",
    "\n",
    "from models.agent import Agent\n",
    "\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from curriculum.reset import reset_random_one_ball\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = GAME(draw=True)  \n",
    "#observations = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_function(information):\n",
    "    colisoes = information['colisoes']\n",
    "    bolas_caidas = information['bolas_caidas']\n",
    "    perdeu = information['perdeu']\n",
    "    ganhou = information['ganhou']\n",
    "    joga_novamente = information['joga_novamente']\n",
    "    bolas_jogador = information.get('bolas_jogador', [])\n",
    "    bolas_adversario = information.get('bolas_adversario', [])\n",
    "    winner = information.get('winner', None)\n",
    "    penalizado = information.get('penalizado', False)\n",
    "\n",
    "    rewards = 0\n",
    "\n",
    "    if joga_novamente:\n",
    "        rewards += 1\n",
    "\n",
    "    if perdeu:\n",
    "        rewards -= 1.5\n",
    "\n",
    "    if ganhou:\n",
    "        rewards += 1.5\n",
    "\n",
    "    if penalizado:\n",
    "        rewards -= 1\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.table.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### jogador faz a jogada\n",
    "#\n",
    "#\n",
    "#env.iniciou_jogada = False\n",
    "#while not env.iniciou_jogada:\n",
    "#    env.table.draw()\n",
    "#\n",
    "#obs, information, terminations, rewards = env.step((env.iniciou_jogada_angulo, \n",
    "#                                                    env.inicou_jogada_intensidade), rewards_function=rewards_function)\n",
    "#\n",
    "#\n",
    "#print(env.iniciou_jogada_angulo)\n",
    "#\n",
    "##print('state', obs)\n",
    "##print('termination', terminations)\n",
    "##print('rewards', rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## computador faz a jogada\n",
    "#print(\"Iniciando jogada\", env.jogador_atual)\n",
    "#\n",
    "#\n",
    "#angulo = torch.pi/2\n",
    "#intensidade = 0.1\n",
    "#\n",
    "#env.iniciou_jogada = False\n",
    "#obs, information, terminations, rewards = env.step( (angulo, intensidade),  rewards_function=rewards_function)\n",
    "#\n",
    "#print(\"Recompensa da jogada:\", rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Começando treinamento\n",
    "                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo política"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ideia é que tenhamos uma rede neural como política, especificamente um transformers. \n",
    "\n",
    "Queremos que ele passe por cada bola e gere uma representação, no fim a representação do nosso estado será o embedding dabola branca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    exp_name: str = \"PPO\"\n",
    "    \"\"\"the name of this experiment\"\"\"\n",
    "    seed: int = 1\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "    torch_deterministic: bool = True\n",
    "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
    "    cuda: bool = torch.cuda.is_available()\n",
    "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    capture_video: bool = False\n",
    "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
    "    save_model: bool = True\n",
    "    \"\"\"whether to save the final model\"\"\"\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    total_timesteps: int = 500000\n",
    "    \"\"\"total timesteps of the experiments\"\"\"\n",
    "    learning_rate: float = 2.5e-4\n",
    "    \"\"\"the learning rate of the optimizer\"\"\"\n",
    "    num_envs: int = 2\n",
    "    \"\"\"the number of parallel game environments\"\"\"\n",
    "    num_steps: int = 10 # 128\n",
    "    \"\"\"the number of steps to run in each environment per policy rollout\"\"\"\n",
    "    anneal_lr: bool = True\n",
    "    \"\"\"Toggle learning rate annealing for policy and value networks\"\"\"\n",
    "    gamma: float = 0.99\n",
    "    \"\"\"the discount factor gamma\"\"\"\n",
    "    gae_lambda: float = 0.95\n",
    "    \"\"\"the lambda for the general advantage estimation\"\"\"\n",
    "    num_minibatches: int = 4\n",
    "    \"\"\"the number of mini-batches\"\"\"\n",
    "    update_epochs: int = 4\n",
    "    \"\"\"the K epochs to update the policy\"\"\"\n",
    "    norm_adv: bool = True\n",
    "    \"\"\"Toggles advantages normalization\"\"\"\n",
    "    clip_coef: float = 0.2\n",
    "    \"\"\"the surrogate clipping coefficient\"\"\"\n",
    "    clip_vloss: bool = True\n",
    "    \"\"\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\"\"\"\n",
    "    ent_coef: float = 0.01\n",
    "    \"\"\"coefficient of the entropy\"\"\"\n",
    "    vf_coef: float = 0.5\n",
    "    \"\"\"coefficient of the value function\"\"\"\n",
    "    max_grad_norm: float = 0.5\n",
    "    \"\"\"the maximum norm for the gradient clipping\"\"\"\n",
    "    target_kl: float = None\n",
    "    \"\"\"the target KL divergence threshold\"\"\"\n",
    "\n",
    "    # to be filled in runtime\n",
    "    batch_size: int = 0\n",
    "    \"\"\"the batch size (computed in runtime)\"\"\"\n",
    "    minibatch_size: int = 0\n",
    "    \"\"\"the mini-batch size (computed in runtime)\"\"\"\n",
    "    num_iterations: int = 0\n",
    "    \"\"\"the number of iterations (computed in runtime)\"\"\"\n",
    "    \n",
    "    model_args  = None\n",
    "    \n",
    "    #model_args: ModelArgs = field(default_factory=ModelArgs)\n",
    "\n",
    "args = Args()\n",
    "args.model_args = Model_args(\n",
    "                    embed_dim  = 128,\n",
    "                    num_heads  = 8,\n",
    "                    ff_dim  = 128 * 2,\n",
    "                    num_layers  =  2,\n",
    "                    dropout  = 0.1,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rewards_function\n",
    "\n",
    "\n",
    "class ambientes():\n",
    "    \n",
    "    def __init__(self, n, agent, rewards_function):\n",
    "        self.n = n\n",
    "        self.agent = agent\n",
    "        self.envs = [GAME(draw=True) for _ in range(n)]\n",
    "        self.rewards_function = rewards_function\n",
    "\n",
    "    def format_observations(self, observations: list[tuple]):\n",
    "        states = []\n",
    "        states_white_ball = []\n",
    "\n",
    "        for obs in observations:\n",
    "            states += [obs[0].unsqueeze(dim=0)]\n",
    "            states_white_ball += [obs[1].unsqueeze(dim=0)]\n",
    "        return torch.concat(states), torch.concat(states_white_ball)\n",
    "\n",
    "    def reset(self):\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            states = list(executor.map(reset_random_one_ball, self.envs))\n",
    "        return self.format_observations(states)\n",
    "\n",
    "    def single_observation_space(self):\n",
    "        return (15, 4)\n",
    "\n",
    "    def single_observation_space_white(self):\n",
    "        return (1, 2)\n",
    "\n",
    "    def single_action_space(self):\n",
    "        return (2,)\n",
    "\n",
    "    def step(self, action):\n",
    "        angulo = action[:, 0]\n",
    "        intensidade = action[:, 1]\n",
    "\n",
    "        def process_env(i_env):\n",
    "            env = self.envs[i_env]\n",
    "            env_observations, env_informations, env_terminations, env_rewards = env.step(\n",
    "                (angulo[i_env], intensidade[i_env]), rewards_function=self.rewards_function\n",
    "            )\n",
    "\n",
    "            if env_terminations:\n",
    "                env_observations = reset_random_one_ball(env=env)\n",
    "\n",
    "            return env_observations, env_informations, env_terminations, env_rewards\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(executor.map(process_env, range(self.n)))\n",
    "\n",
    "        get_observations, informations, terminations, rewards = zip(*results)\n",
    "\n",
    "        return self.format_observations(get_observations), list(informations), list(terminations), list(rewards)\n",
    "\n",
    "# Teste\n",
    "# agent = Agent(action_dim=2, model_args=args.model_args)\n",
    "# a = ambientes(2, agent, rewards_function)\n",
    "# state, balls = a.reset()\n",
    "# action, logprob, _, value = agent.get_action_and_value(state, balls)\n",
    "# print(action)\n",
    "# observations, informations, terminations, rewards = a.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\CEIA\\RL\\8pool-with-reinforcement-learning\\utils\\Ball.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.posicao = torch.tensor(posicao, device=cfg.device, dtype=torch.float32)\n",
      "Iterations:   0%|          | 0/25000 [00:00<?, ?it/s]C:\\Users\\fazzi\\AppData\\Local\\Temp\\ipykernel_56560\\1710523724.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_obs, next_done , next_obs_white = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device), torch.tensor(next_obs_white).to(device)\n",
      "c:\\CEIA\\RL\\8pool-with-reinforcement-learning\\utils\\Table.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.bola_branca.posicao = torch.tensor(cfg.bola_branca_posicao_inicial,device=cfg.device, dtype=torch.float32)\n",
      "Iterations:   0%|          | 1/25000 [00:20<144:42:13, 20.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 2/25000 [00:53<191:37:02, 27.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 3/25000 [01:18<183:28:18, 26.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 4/25000 [01:48<193:57:28, 27.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 5/25000 [02:13<187:20:35, 26.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 6/25000 [02:34<173:12:12, 24.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 7/25000 [03:01<177:23:26, 25.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 8/25000 [03:31<187:07:52, 26.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 9/25000 [03:52<173:24:04, 24.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 10/25000 [04:10<159:24:08, 22.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 11/25000 [04:27<145:35:34, 20.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 12/25000 [04:46<143:20:45, 20.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 13/25000 [05:10<150:15:16, 21.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 14/25000 [05:25<136:25:18, 19.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 15/25000 [05:41<127:53:34, 18.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 16/25000 [06:04<137:29:34, 19.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 17/25000 [06:18<124:29:36, 17.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 18/25000 [06:38<129:35:53, 18.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 19/25000 [06:48<111:08:30, 16.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 20/25000 [07:01<105:16:37, 15.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 21/25000 [07:14<100:24:45, 14.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 22/25000 [07:30<104:01:00, 14.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 23/25000 [07:45<104:13:57, 15.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 24/25000 [08:03<110:08:17, 15.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 25/25000 [08:19<111:17:49, 16.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 26/25000 [08:38<115:47:45, 16.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 27/25000 [08:57<120:39:44, 17.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 28/25000 [09:09<110:02:32, 15.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 29/25000 [09:23<105:19:17, 15.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 30/25000 [09:45<119:33:59, 17.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 31/25000 [10:05<126:53:28, 18.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 32/25000 [10:31<141:10:50, 20.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 32/25000 [10:45<139:55:54, 20.18s/it]\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\CEIA\\RL\\8pool-with-reinforcement-learning\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "metrics = defaultdict(list)\n",
    "    \n",
    "def run(args: Args):\n",
    "    args.batch_size = int(args.num_envs * args.num_steps)\n",
    "    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "    args.num_iterations = args.total_timesteps // args.batch_size\n",
    "    args.run_name = f\"__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "\n",
    "    writer = SummaryWriter(f\"runs/{args.run_name}\")\n",
    "    writer.add_text(\n",
    "        \"hyperparameters\",\n",
    "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    "    )\n",
    "\n",
    "    # seeding\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "    # env setup\n",
    "    agent = Agent(action_dim=2,model_args=args.model_args).to(device)\n",
    "    envs = ambientes(n=args.num_envs, agent=agent,rewards_function=rewards_function)\n",
    "\n",
    "\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
    "\n",
    "\n",
    "    # ALGO Logic: Storage setup\n",
    "    obs = torch.zeros((args.num_steps, args.num_envs) + (15,4)).to(device)\n",
    "    obs_white_ball = torch.zeros((args.num_steps, args.num_envs) + (2,)).to(device)\n",
    "    \n",
    "    actions = torch.zeros((args.num_steps, args.num_envs) + (2,)).to(device)\n",
    "    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # start the game\n",
    "    global_step = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #next_obs, _ = envs.reset(seed=args.seed)\n",
    "    \n",
    "    next_obs, next_obs_white = envs.reset()\n",
    "    next_done = torch.zeros(args.num_envs).to(device)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    pbar = tqdm(range(1, args.num_iterations + 1), desc=\"Iterations\")\n",
    "    for iteration in pbar:\n",
    "        # annealing the rate if instructed to do so.\n",
    "        if args.anneal_lr:\n",
    "            frac = 1.0 - (iteration - 1.0) / args.num_iterations\n",
    "            lrnow = frac * args.learning_rate\n",
    "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "        for step in range(0, args.num_steps):\n",
    "            global_step += args.num_envs\n",
    "            obs[step] = next_obs\n",
    "            obs_white_ball[step] = next_obs_white\n",
    "            \n",
    "            dones[step] = next_done\n",
    "\n",
    "            # action logic\n",
    "            with torch.no_grad():\n",
    "                action, logprob, _, value = agent.get_action_and_value(next_obs, next_obs_white)\n",
    "                values[step] = value.flatten()\n",
    "                \n",
    "            actions[step] = action\n",
    "            logprobs[step] = logprob\n",
    "\n",
    "            # execute the game and log data.\n",
    "            (next_obs,next_obs_white), infos, next_done, reward = envs.step(action)\n",
    "\n",
    "            # \n",
    "\n",
    "            rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "            next_obs, next_done , next_obs_white = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device), torch.tensor(next_obs_white).to(device)\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # TODO: IMPLEMENTAR NO AMBIENTE\n",
    "            \n",
    "            #if \"final_info\" in infos:\n",
    "            #    for info in infos[\"final_info\"]:\n",
    "            #        if info and \"episode\" in info:\n",
    "            #            writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "            #            writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "            #            metrics[\"charts/episodic_return\"].append(info[\"episode\"][\"r\"])\n",
    "            #            metrics[\"charts/episodic_length\"].append(info[\"episode\"][\"l\"])\n",
    "            #            pbar.set_postfix_str(f\"step={global_step}, return={info['episode']['r']}\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \"\"\"\n",
    "            PASS OK\n",
    "        \"\"\"  \n",
    "\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # bootstrap value if not done\n",
    "            next_value = agent.get_value(next_obs,next_obs_white).reshape(1, -1)\n",
    "            advantages = torch.zeros_like(rewards).to(device)\n",
    "            lastgaelam = 0\n",
    "\n",
    "            # Calculate returns and advantages using GAE\n",
    "            for t in reversed(range(args.num_steps)):\n",
    "                if t == args.num_steps - 1:\n",
    "                    nextnonterminal = 1.0 - next_done\n",
    "                    nextvalues = next_value\n",
    "                else:\n",
    "                    nextnonterminal = 1.0 - dones[t + 1]\n",
    "                    nextvalues = values[t + 1]\n",
    "                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * lastgaelam\n",
    "            returns = advantages + values\n",
    "\n",
    "        # flatten the batch\n",
    "        \n",
    "        b_obs = obs.reshape((-1,) + envs.single_observation_space())\n",
    "        b_obs_white = obs_white_ball.reshape((-1,) + envs.single_observation_space_white()).squeeze(1)\n",
    "        \n",
    "        \n",
    "        b_logprobs = logprobs.reshape(-1)\n",
    "        b_actions = actions.reshape((-1,) + envs.single_action_space())\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Optimizing the policy and value network\n",
    "        b_inds = np.arange(args.batch_size)\n",
    "        clipfracs = []\n",
    "        for epoch in range(args.update_epochs):\n",
    "            np.random.shuffle(b_inds)\n",
    "            for start in range(0, args.batch_size, args.minibatch_size):\n",
    "                end = start + args.minibatch_size\n",
    "                mb_inds = b_inds[start:end]                \n",
    "\n",
    "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_obs_white[mb_inds], b_actions[mb_inds])\n",
    "                \n",
    "                \n",
    "                logratio = b_logprobs[mb_inds] - newlogprob\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "\n",
    "                mb_advantages = b_advantages[mb_inds]\n",
    "                if args.norm_adv:\n",
    "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = mb_advantages * ratio\n",
    "                pg_loss2 = mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "                pg_loss = torch.min(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                newvalue = newvalue.view(-1)\n",
    "                if args.clip_vloss:\n",
    "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                        newvalue - b_values[mb_inds],\n",
    "                        -args.clip_coef,\n",
    "                        args.clip_coef,\n",
    "                    )\n",
    "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "                else:\n",
    "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "                # Entropy Loss\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss + args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                \n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "            if args.target_kl is not None and approx_kl > args.target_kl:\n",
    "                break\n",
    "\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "        \n",
    "        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "        metrics[\"losses/value_loss\"].append(v_loss.item())\n",
    "        metrics[\"losses/policy_loss\"].append(pg_loss.item())\n",
    "        metrics[\"losses/entropy\"].append(entropy_loss.item())\n",
    "        metrics[\"losses/SPS\"].append(int(global_step / (time.time() - start_time)))\n",
    "        \n",
    "        \n",
    "        if args.save_model:\n",
    "            print('epoch', epoch, 'saving model')\n",
    "            model_path = f\"runs/{args.run_name}/{args.exp_name}_model_iteration_{iteration}\"\n",
    "            torch.save(agent.state_dict(), model_path)\n",
    "        \n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    if args.save_model:\n",
    "        model_path = f\"runs/{args.run_name}/{args.exp_name}_model\"\n",
    "        torch.save(agent.state_dict(), model_path)\n",
    "\n",
    "    #envs.close()\n",
    "    \"\"\"\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    # final evaluation\n",
    "    eval_episodes = 3\n",
    "    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, 0, True, args.run_name)])\n",
    "    agent.eval()\n",
    "    obs, _ = envs.reset()\n",
    "    episodic_returns = []\n",
    "    while len(episodic_returns) < eval_episodes:\n",
    "        action, logprob, entropy, value = agent.get_action_and_value(torch.Tensor(obs).to(device))\n",
    "        action = action.cpu().numpy()\n",
    "        next_obs, _, _, _, infos = envs.step(action)\n",
    "        obs = next_obs\n",
    "        if \"final_info\" in infos:\n",
    "            for info in infos[\"final_info\"]:\n",
    "                if \"episode\" not in info:\n",
    "                    continue\n",
    "                print(f\"eval_episode={len(episodic_returns)}, episodic_return={info['episode']['r']}\")\n",
    "                episodic_returns += [info[\"episode\"][\"r\"]]\n",
    "    \n",
    "    return metrics\n",
    "    \"\"\"\n",
    "\n",
    "cfg.use_clock = False\n",
    "run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mmetrics\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcharts/episodic_return\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(metrics[\"charts/episodic_return\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics[\"losses/policy_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics[\"losses/value_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics[\"losses/entropy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
